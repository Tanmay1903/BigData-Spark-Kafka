# BigData-Spark-Kafka
- To implement spark code to determine the top 100 most Frequently occurring words and top 100 Most Frequently occurring words having more than 6 characters in a given dataset of 16GB.
- Gain proficiency in log analytics and implement the log analytics techniques described in the article.
- Implement a data processing and analysis pipeline using Big Data technologies, including Kafka (producer and consumer), Spark Streaming, Parquet files, and HDFS File System. The objective is to process and analyze log files by establishing a flow that begins with a Kafka producer, followed by a Kafka consumer that performs transformations using Spark. 

## Problem Statement
- Problem 1: Develop Spark code to determine the top 100 most frequently occurring words and the top 100 most frequently occurring words with more than 6 characters in a given dataset of 16GB.

- Problem 2: Gain proficiency in log analytics. 
  - Create a summary report that identifies the endpoint with the highest number of invocations on a specific day of the week, along with the corresponding count of invocations, for the entire dataset.
  - Find the top 10 years with the least occurrences of 404 status codes in the dataset and provide the corresponding count of 404 status codes for each year.

- Problem 3: Implement a data processing and analysis pipeline using Big Data technologies, including Kafka (producer and consumer), Spark Streaming, Parquet files, and HDFS File System. The pipeline should process and analyze log files, starting with a Kafka producer, followed by a Kafka consumer performing transformations using Spark. The transformed data should be converted into Parquet file format and stored in the HDFS. 

- Bonus Problem: The bonus task requires the development of a clustering algorithm capable of grouping requests based on several factors, including the host that invoked it, the time at which the endpoint was accessed, the status code received, and the data size of the returned information.

