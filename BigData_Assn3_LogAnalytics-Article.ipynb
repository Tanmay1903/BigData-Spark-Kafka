{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3b7057d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/05/28 12:55:38 WARN Utils: Your hostname, Tanmays-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 10.0.0.123 instead (on interface en0)\n",
      "23/05/28 12:55:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/05/28 12:55:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "/Users/tanmaysingla/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql.context import SQLContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "    \n",
    "sc = SparkContext()\n",
    "sqlContext = SQLContext(sc)\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "# load up other dependencies\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b521b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.text(\"42MBSmallServerLog.log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be563179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f584150",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "299999"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b62bd2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10.82.240.107 - - [26/Dec/2118:12:00:00 +0530] \"GET /Archives/edgar/data/0001411207/000156459021052445/0001564590-21-052445-index.htm HTTP/1.0\" 304 1638',\n",
       " '169.174.119.238 - - [26/Dec/2118:12:00:00 +0530] \"DELETE /Archives/edgar/data/0000004281/000119312517062657/R97.htm HTTP/1.0\" 303 40118',\n",
       " '116.82.220.168 - - [26/Dec/2118:12:00:00 +0530] \"POST /Archives/edgar/data/0001168220/000116822010000017/form10k_123109.htm HTTP/1.0\" 403 5856',\n",
       " '99.149.129.109 - - [26/Dec/2118:12:00:00 +0530] \"POST /Archives/edgar/data/0001765880/000121390021017179/s131103_10k.htm HTTP/1.0\" 500 25206',\n",
       " '167.134.232.8 - - [26/Dec/2118:12:00:00 +0530] \"PUT /Archives/edgar/data/1537583/999999999720003444/9999999997-20-003444-index.htm HTTP/1.0\" 200 29727',\n",
       " '128.171.81.194 - - [26/Dec/2118:12:00:00 +0530] \"GET /Archives/edgar/data/0001491487/000114420415051928/0001144204-15-051928.txt HTTP/1.0\" 304 3695',\n",
       " '74.59.229.245 - - [26/Dec/2118:12:00:00 +0530] \"GET /Archives/edgar/data/0001056673/000094930398000131/0000949303-98-000131.txt HTTP/1.0\" 200 37021',\n",
       " '198.123.235.207 - - [26/Dec/2118:12:00:00 +0530] \"DELETE /Archives/edgar/data/0000352825/000035282519000012/0000352825-19-000012.txt HTTP/1.0\" 403 46602',\n",
       " '77.26.34.100 - - [26/Dec/2118:12:00:00 +0530] \"PUT /Archives/edgar/data/0001135856/000113585616000445/xslFormDX01/primary_doc.xml HTTP/1.0\" 502 50169',\n",
       " '83.211.223.129 - - [26/Dec/2118:12:00:00 +0530] \"POST /Archives/edgar/data/0001260125/000114420419015080/tv516463_10k.htm HTTP/1.0\" 403 39593',\n",
       " '223.88.81.117 - - [26/Dec/2118:12:00:00 +0530] \"DELETE /Archives/edgar/data/0001349577/000090342321000016/til20061-10k0312.htm HTTP/1.0\" 403 6995',\n",
       " '15.0.1.210 - - [26/Dec/2118:12:00:00 +0530] \"POST /Archives/edgar/data/0001801390/000121390021017183/s131106_10k.htm HTTP/1.0\" 404 557',\n",
       " '26.81.42.55 - - [26/Dec/2118:12:00:00 +0530] \"DELETE /Archives/edgar/data/0001767306/000093247112005362/0000932471-12-005362.txt HTTP/1.0\" 502 14105',\n",
       " '6.207.234.209 - - [26/Dec/2118:12:00:00 +0530] \"GET /Archives/edgar/data/0001157987/000117494721000301/form10k-25693_czn1.htm HTTP/1.0\" 502 59076',\n",
       " '212.158.71.182 - - [26/Dec/2118:12:00:00 +0530] \"GET /Archives/edgar/data/0000200245/000095010323001599/dp188169_424b2-us2316597.htm HTTP/1.0\" 404 15539']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_logs = [item['value'] for item in df.take(15)]\n",
    "sample_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e57bc8e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10.82.240.107',\n",
       " '169.174.119.238',\n",
       " '116.82.220.168',\n",
       " '99.149.129.109',\n",
       " '167.134.232.8',\n",
       " '128.171.81.194',\n",
       " '74.59.229.245',\n",
       " '198.123.235.207',\n",
       " '77.26.34.100',\n",
       " '83.211.223.129',\n",
       " '223.88.81.117',\n",
       " '15.0.1.210',\n",
       " '26.81.42.55',\n",
       " '6.207.234.209',\n",
       " '212.158.71.182']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "host_pattern = r'(^\\S+\\.[\\S+\\.]+\\S+)\\s'\n",
    "hosts = [re.search(host_pattern, item).group(1)\n",
    "           if re.search(host_pattern, item)\n",
    "           else 'no match'\n",
    "           for item in sample_logs]\n",
    "hosts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "323a642d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['26/Dec/2118:12:00:00 +0530',\n",
       " '26/Dec/2118:12:00:00 +0530',\n",
       " '26/Dec/2118:12:00:00 +0530',\n",
       " '26/Dec/2118:12:00:00 +0530',\n",
       " '26/Dec/2118:12:00:00 +0530',\n",
       " '26/Dec/2118:12:00:00 +0530',\n",
       " '26/Dec/2118:12:00:00 +0530',\n",
       " '26/Dec/2118:12:00:00 +0530',\n",
       " '26/Dec/2118:12:00:00 +0530',\n",
       " '26/Dec/2118:12:00:00 +0530',\n",
       " '26/Dec/2118:12:00:00 +0530',\n",
       " '26/Dec/2118:12:00:00 +0530',\n",
       " '26/Dec/2118:12:00:00 +0530',\n",
       " '26/Dec/2118:12:00:00 +0530',\n",
       " '26/Dec/2118:12:00:00 +0530']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ts_pattern = r'\\[(\\d{2}/\\w{3}/\\d{4}:\\d{2}:\\d{2}:\\d{2} \\+\\d{4})\\]'\n",
    "timestamps = [re.search(ts_pattern, item).group(1) if re.search(ts_pattern, item) else 'no match' for item in sample_logs]\n",
    "timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fab72e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('GET',\n",
       "  '/Archives/edgar/data/0001411207/000156459021052445/0001564590-21-052445-index.htm',\n",
       "  'HTTP/1.0'),\n",
       " ('DELETE',\n",
       "  '/Archives/edgar/data/0000004281/000119312517062657/R97.htm',\n",
       "  'HTTP/1.0'),\n",
       " ('POST',\n",
       "  '/Archives/edgar/data/0001168220/000116822010000017/form10k_123109.htm',\n",
       "  'HTTP/1.0'),\n",
       " ('POST',\n",
       "  '/Archives/edgar/data/0001765880/000121390021017179/s131103_10k.htm',\n",
       "  'HTTP/1.0'),\n",
       " ('PUT',\n",
       "  '/Archives/edgar/data/1537583/999999999720003444/9999999997-20-003444-index.htm',\n",
       "  'HTTP/1.0'),\n",
       " ('GET',\n",
       "  '/Archives/edgar/data/0001491487/000114420415051928/0001144204-15-051928.txt',\n",
       "  'HTTP/1.0'),\n",
       " ('GET',\n",
       "  '/Archives/edgar/data/0001056673/000094930398000131/0000949303-98-000131.txt',\n",
       "  'HTTP/1.0'),\n",
       " ('DELETE',\n",
       "  '/Archives/edgar/data/0000352825/000035282519000012/0000352825-19-000012.txt',\n",
       "  'HTTP/1.0'),\n",
       " ('PUT',\n",
       "  '/Archives/edgar/data/0001135856/000113585616000445/xslFormDX01/primary_doc.xml',\n",
       "  'HTTP/1.0'),\n",
       " ('POST',\n",
       "  '/Archives/edgar/data/0001260125/000114420419015080/tv516463_10k.htm',\n",
       "  'HTTP/1.0'),\n",
       " ('DELETE',\n",
       "  '/Archives/edgar/data/0001349577/000090342321000016/til20061-10k0312.htm',\n",
       "  'HTTP/1.0'),\n",
       " ('POST',\n",
       "  '/Archives/edgar/data/0001801390/000121390021017183/s131106_10k.htm',\n",
       "  'HTTP/1.0'),\n",
       " ('DELETE',\n",
       "  '/Archives/edgar/data/0001767306/000093247112005362/0000932471-12-005362.txt',\n",
       "  'HTTP/1.0'),\n",
       " ('GET',\n",
       "  '/Archives/edgar/data/0001157987/000117494721000301/form10k-25693_czn1.htm',\n",
       "  'HTTP/1.0'),\n",
       " ('GET',\n",
       "  '/Archives/edgar/data/0000200245/000095010323001599/dp188169_424b2-us2316597.htm',\n",
       "  'HTTP/1.0')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "method_uri_protocol_pattern = r'\\\"(\\S+)\\s(\\S+)\\s*(\\S*)\\\"'\n",
    "method_uri_protocol = [re.search(method_uri_protocol_pattern, item).groups()\n",
    "               if re.search(method_uri_protocol_pattern, item)\n",
    "               else 'no match'\n",
    "              for item in sample_logs]\n",
    "method_uri_protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38e75ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['304', '303', '403', '500', '200', '304', '200', '403', '502', '403', '403', '404', '502', '502', '404']\n"
     ]
    }
   ],
   "source": [
    "status_pattern = r'\\s(\\d{3})\\s'\n",
    "status = [re.search(status_pattern, item).group(1) for item in sample_logs]\n",
    "print(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47acf08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1638', '40118', '5856', '25206', '29727', '3695', '37021', '46602', '50169', '39593', '6995', '557', '14105', '59076', '15539']\n"
     ]
    }
   ],
   "source": [
    "content_size_pattern = r'\\s(\\d+)$'\n",
    "content_size = [re.search(content_size_pattern, item).group(1) for item in sample_logs]\n",
    "print(content_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80e8446f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------------+------+--------------------+--------+------+------------+\n",
      "|           host|           timestamp|method|            endpoint|protocol|status|content_size|\n",
      "+---------------+--------------------+------+--------------------+--------+------+------------+\n",
      "|  10.82.240.107|26/Dec/2118:12:00...|   GET|/Archives/edgar/d...|HTTP/1.0|   304|        1638|\n",
      "|169.174.119.238|26/Dec/2118:12:00...|DELETE|/Archives/edgar/d...|HTTP/1.0|   303|       40118|\n",
      "| 116.82.220.168|26/Dec/2118:12:00...|  POST|/Archives/edgar/d...|HTTP/1.0|   403|        5856|\n",
      "| 99.149.129.109|26/Dec/2118:12:00...|  POST|/Archives/edgar/d...|HTTP/1.0|   500|       25206|\n",
      "|  167.134.232.8|26/Dec/2118:12:00...|   PUT|/Archives/edgar/d...|HTTP/1.0|   200|       29727|\n",
      "| 128.171.81.194|26/Dec/2118:12:00...|   GET|/Archives/edgar/d...|HTTP/1.0|   304|        3695|\n",
      "|  74.59.229.245|26/Dec/2118:12:00...|   GET|/Archives/edgar/d...|HTTP/1.0|   200|       37021|\n",
      "|198.123.235.207|26/Dec/2118:12:00...|DELETE|/Archives/edgar/d...|HTTP/1.0|   403|       46602|\n",
      "|   77.26.34.100|26/Dec/2118:12:00...|   PUT|/Archives/edgar/d...|HTTP/1.0|   502|       50169|\n",
      "| 83.211.223.129|26/Dec/2118:12:00...|  POST|/Archives/edgar/d...|HTTP/1.0|   403|       39593|\n",
      "+---------------+--------------------+------+--------------------+--------+------+------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "(299999, 7)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import regexp_extract\n",
    "\n",
    "logs_df = df.select(regexp_extract('value', host_pattern, 1).alias('host'),\n",
    "                         regexp_extract('value', ts_pattern, 1).alias('timestamp'),\n",
    "                         regexp_extract('value', method_uri_protocol_pattern, 1).alias('method'),\n",
    "                         regexp_extract('value', method_uri_protocol_pattern, 2).alias('endpoint'),\n",
    "                         regexp_extract('value', method_uri_protocol_pattern, 3).alias('protocol'),\n",
    "                         regexp_extract('value', status_pattern, 1).cast('integer').alias('status'),\n",
    "                         regexp_extract('value', content_size_pattern, 1).cast('integer').alias('content_size'))\n",
    "logs_df.show(10, truncate=True)\n",
    "print((logs_df.count(), len(logs_df.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62a0db6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.filter(df['value'].isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26659e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_rows_df = logs_df.filter(logs_df['host'].isNull()| \n",
    "                             logs_df['timestamp'].isNull() | \n",
    "                             logs_df['method'].isNull() |\n",
    "                             logs_df['endpoint'].isNull() |\n",
    "                             logs_df['status'].isNull() |\n",
    "                             logs_df['content_size'].isNull()|\n",
    "                             logs_df['protocol'].isNull())\n",
    "bad_rows_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15c4f057",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "\n",
    "month_map = {\n",
    "  'Jan': 1, 'Feb': 2, 'Mar':3, 'Apr':4, 'May':5, 'Jun':6, 'Jul':7,\n",
    "  'Aug':8,  'Sep': 9, 'Oct':10, 'Nov': 11, 'Dec': 12\n",
    "}\n",
    "\n",
    "def parse_clf_time(text):\n",
    "    \"\"\" Convert Common Log time format into a Python datetime object\n",
    "    Args:\n",
    "        text (str): date and time in Apache time format [dd/mmm/yyyy:hh:mm:ss (+/-)zzzz]\n",
    "    Returns:\n",
    "        a string suitable for passing to CAST('timestamp')\n",
    "    \"\"\"\n",
    "    # NOTE: We're ignoring the time zones here, might need to be handled depending on the problem you are solving\n",
    "    return \"{0:04d}-{1:02d}-{2:02d} {3:02d}:{4:02d}:{5:02d}\".format(\n",
    "      int(text[7:11]),\n",
    "      month_map[text[3:6]],\n",
    "      int(text[0:2]),\n",
    "      int(text[12:14]),\n",
    "      int(text[15:17]),\n",
    "      int(text[18:20])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9b9a2b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 14:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+------+--------------------+--------+------+------------+-------------------+\n",
      "|           host|method|            endpoint|protocol|status|content_size|               time|\n",
      "+---------------+------+--------------------+--------+------+------------+-------------------+\n",
      "|  10.82.240.107|   GET|/Archives/edgar/d...|HTTP/1.0|   304|        1638|2118-12-26 12:00:00|\n",
      "|169.174.119.238|DELETE|/Archives/edgar/d...|HTTP/1.0|   303|       40118|2118-12-26 12:00:00|\n",
      "| 116.82.220.168|  POST|/Archives/edgar/d...|HTTP/1.0|   403|        5856|2118-12-26 12:00:00|\n",
      "| 99.149.129.109|  POST|/Archives/edgar/d...|HTTP/1.0|   500|       25206|2118-12-26 12:00:00|\n",
      "|  167.134.232.8|   PUT|/Archives/edgar/d...|HTTP/1.0|   200|       29727|2118-12-26 12:00:00|\n",
      "| 128.171.81.194|   GET|/Archives/edgar/d...|HTTP/1.0|   304|        3695|2118-12-26 12:00:00|\n",
      "|  74.59.229.245|   GET|/Archives/edgar/d...|HTTP/1.0|   200|       37021|2118-12-26 12:00:00|\n",
      "|198.123.235.207|DELETE|/Archives/edgar/d...|HTTP/1.0|   403|       46602|2118-12-26 12:00:00|\n",
      "|   77.26.34.100|   PUT|/Archives/edgar/d...|HTTP/1.0|   502|       50169|2118-12-26 12:00:00|\n",
      "| 83.211.223.129|  POST|/Archives/edgar/d...|HTTP/1.0|   403|       39593|2118-12-26 12:00:00|\n",
      "+---------------+------+--------------------+--------+------+------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "udf_parse_time = udf(parse_clf_time)\n",
    "\n",
    "logs_df = (logs_df.select('*', udf_parse_time(logs_df['timestamp']).cast('timestamp').alias('time')).drop('timestamp'))\n",
    "logs_df.show(10, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cab39dfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[host: string, method: string, endpoint: string, protocol: string, status: int, content_size: int, time: timestamp]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logs_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0491ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>summary</th>\n",
       "      <th>content_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>count</td>\n",
       "      <td>299999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean</td>\n",
       "      <td>30013.70617235391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>stddev</td>\n",
       "      <td>17331.474659685442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>min</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>max</td>\n",
       "      <td>60000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  summary        content_size\n",
       "0   count              299999\n",
       "1    mean   30013.70617235391\n",
       "2  stddev  17331.474659685442\n",
       "3     min                   1\n",
       "4     max               60000"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_size_summary_df = logs_df.describe(['content_size'])\n",
    "content_size_summary_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7cde7c43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min_content_size</th>\n",
       "      <th>max_content_size</th>\n",
       "      <th>mean_content_size</th>\n",
       "      <th>std_content_size</th>\n",
       "      <th>count_content_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60000</td>\n",
       "      <td>30013.706172</td>\n",
       "      <td>17331.47466</td>\n",
       "      <td>299999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   min_content_size  max_content_size  mean_content_size  std_content_size  \\\n",
       "0                 1             60000       30013.706172       17331.47466   \n",
       "\n",
       "   count_content_size  \n",
       "0              299999  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "(logs_df.agg(F.min(logs_df['content_size']).alias('min_content_size'),\n",
    "             F.max(logs_df['content_size']).alias('max_content_size'),\n",
    "             F.mean(logs_df['content_size']).alias('mean_content_size'),\n",
    "             F.stddev(logs_df['content_size']).alias('std_content_size'),\n",
    "             F.count(logs_df['content_size']).alias('count_content_size'))\n",
    "        .toPandas())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9fa49b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 24:======================================>               (142 + 8) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total distinct HTTP Status Codes: 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "status_freq_df = (logs_df\n",
    "                     .groupBy('status')\n",
    "                     .count()\n",
    "                     .sort('status')\n",
    "                     .cache())\n",
    "print('Total distinct HTTP Status Codes:', status_freq_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d534a43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|status|count|\n",
      "+------+-----+\n",
      "|   200|42914|\n",
      "|   303|42746|\n",
      "|   304|43161|\n",
      "|   403|42816|\n",
      "|   404|42824|\n",
      "|   500|42703|\n",
      "|   502|42835|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "status_freq_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4489e47f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>status</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>304</td>\n",
       "      <td>43161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>200</td>\n",
       "      <td>42914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>502</td>\n",
       "      <td>42835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>404</td>\n",
       "      <td>42824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>403</td>\n",
       "      <td>42816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>303</td>\n",
       "      <td>42746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>500</td>\n",
       "      <td>42703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   status  count\n",
       "2     304  43161\n",
       "0     200  42914\n",
       "6     502  42835\n",
       "4     404  42824\n",
       "3     403  42816\n",
       "1     303  42746\n",
       "5     500  42703"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "status_freq_pd_df = (status_freq_df\n",
    "                         .toPandas()\n",
    "                         .sort_values(by=['count'],\n",
    "                                      ascending=False))\n",
    "status_freq_pd_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aa7041ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x7fc3fb3a1820>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "sns.catplot(x='status', y='count', data=status_freq_pd_df, \n",
    "            kind='bar', order=status_freq_pd_df['status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "faca1223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+------------------+\n",
      "|status|count|        log(count)|\n",
      "+------+-----+------------------+\n",
      "|   200|42914|10.666953392005027|\n",
      "|   303|42746|10.663030902735755|\n",
      "|   304|43161|10.672692588705143|\n",
      "|   403|42816|  10.6646671434879|\n",
      "|   404|42824|10.664853972073217|\n",
      "|   500|42703|10.662024454360006|\n",
      "|   502|42835|10.665110804398145|\n",
      "+------+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_freq_df = status_freq_df.withColumn('log(count)', \n",
    "                                        F.log(status_freq_df['count']))\n",
    "log_freq_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1dddffc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x7fc3fb4f3b80>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "log_freq_pd_df = (log_freq_df\n",
    "                    .toPandas()\n",
    "                    .sort_values(by=['log(count)'],\n",
    "                                 ascending=False))\n",
    "sns.catplot(x='status', y='log(count)', data=log_freq_pd_df, \n",
    "            kind='bar', order=status_freq_pd_df['status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "179c72d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 54:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|host           |count|\n",
      "+---------------+-----+\n",
      "|144.68.35.38   |2    |\n",
      "|11.231.153.156 |2    |\n",
      "|190.183.170.214|2    |\n",
      "|208.122.146.78 |2    |\n",
      "|126.174.73.220 |2    |\n",
      "|103.136.194.163|2    |\n",
      "|33.177.184.225 |2    |\n",
      "|189.28.45.169  |2    |\n",
      "|107.214.201.42 |2    |\n",
      "|73.235.251.176 |2    |\n",
      "+---------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "host_sum_df =(logs_df\n",
    "               .groupBy('host')\n",
    "               .count()\n",
    "               .sort('count', ascending=False).limit(10))\n",
    "\n",
    "host_sum_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a65682e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>endpoint</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/Archives/edgar/data/0001157987/00011749472100...</td>\n",
       "      <td>2107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/Archives/edgar/data/0000205007/00011455492300...</td>\n",
       "      <td>2103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/Archives/edgar/data/0001179929/00011799292300...</td>\n",
       "      <td>2103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/Archives/edgar/data/1390098/00012139002302329...</td>\n",
       "      <td>2101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/Archives/edgar/data/0001179929/00011799292300...</td>\n",
       "      <td>2083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>/Archives/edgar/data/0001867102/00011046592208...</td>\n",
       "      <td>2080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>/Archives/edgar/data/0001591698/00010474691400...</td>\n",
       "      <td>2080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>/Archives/edgar/data/21665/0000930413-12-00312...</td>\n",
       "      <td>2076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>/Archives/edgar/data/0001018724/00010187242100...</td>\n",
       "      <td>2074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>/Archives/edgar/data/0001047862/00010478622200...</td>\n",
       "      <td>2065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>/Archives/edgar/data/0001326380/00013263802300...</td>\n",
       "      <td>2063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>/Archives/edgar/data/17485/0001144204-12-03009...</td>\n",
       "      <td>2061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>/Archives/edgar/data/0001035018/00011931250727...</td>\n",
       "      <td>2057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>/Archives/edgar/data/0001767306/00009324711200...</td>\n",
       "      <td>2053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>/Archives/edgar/data/0001780984/00011931252109...</td>\n",
       "      <td>2050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>/Archives/edgar/data/0001260125/00011442041901...</td>\n",
       "      <td>2048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>/Archives/edgar/data/0001608293/00011931251422...</td>\n",
       "      <td>2048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>/Archives/edgar/data/0001439124/00012928142200...</td>\n",
       "      <td>2047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>/Archives/edgar/data/0001326380/00013263802300...</td>\n",
       "      <td>2047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>/Archives/edgar/data/0001203957/00011931251022...</td>\n",
       "      <td>2042</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             endpoint  count\n",
       "0   /Archives/edgar/data/0001157987/00011749472100...   2107\n",
       "1   /Archives/edgar/data/0000205007/00011455492300...   2103\n",
       "2   /Archives/edgar/data/0001179929/00011799292300...   2103\n",
       "3   /Archives/edgar/data/1390098/00012139002302329...   2101\n",
       "4   /Archives/edgar/data/0001179929/00011799292300...   2083\n",
       "5   /Archives/edgar/data/0001867102/00011046592208...   2080\n",
       "6   /Archives/edgar/data/0001591698/00010474691400...   2080\n",
       "7   /Archives/edgar/data/21665/0000930413-12-00312...   2076\n",
       "8   /Archives/edgar/data/0001018724/00010187242100...   2074\n",
       "9   /Archives/edgar/data/0001047862/00010478622200...   2065\n",
       "10  /Archives/edgar/data/0001326380/00013263802300...   2063\n",
       "11  /Archives/edgar/data/17485/0001144204-12-03009...   2061\n",
       "12  /Archives/edgar/data/0001035018/00011931250727...   2057\n",
       "13  /Archives/edgar/data/0001767306/00009324711200...   2053\n",
       "14  /Archives/edgar/data/0001780984/00011931252109...   2050\n",
       "15  /Archives/edgar/data/0001260125/00011442041901...   2048\n",
       "16  /Archives/edgar/data/0001608293/00011931251422...   2048\n",
       "17  /Archives/edgar/data/0001439124/00012928142200...   2047\n",
       "18  /Archives/edgar/data/0001326380/00013263802300...   2047\n",
       "19  /Archives/edgar/data/0001203957/00011931251022...   2042"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths_df = (logs_df\n",
    "            .groupBy('endpoint')\n",
    "            .count()\n",
    "            .sort('count', ascending=False).limit(20))\n",
    "\n",
    "paths_pd_df = paths_df.toPandas()\n",
    "paths_pd_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5af00ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------+-----+\n",
      "|endpoint                                                                   |count|\n",
      "+---------------------------------------------------------------------------+-----+\n",
      "|/Archives/edgar/data/0000205007/000114554923005577/0001145549-23-005577.txt|1808 |\n",
      "|/Archives/edgar/data/0001157987/000117494721000301/form10k-25693_czn1.htm  |1807 |\n",
      "|/Archives/edgar/data/1390098/000121390023023295/0001213900-23-023295.txt   |1804 |\n",
      "|/Archives/edgar/data/17485/0001144204-12-030095.txt                        |1792 |\n",
      "|/Archives/edgar/data/0001242615/000120919108049247/xslF345X02/doc3.xml     |1790 |\n",
      "|/Archives/edgar/data/21665/0000930413-12-003127.txt                        |1788 |\n",
      "|/Archives/edgar/data/0001179929/000117992923000043/moh-20230320_g29.jpg    |1788 |\n",
      "|/Archives/edgar/data/0001867102/000110465922086725/                        |1785 |\n",
      "|/Archives/edgar/data/0001591698/000104746914000931/0001047469-14-000931.txt|1775 |\n",
      "|/Archives/edgar/data/0001439124/000129281422000788/                        |1767 |\n",
      "+---------------------------------------------------------------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "not200_df = (logs_df\n",
    "               .filter(logs_df['status'] != 200))\n",
    "\n",
    "error_endpoints_freq_df = (not200_df\n",
    "                               .groupBy('endpoint')\n",
    "                               .count()\n",
    "                               .sort('count', ascending=False)\n",
    "                               .limit(10)\n",
    "                          )\n",
    "                          \n",
    "error_endpoints_freq_df.show(truncate=False)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f445a66d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "299988"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_host_count = (logs_df\n",
    "                     .select('host')\n",
    "                     .distinct()\n",
    "                     .count())\n",
    "unique_host_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b79854af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---+\n",
      "|host           |day|\n",
      "+---------------+---+\n",
      "|10.82.240.107  |26 |\n",
      "|169.174.119.238|26 |\n",
      "|116.82.220.168 |26 |\n",
      "|99.149.129.109 |26 |\n",
      "|167.134.232.8  |26 |\n",
      "+---------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "host_day_df = logs_df.select(logs_df.host, \n",
    "                             F.dayofmonth('time').alias('day'))\n",
    "host_day_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e950e2d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 70:=======>                                                  (1 + 7) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---+\n",
      "|host           |day|\n",
      "+---------------+---+\n",
      "|179.223.117.124|26 |\n",
      "|120.254.156.253|26 |\n",
      "|168.160.209.72 |26 |\n",
      "|57.254.64.81   |26 |\n",
      "|60.91.64.246   |26 |\n",
      "+---------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "host_day_distinct_df = (host_day_df\n",
    "                          .dropDuplicates())\n",
    "host_day_distinct_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c0c9d69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26</td>\n",
       "      <td>299988</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   day   count\n",
       "0   26  299988"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def_mr = pd.get_option('max_rows')\n",
    "# pd.set_option('max_rows', 10)\n",
    "\n",
    "daily_hosts_df = (host_day_distinct_df\n",
    "                     .groupBy('day')\n",
    "                     .count()\n",
    "                     .sort(\"day\"))\n",
    "\n",
    "daily_hosts_df = daily_hosts_df.toPandas()\n",
    "daily_hosts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "30bd110d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 750x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "c = sns.catplot(x='day', y='count', \n",
    "                data=daily_hosts_df, \n",
    "                kind='point', height=5, \n",
    "                aspect=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "398112d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 404 responses: 42824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 86:=======>                                                  (1 + 7) / 8]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "not_found_df = logs_df.filter(logs_df[\"status\"] == 404).cache()\n",
    "print(('Total 404 responses: {}').format(not_found_df.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dcdb1e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------+-----+\n",
      "|endpoint                                                                      |count|\n",
      "+------------------------------------------------------------------------------+-----+\n",
      "|/Archives/edgar/data/21665/0000930413-12-003127.txt                           |328  |\n",
      "|/Archives/edgar/data/0001179929/000117992923000043/moh-20230320_g24.jpg       |325  |\n",
      "|/Archives/edgar/data/0000352825/000035282519000012/0000352825-19-000012.txt   |325  |\n",
      "|/Archives/edgar/data/0001801390/000121390021017183/s131106_10k.htm            |324  |\n",
      "|/Archives/edgar/data/0001671697/000092963819000390/form10k.htm                |322  |\n",
      "|/Archives/edgar/data/0001242699/000143774911009194/0001437749-11-009194.txt   |321  |\n",
      "|/Archives/edgar/data/0001157987/000117494721000301/form10k-25693_czn1.htm     |321  |\n",
      "|/Archives/edgar/data/0000004281/000119312517062657/R99.htm                    |316  |\n",
      "|/Archives/edgar/data/0001330427/000093247112005362/0000932471-12-005362.txt   |316  |\n",
      "|/Archives/edgar/data/0000788606/000093247112005362/0000932471-12-005362.txt   |314  |\n",
      "|/Archives/edgar/data/0001852440/000000000022002912/filename2.txt              |310  |\n",
      "|/Archives/edgar/data/35315/000175272423067615/primary_doc.xml                 |310  |\n",
      "|/Archives/edgar/data/27419/0001104659-12-037689.txt                           |309  |\n",
      "|/Archives/edgar/data/0001285785/000118143108058983/xslF345X03/rrd221762.xml   |308  |\n",
      "|/Archives/edgar/data/0000318833/000035420421000574/SEC13G_Filing.htm          |308  |\n",
      "|/Archives/edgar/data/0000862341/000093247112005362/0000932471-12-005362.txt   |308  |\n",
      "|/Archives/edgar/data/1356093/000088430008000018/0000884300-08-000018.txt      |308  |\n",
      "|/Archives/edgar/data/1537583/999999999720003444/9999999997-20-003444-index.htm|306  |\n",
      "|/Archives/edgar/data/0000004281/000119312517062657/R100.htm                   |306  |\n",
      "|/Archives/edgar/data/0001491487/000114420415051928/0001144204-15-051928.txt   |306  |\n",
      "+------------------------------------------------------------------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "endpoints_404_count_df = (not_found_df\n",
    "                          .groupBy(\"endpoint\")\n",
    "                          .count()\n",
    "                          .sort(\"count\", ascending=False)\n",
    "                          .limit(20))\n",
    "\n",
    "endpoints_404_count_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d618594a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+\n",
      "|host           |count|\n",
      "+---------------+-----+\n",
      "|46.49.74.5     |1    |\n",
      "|62.98.163.240  |1    |\n",
      "|151.9.24.39    |1    |\n",
      "|21.121.27.246  |1    |\n",
      "|201.191.249.168|1    |\n",
      "|215.37.109.179 |1    |\n",
      "|146.33.44.163  |1    |\n",
      "|60.248.156.46  |1    |\n",
      "|104.97.143.248 |1    |\n",
      "|119.66.156.165 |1    |\n",
      "|71.234.35.22   |1    |\n",
      "|133.54.41.150  |1    |\n",
      "|209.195.69.167 |1    |\n",
      "|179.19.171.27  |1    |\n",
      "|58.61.175.89   |1    |\n",
      "|119.75.162.84  |1    |\n",
      "|158.71.19.129  |1    |\n",
      "|101.188.122.74 |1    |\n",
      "|191.130.6.173  |1    |\n",
      "|72.216.194.228 |1    |\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hosts_404_count_df = (not_found_df\n",
    "                          .groupBy(\"host\")\n",
    "                          .count()\n",
    "                          .sort(\"count\", ascending=False)\n",
    "                          .limit(20))\n",
    "\n",
    "hosts_404_count_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "763cdc1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 750x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hourly_avg_errors_sorted_df = (not_found_df\n",
    "                                   .groupBy(F.hour('time')\n",
    "                                             .alias('hour'))\n",
    "                                   .count()\n",
    "                                   .sort('hour'))\n",
    "hourly_avg_errors_sorted_pd_df = hourly_avg_errors_sorted_df.toPandas()\n",
    "\n",
    "c = sns.catplot(x='hour', y='count', \n",
    "                data=hourly_avg_errors_sorted_pd_df, \n",
    "                kind='bar', height=5, aspect=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "30836c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module pyspark.sql.functions in pyspark.sql:\n",
      "\n",
      "NAME\n",
      "    pyspark.sql.functions - A collections of builtin functions\n",
      "\n",
      "FUNCTIONS\n",
      "    abs(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes the absolute value.\n",
      "        \n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(abs(lit(-1))).show()\n",
      "        +-------+\n",
      "        |abs(-1)|\n",
      "        +-------+\n",
      "        |      1|\n",
      "        +-------+\n",
      "    \n",
      "    acos(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes inverse cosine of the input column.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            inverse cosine of `col`, as if computed by `java.lang.Math.acos()`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1, 3)\n",
      "        >>> df.select(acos(df.id)).show()\n",
      "        +--------+\n",
      "        |ACOS(id)|\n",
      "        +--------+\n",
      "        |     0.0|\n",
      "        |     NaN|\n",
      "        +--------+\n",
      "    \n",
      "    acosh(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes inverse hyperbolic cosine of the input column.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(2)\n",
      "        >>> df.select(acosh(col(\"id\"))).show()\n",
      "        +---------+\n",
      "        |ACOSH(id)|\n",
      "        +---------+\n",
      "        |      NaN|\n",
      "        |      0.0|\n",
      "        +---------+\n",
      "    \n",
      "    add_months(start: 'ColumnOrName', months: Union[ForwardRef('ColumnOrName'), int]) -> pyspark.sql.column.Column\n",
      "        Returns the date that is `months` months after `start`. If `months` is a negative value\n",
      "        then these amount of months will be deducted from the `start`.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        start : :class:`~pyspark.sql.Column` or str\n",
      "            date column to work on.\n",
      "        months : :class:`~pyspark.sql.Column` or str or int\n",
      "            how many months after the given date to calculate.\n",
      "            Accepts negative value as well to calculate backwards.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a date after/before given number of months.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08', 2)], ['dt', 'add'])\n",
      "        >>> df.select(add_months(df.dt, 1).alias('next_month')).collect()\n",
      "        [Row(next_month=datetime.date(2015, 5, 8))]\n",
      "        >>> df.select(add_months(df.dt, df.add.cast('integer')).alias('next_month')).collect()\n",
      "        [Row(next_month=datetime.date(2015, 6, 8))]\n",
      "        >>> df.select(add_months('dt', -2).alias('prev_month')).collect()\n",
      "        [Row(prev_month=datetime.date(2015, 2, 8))]\n",
      "    \n",
      "    aggregate(col: 'ColumnOrName', initialValue: 'ColumnOrName', merge: Callable[[pyspark.sql.column.Column, pyspark.sql.column.Column], pyspark.sql.column.Column], finish: Optional[Callable[[pyspark.sql.column.Column], pyspark.sql.column.Column]] = None) -> pyspark.sql.column.Column\n",
      "        Applies a binary operator to an initial state and all elements in the array,\n",
      "        and reduces this to a single state. The final state is converted into the final result\n",
      "        by applying a finish function.\n",
      "        \n",
      "        Both functions can use methods of :class:`~pyspark.sql.Column`, functions defined in\n",
      "        :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\n",
      "        Python ``UserDefinedFunctions`` are not supported\n",
      "        (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        initialValue : :class:`~pyspark.sql.Column` or str\n",
      "            initial value. Name of column or expression\n",
      "        merge : function\n",
      "            a binary function ``(acc: Column, x: Column) -> Column...`` returning expression\n",
      "            of the same type as ``zero``\n",
      "        finish : function\n",
      "            an optional unary function ``(x: Column) -> Column: ...``\n",
      "            used to convert accumulated value.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            final value after aggregate function is applied.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1, [20.0, 4.0, 2.0, 6.0, 10.0])], (\"id\", \"values\"))\n",
      "        >>> df.select(aggregate(\"values\", lit(0.0), lambda acc, x: acc + x).alias(\"sum\")).show()\n",
      "        +----+\n",
      "        | sum|\n",
      "        +----+\n",
      "        |42.0|\n",
      "        +----+\n",
      "        \n",
      "        >>> def merge(acc, x):\n",
      "        ...     count = acc.count + 1\n",
      "        ...     sum = acc.sum + x\n",
      "        ...     return struct(count.alias(\"count\"), sum.alias(\"sum\"))\n",
      "        >>> df.select(\n",
      "        ...     aggregate(\n",
      "        ...         \"values\",\n",
      "        ...         struct(lit(0).alias(\"count\"), lit(0.0).alias(\"sum\")),\n",
      "        ...         merge,\n",
      "        ...         lambda acc: acc.sum / acc.count,\n",
      "        ...     ).alias(\"mean\")\n",
      "        ... ).show()\n",
      "        +----+\n",
      "        |mean|\n",
      "        +----+\n",
      "        | 8.4|\n",
      "        +----+\n",
      "    \n",
      "    approxCountDistinct(col: 'ColumnOrName', rsd: Optional[float] = None) -> pyspark.sql.column.Column\n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        .. deprecated:: 2.1.0\n",
      "            Use :func:`approx_count_distinct` instead.\n",
      "    \n",
      "    approx_count_distinct(col: 'ColumnOrName', rsd: Optional[float] = None) -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns a new :class:`~pyspark.sql.Column` for approximate distinct count\n",
      "        of column `col`.\n",
      "        \n",
      "        .. versionadded:: 2.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "        rsd : float, optional\n",
      "            maximum relative standard deviation allowed (default = 0.05).\n",
      "            For rsd < 0.01, it is more efficient to use :func:`count_distinct`\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column of computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([1,2,2,3], \"INT\")\n",
      "        >>> df.agg(approx_count_distinct(\"value\").alias('distinct_values')).show()\n",
      "        +---------------+\n",
      "        |distinct_values|\n",
      "        +---------------+\n",
      "        |              3|\n",
      "        +---------------+\n",
      "    \n",
      "    array(*cols: Union[ForwardRef('ColumnOrName'), List[ForwardRef('ColumnOrName_')], Tuple[ForwardRef('ColumnOrName_'), ...]]) -> pyspark.sql.column.Column\n",
      "        Creates a new array column.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        cols : :class:`~pyspark.sql.Column` or str\n",
      "            column names or :class:`~pyspark.sql.Column`\\s that have\n",
      "            the same data type.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a column of array type.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"Alice\", 2), (\"Bob\", 5)], (\"name\", \"age\"))\n",
      "        >>> df.select(array('age', 'age').alias(\"arr\")).collect()\n",
      "        [Row(arr=[2, 2]), Row(arr=[5, 5])]\n",
      "        >>> df.select(array([df.age, df.age]).alias(\"arr\")).collect()\n",
      "        [Row(arr=[2, 2]), Row(arr=[5, 5])]\n",
      "        >>> df.select(array('age', 'age').alias(\"col\")).printSchema()\n",
      "        root\n",
      "         |-- col: array (nullable = false)\n",
      "         |    |-- element: long (containsNull = true)\n",
      "    \n",
      "    array_append(col: 'ColumnOrName', value: Any) -> pyspark.sql.column.Column\n",
      "        Collection function: returns an array of the elements in col1 along\n",
      "        with the added element in col2 at the last of the array.\n",
      "        \n",
      "        .. versionadded:: 3.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing array\n",
      "        value :\n",
      "            a literal value, or a :class:`~pyspark.sql.Column` expression.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            an array of values from first array along with the element.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Row\n",
      "        >>> df = spark.createDataFrame([Row(c1=[\"b\", \"a\", \"c\"], c2=\"c\")])\n",
      "        >>> df.select(array_append(df.c1, df.c2)).collect()\n",
      "        [Row(array_append(c1, c2)=['b', 'a', 'c', 'c'])]\n",
      "        >>> df.select(array_append(df.c1, 'x')).collect()\n",
      "        [Row(array_append(c1, x)=['b', 'a', 'c', 'x'])]\n",
      "    \n",
      "    array_compact(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Collection function: removes null values from the array.\n",
      "        \n",
      "        .. versionadded:: 3.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            an array by exluding the null values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([1, None, 2, 3],), ([4, 5, None, 4],)], ['data'])\n",
      "        >>> df.select(array_compact(df.data)).collect()\n",
      "        [Row(array_compact(data)=[1, 2, 3]), Row(array_compact(data)=[4, 5, 4])]\n",
      "    \n",
      "    array_contains(col: 'ColumnOrName', value: Any) -> pyspark.sql.column.Column\n",
      "        Collection function: returns null if the array is null, true if the array contains the\n",
      "        given value, and false otherwise.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing array\n",
      "        value :\n",
      "            value or column to check for in array\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a column of Boolean type.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([\"a\", \"b\", \"c\"],), ([],)], ['data'])\n",
      "        >>> df.select(array_contains(df.data, \"a\")).collect()\n",
      "        [Row(array_contains(data, a)=True), Row(array_contains(data, a)=False)]\n",
      "        >>> df.select(array_contains(df.data, lit(\"a\"))).collect()\n",
      "        [Row(array_contains(data, a)=True), Row(array_contains(data, a)=False)]\n",
      "    \n",
      "    array_distinct(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Collection function: removes duplicate values from the array.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            an array of unique values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([1, 2, 3, 2],), ([4, 5, 5, 4],)], ['data'])\n",
      "        >>> df.select(array_distinct(df.data)).collect()\n",
      "        [Row(array_distinct(data)=[1, 2, 3]), Row(array_distinct(data)=[4, 5])]\n",
      "    \n",
      "    array_except(col1: 'ColumnOrName', col2: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Collection function: returns an array of the elements in col1 but not in col2,\n",
      "        without duplicates.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col1 : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing array\n",
      "        col2 : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing array\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            an array of values from first array that are not in the second.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Row\n",
      "        >>> df = spark.createDataFrame([Row(c1=[\"b\", \"a\", \"c\"], c2=[\"c\", \"d\", \"a\", \"f\"])])\n",
      "        >>> df.select(array_except(df.c1, df.c2)).collect()\n",
      "        [Row(array_except(c1, c2)=['b'])]\n",
      "    \n",
      "    array_insert(arr: 'ColumnOrName', pos: Union[ForwardRef('ColumnOrName'), int], value: Any) -> pyspark.sql.column.Column\n",
      "        Collection function: adds an item into a given array at a specified array index.\n",
      "        Array indices start at 1, or start from the end if index is negative.\n",
      "        Index above array size appends the array, or prepends the array if index is negative,\n",
      "        with 'null' elements.\n",
      "        \n",
      "        .. versionadded:: 3.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        arr : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing an array\n",
      "        pos : :class:`~pyspark.sql.Column` or str or int\n",
      "            name of Numeric type column indicating position of insertion\n",
      "            (starting at index 1, negative position is a start from the back of the array)\n",
      "        value :\n",
      "            a literal value, or a :class:`~pyspark.sql.Column` expression.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            an array of values, including the new specified value\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame(\n",
      "        ...     [(['a', 'b', 'c'], 2, 'd'), (['c', 'b', 'a'], -2, 'd')],\n",
      "        ...     ['data', 'pos', 'val']\n",
      "        ... )\n",
      "        >>> df.select(array_insert(df.data, df.pos.cast('integer'), df.val).alias('data')).collect()\n",
      "        [Row(data=['a', 'd', 'b', 'c']), Row(data=['c', 'd', 'b', 'a'])]\n",
      "        >>> df.select(array_insert(df.data, 5, 'hello').alias('data')).collect()\n",
      "        [Row(data=['a', 'b', 'c', None, 'hello']), Row(data=['c', 'b', 'a', None, 'hello'])]\n",
      "    \n",
      "    array_intersect(col1: 'ColumnOrName', col2: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Collection function: returns an array of the elements in the intersection of col1 and col2,\n",
      "        without duplicates.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col1 : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing array\n",
      "        col2 : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing array\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            an array of values in the intersection of two arrays.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Row\n",
      "        >>> df = spark.createDataFrame([Row(c1=[\"b\", \"a\", \"c\"], c2=[\"c\", \"d\", \"a\", \"f\"])])\n",
      "        >>> df.select(array_intersect(df.c1, df.c2)).collect()\n",
      "        [Row(array_intersect(c1, c2)=['a', 'c'])]\n",
      "    \n",
      "    array_join(col: 'ColumnOrName', delimiter: str, null_replacement: Optional[str] = None) -> pyspark.sql.column.Column\n",
      "        Concatenates the elements of `column` using the `delimiter`. Null values are replaced with\n",
      "        `null_replacement` if set, otherwise they are ignored.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        delimiter : str\n",
      "            delimiter used to concatenate elements\n",
      "        null_replacement : str, optional\n",
      "            if set then null values will be replaced by this value\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a column of string type. Concatenated values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([\"a\", \"b\", \"c\"],), ([\"a\", None],)], ['data'])\n",
      "        >>> df.select(array_join(df.data, \",\").alias(\"joined\")).collect()\n",
      "        [Row(joined='a,b,c'), Row(joined='a')]\n",
      "        >>> df.select(array_join(df.data, \",\", \"NULL\").alias(\"joined\")).collect()\n",
      "        [Row(joined='a,b,c'), Row(joined='a,NULL')]\n",
      "    \n",
      "    array_max(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Collection function: returns the maximum value of the array.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            maximum value of an array.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([2, 1, 3],), ([None, 10, -1],)], ['data'])\n",
      "        >>> df.select(array_max(df.data).alias('max')).collect()\n",
      "        [Row(max=3), Row(max=10)]\n",
      "    \n",
      "    array_min(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Collection function: returns the minimum value of the array.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            minimum value of array.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([2, 1, 3],), ([None, 10, -1],)], ['data'])\n",
      "        >>> df.select(array_min(df.data).alias('min')).collect()\n",
      "        [Row(min=1), Row(min=-1)]\n",
      "    \n",
      "    array_position(col: 'ColumnOrName', value: Any) -> pyspark.sql.column.Column\n",
      "        Collection function: Locates the position of the first occurrence of the given value\n",
      "        in the given array. Returns null if either of the arguments are null.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The position is not zero based, but 1 based index. Returns 0 if the given\n",
      "        value could not be found in the array.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        value : Any\n",
      "            value to look for.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            position of the value in the given array if found and 0 otherwise.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([\"c\", \"b\", \"a\"],), ([],)], ['data'])\n",
      "        >>> df.select(array_position(df.data, \"a\")).collect()\n",
      "        [Row(array_position(data, a)=3), Row(array_position(data, a)=0)]\n",
      "    \n",
      "    array_remove(col: 'ColumnOrName', element: Any) -> pyspark.sql.column.Column\n",
      "        Collection function: Remove all elements that equal to element from the given array.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing array\n",
      "        element :\n",
      "            element to be removed from the array\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            an array excluding given value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([1, 2, 3, 1, 1],), ([],)], ['data'])\n",
      "        >>> df.select(array_remove(df.data, 1)).collect()\n",
      "        [Row(array_remove(data, 1)=[2, 3]), Row(array_remove(data, 1)=[])]\n",
      "    \n",
      "    array_repeat(col: 'ColumnOrName', count: Union[ForwardRef('ColumnOrName'), int]) -> pyspark.sql.column.Column\n",
      "        Collection function: creates an array containing a column repeated count times.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            column name or column that contains the element to be repeated\n",
      "        count : :class:`~pyspark.sql.Column` or str or int\n",
      "            column name, column, or int containing the number of times to repeat the first argument\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            an array of repeated elements.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('ab',)], ['data'])\n",
      "        >>> df.select(array_repeat(df.data, 3).alias('r')).collect()\n",
      "        [Row(r=['ab', 'ab', 'ab'])]\n",
      "    \n",
      "    array_sort(col: 'ColumnOrName', comparator: Optional[Callable[[pyspark.sql.column.Column, pyspark.sql.column.Column], pyspark.sql.column.Column]] = None) -> pyspark.sql.column.Column\n",
      "        Collection function: sorts the input array in ascending order. The elements of the input array\n",
      "        must be orderable. Null elements will be placed at the end of the returned array.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Can take a `comparator` function.\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        comparator : callable, optional\n",
      "            A binary ``(Column, Column) -> Column: ...``.\n",
      "            The comparator will take two\n",
      "            arguments representing two elements of the array. It returns a negative integer, 0, or a\n",
      "            positive integer as the first element is less than, equal to, or greater than the second\n",
      "            element. If the comparator function returns null, the function will fail and raise an error.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            sorted array.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([2, 1, None, 3],),([1],),([],)], ['data'])\n",
      "        >>> df.select(array_sort(df.data).alias('r')).collect()\n",
      "        [Row(r=[1, 2, 3, None]), Row(r=[1]), Row(r=[])]\n",
      "        >>> df = spark.createDataFrame([([\"foo\", \"foobar\", None, \"bar\"],),([\"foo\"],),([],)], ['data'])\n",
      "        >>> df.select(array_sort(\n",
      "        ...     \"data\",\n",
      "        ...     lambda x, y: when(x.isNull() | y.isNull(), lit(0)).otherwise(length(y) - length(x))\n",
      "        ... ).alias(\"r\")).collect()\n",
      "        [Row(r=['foobar', 'foo', None, 'bar']), Row(r=['foo']), Row(r=[])]\n",
      "    \n",
      "    array_union(col1: 'ColumnOrName', col2: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Collection function: returns an array of the elements in the union of col1 and col2,\n",
      "        without duplicates.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col1 : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing array\n",
      "        col2 : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing array\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            an array of values in union of two arrays.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Row\n",
      "        >>> df = spark.createDataFrame([Row(c1=[\"b\", \"a\", \"c\"], c2=[\"c\", \"d\", \"a\", \"f\"])])\n",
      "        >>> df.select(array_union(df.c1, df.c2)).collect()\n",
      "        [Row(array_union(c1, c2)=['b', 'a', 'c', 'd', 'f'])]\n",
      "    \n",
      "    arrays_overlap(a1: 'ColumnOrName', a2: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Collection function: returns true if the arrays contain any common non-null element; if not,\n",
      "        returns null if both the arrays are non-empty and any of them contains a null element; returns\n",
      "        false otherwise.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a column of Boolean type.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([\"a\", \"b\"], [\"b\", \"c\"]), ([\"a\"], [\"b\", \"c\"])], ['x', 'y'])\n",
      "        >>> df.select(arrays_overlap(df.x, df.y).alias(\"overlap\")).collect()\n",
      "        [Row(overlap=True), Row(overlap=False)]\n",
      "    \n",
      "    arrays_zip(*cols: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Collection function: Returns a merged array of structs in which the N-th struct contains all\n",
      "        N-th values of input arrays. If one of the arrays is shorter than others then\n",
      "        resulting struct type value will be a `null` for missing elements.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        cols : :class:`~pyspark.sql.Column` or str\n",
      "            columns of arrays to be merged.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            merged array of entries.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.functions import arrays_zip\n",
      "        >>> df = spark.createDataFrame([(([1, 2, 3], [2, 4, 6], [3, 6]))], ['vals1', 'vals2', 'vals3'])\n",
      "        >>> df = df.select(arrays_zip(df.vals1, df.vals2, df.vals3).alias('zipped'))\n",
      "        >>> df.show(truncate=False)\n",
      "        +------------------------------------+\n",
      "        |zipped                              |\n",
      "        +------------------------------------+\n",
      "        |[{1, 2, 3}, {2, 4, 6}, {3, 6, null}]|\n",
      "        +------------------------------------+\n",
      "        >>> df.printSchema()\n",
      "        root\n",
      "         |-- zipped: array (nullable = true)\n",
      "         |    |-- element: struct (containsNull = false)\n",
      "         |    |    |-- vals1: long (nullable = true)\n",
      "         |    |    |-- vals2: long (nullable = true)\n",
      "         |    |    |-- vals3: long (nullable = true)\n",
      "    \n",
      "    asc(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a sort expression based on the ascending order of the given column name.\n",
      "        \n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to sort by in the ascending order.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column specifying the order.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Sort by the column 'id' in the descending order.\n",
      "        \n",
      "        >>> df = spark.range(5)\n",
      "        >>> df = df.sort(desc(\"id\"))\n",
      "        >>> df.show()\n",
      "        +---+\n",
      "        | id|\n",
      "        +---+\n",
      "        |  4|\n",
      "        |  3|\n",
      "        |  2|\n",
      "        |  1|\n",
      "        |  0|\n",
      "        +---+\n",
      "        \n",
      "        Sort by the column 'id' in the ascending order.\n",
      "        \n",
      "        >>> df.orderBy(asc(\"id\")).show()\n",
      "        +---+\n",
      "        | id|\n",
      "        +---+\n",
      "        |  0|\n",
      "        |  1|\n",
      "        |  2|\n",
      "        |  3|\n",
      "        |  4|\n",
      "        +---+\n",
      "    \n",
      "    asc_nulls_first(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a sort expression based on the ascending order of the given\n",
      "        column name, and null values return before non-null values.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to sort by in the ascending order.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column specifying the order.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df1 = spark.createDataFrame([(1, \"Bob\"),\n",
      "        ...                              (0, None),\n",
      "        ...                              (2, \"Alice\")], [\"age\", \"name\"])\n",
      "        >>> df1.sort(asc_nulls_first(df1.name)).show()\n",
      "        +---+-----+\n",
      "        |age| name|\n",
      "        +---+-----+\n",
      "        |  0| null|\n",
      "        |  2|Alice|\n",
      "        |  1|  Bob|\n",
      "        +---+-----+\n",
      "    \n",
      "    asc_nulls_last(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a sort expression based on the ascending order of the given\n",
      "        column name, and null values appear after non-null values.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to sort by in the ascending order.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column specifying the order.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df1 = spark.createDataFrame([(0, None),\n",
      "        ...                              (1, \"Bob\"),\n",
      "        ...                              (2, \"Alice\")], [\"age\", \"name\"])\n",
      "        >>> df1.sort(asc_nulls_last(df1.name)).show()\n",
      "        +---+-----+\n",
      "        |age| name|\n",
      "        +---+-----+\n",
      "        |  2|Alice|\n",
      "        |  1|  Bob|\n",
      "        |  0| null|\n",
      "        +---+-----+\n",
      "    \n",
      "    ascii(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes the numeric value of the first character of the string column.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            numeric value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([\"Spark\", \"PySpark\", \"Pandas API\"], \"STRING\")\n",
      "        >>> df.select(ascii(\"value\")).show()\n",
      "        +------------+\n",
      "        |ascii(value)|\n",
      "        +------------+\n",
      "        |          83|\n",
      "        |          80|\n",
      "        |          80|\n",
      "        +------------+\n",
      "    \n",
      "    asin(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes inverse sine of the input column.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            inverse sine of `col`, as if computed by `java.lang.Math.asin()`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(0,), (2,)])\n",
      "        >>> df.select(asin(df.schema.fieldNames()[0])).show()\n",
      "        +--------+\n",
      "        |ASIN(_1)|\n",
      "        +--------+\n",
      "        |     0.0|\n",
      "        |     NaN|\n",
      "        +--------+\n",
      "    \n",
      "    asinh(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes inverse hyperbolic sine of the input column.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(asinh(col(\"id\"))).show()\n",
      "        +---------+\n",
      "        |ASINH(id)|\n",
      "        +---------+\n",
      "        |      0.0|\n",
      "        +---------+\n",
      "    \n",
      "    assert_true(col: 'ColumnOrName', errMsg: Union[pyspark.sql.column.Column, str, NoneType] = None) -> pyspark.sql.column.Column\n",
      "        Returns `null` if the input column is `true`; throws an exception\n",
      "        with the provided error message otherwise.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            column name or column that represents the input column to test\n",
      "        errMsg : :class:`~pyspark.sql.Column` or str, optional\n",
      "            A Python string literal or column containing the error message\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            `null` if the input column is `true` otherwise throws an error with specified message.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(0,1)], ['a', 'b'])\n",
      "        >>> df.select(assert_true(df.a < df.b).alias('r')).collect()\n",
      "        [Row(r=None)]\n",
      "        >>> df.select(assert_true(df.a < df.b, df.a).alias('r')).collect()\n",
      "        [Row(r=None)]\n",
      "        >>> df.select(assert_true(df.a < df.b, 'error').alias('r')).collect()\n",
      "        [Row(r=None)]\n",
      "        >>> df.select(assert_true(df.a > df.b, 'My error msg').alias('r')).collect() # doctest: +SKIP\n",
      "        ...\n",
      "        java.lang.RuntimeException: My error msg\n",
      "        ...\n",
      "    \n",
      "    atan(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Compute inverse tangent of the input column.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            inverse tangent of `col`, as if computed by `java.lang.Math.atan()`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(atan(df.id)).show()\n",
      "        +--------+\n",
      "        |ATAN(id)|\n",
      "        +--------+\n",
      "        |     0.0|\n",
      "        +--------+\n",
      "    \n",
      "    atan2(col1: Union[ForwardRef('ColumnOrName'), float], col2: Union[ForwardRef('ColumnOrName'), float]) -> pyspark.sql.column.Column\n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col1 : str, :class:`~pyspark.sql.Column` or float\n",
      "            coordinate on y-axis\n",
      "        col2 : str, :class:`~pyspark.sql.Column` or float\n",
      "            coordinate on x-axis\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the `theta` component of the point\n",
      "            (`r`, `theta`)\n",
      "            in polar coordinates that corresponds to the point\n",
      "            (`x`, `y`) in Cartesian coordinates,\n",
      "            as if computed by `java.lang.Math.atan2()`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(atan2(lit(1), lit(2))).first()\n",
      "        Row(ATAN2(1, 2)=0.46364...)\n",
      "    \n",
      "    atanh(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes inverse hyperbolic tangent of the input column.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(0,), (2,)], schema=[\"numbers\"])\n",
      "        >>> df.select(atanh(df[\"numbers\"])).show()\n",
      "        +--------------+\n",
      "        |ATANH(numbers)|\n",
      "        +--------------+\n",
      "        |           0.0|\n",
      "        |           NaN|\n",
      "        +--------------+\n",
      "    \n",
      "    avg(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the average of the values in a group.\n",
      "        \n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(10)\n",
      "        >>> df.select(avg(col(\"id\"))).show()\n",
      "        +-------+\n",
      "        |avg(id)|\n",
      "        +-------+\n",
      "        |    4.5|\n",
      "        +-------+\n",
      "    \n",
      "    base64(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes the BASE64 encoding of a binary column and returns it as a string column.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            BASE64 encoding of string value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([\"Spark\", \"PySpark\", \"Pandas API\"], \"STRING\")\n",
      "        >>> df.select(base64(\"value\")).show()\n",
      "        +----------------+\n",
      "        |   base64(value)|\n",
      "        +----------------+\n",
      "        |        U3Bhcms=|\n",
      "        |    UHlTcGFyaw==|\n",
      "        |UGFuZGFzIEFQSQ==|\n",
      "        +----------------+\n",
      "    \n",
      "    bin(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the string representation of the binary value of the given column.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            binary representation of given value as string.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([2,5], \"INT\")\n",
      "        >>> df.select(bin(df.value).alias('c')).collect()\n",
      "        [Row(c='10'), Row(c='101')]\n",
      "    \n",
      "    bit_length(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Calculates the bit length for the specified string column.\n",
      "        \n",
      "        .. versionadded:: 3.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            Source column or strings\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            Bit length of the col\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.functions import bit_length\n",
      "        >>> spark.createDataFrame([('cat',), ( '🐈',)], ['cat']) \\\n",
      "        ...      .select(bit_length('cat')).collect()\n",
      "            [Row(bit_length(cat)=24), Row(bit_length(cat)=32)]\n",
      "    \n",
      "    bitwiseNOT(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes bitwise not.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        .. deprecated:: 3.2.0\n",
      "            Use :func:`bitwise_not` instead.\n",
      "    \n",
      "    bitwise_not(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes bitwise not.\n",
      "        \n",
      "        .. versionadded:: 3.2.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(bitwise_not(lit(0))).show()\n",
      "        +---+\n",
      "        | ~0|\n",
      "        +---+\n",
      "        | -1|\n",
      "        +---+\n",
      "        >>> df.select(bitwise_not(lit(1))).show()\n",
      "        +---+\n",
      "        | ~1|\n",
      "        +---+\n",
      "        | -2|\n",
      "        +---+\n",
      "    \n",
      "    broadcast(df: pyspark.sql.dataframe.DataFrame) -> pyspark.sql.dataframe.DataFrame\n",
      "        Marks a DataFrame as small enough for use in broadcast joins.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.DataFrame`\n",
      "            DataFrame marked as ready for broadcast join.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import types\n",
      "        >>> df = spark.createDataFrame([1, 2, 3, 3, 4], types.IntegerType())\n",
      "        >>> df_small = spark.range(3)\n",
      "        >>> df_b = broadcast(df_small)\n",
      "        >>> df.join(df_b, df.value == df_small.id).show()\n",
      "        +-----+---+\n",
      "        |value| id|\n",
      "        +-----+---+\n",
      "        |    1|  1|\n",
      "        |    2|  2|\n",
      "        +-----+---+\n",
      "    \n",
      "    bround(col: 'ColumnOrName', scale: int = 0) -> pyspark.sql.column.Column\n",
      "        Round the given value to `scale` decimal places using HALF_EVEN rounding mode if `scale` >= 0\n",
      "        or at integral part when `scale` < 0.\n",
      "        \n",
      "        .. versionadded:: 2.0.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            input column to round.\n",
      "        scale : int optional default 0\n",
      "            scale value.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            rounded values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.createDataFrame([(2.5,)], ['a']).select(bround('a', 0).alias('r')).collect()\n",
      "        [Row(r=2.0)]\n",
      "    \n",
      "    bucket(numBuckets: Union[pyspark.sql.column.Column, int], col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Partition transform function: A transform for any type that partitions\n",
      "        by a hash of the input column.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.writeTo(\"catalog.db.table\").partitionedBy(  # doctest: +SKIP\n",
      "        ...     bucket(42, \"ts\")\n",
      "        ... ).createOrReplace()\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target date or timestamp column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            data partitioned by given columns.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This function can be used only in combination with\n",
      "        :py:meth:`~pyspark.sql.readwriter.DataFrameWriterV2.partitionedBy`\n",
      "        method of the `DataFrameWriterV2`.\n",
      "    \n",
      "    call_udf(udfName: str, *cols: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Call an user-defined function.\n",
      "        \n",
      "        .. versionadded:: 3.4.0\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        udfName : str\n",
      "            name of the user defined function (UDF)\n",
      "        cols : :class:`~pyspark.sql.Column` or str\n",
      "            column names or :class:`~pyspark.sql.Column`\\s to be used in the UDF\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            result of executed udf.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.functions import call_udf, col\n",
      "        >>> from pyspark.sql.types import IntegerType, StringType\n",
      "        >>> df = spark.createDataFrame([(1, \"a\"),(2, \"b\"), (3, \"c\")],[\"id\", \"name\"])\n",
      "        >>> _ = spark.udf.register(\"intX2\", lambda i: i * 2, IntegerType())\n",
      "        >>> df.select(call_udf(\"intX2\", \"id\")).show()\n",
      "        +---------+\n",
      "        |intX2(id)|\n",
      "        +---------+\n",
      "        |        2|\n",
      "        |        4|\n",
      "        |        6|\n",
      "        +---------+\n",
      "        >>> _ = spark.udf.register(\"strX2\", lambda s: s * 2, StringType())\n",
      "        >>> df.select(call_udf(\"strX2\", col(\"name\"))).show()\n",
      "        +-----------+\n",
      "        |strX2(name)|\n",
      "        +-----------+\n",
      "        |         aa|\n",
      "        |         bb|\n",
      "        |         cc|\n",
      "        +-----------+\n",
      "    \n",
      "    cbrt(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes the cube-root of the given value.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(cbrt(lit(27))).show()\n",
      "        +--------+\n",
      "        |CBRT(27)|\n",
      "        +--------+\n",
      "        |     3.0|\n",
      "        +--------+\n",
      "    \n",
      "    ceil(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes the ceiling of the given value.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(ceil(lit(-0.1))).show()\n",
      "        +----------+\n",
      "        |CEIL(-0.1)|\n",
      "        +----------+\n",
      "        |         0|\n",
      "        +----------+\n",
      "    \n",
      "    coalesce(*cols: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the first column that is not null.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        cols : :class:`~pyspark.sql.Column` or str\n",
      "            list of columns to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            value of the first column that is not null.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> cDf = spark.createDataFrame([(None, None), (1, None), (None, 2)], (\"a\", \"b\"))\n",
      "        >>> cDf.show()\n",
      "        +----+----+\n",
      "        |   a|   b|\n",
      "        +----+----+\n",
      "        |null|null|\n",
      "        |   1|null|\n",
      "        |null|   2|\n",
      "        +----+----+\n",
      "        \n",
      "        >>> cDf.select(coalesce(cDf[\"a\"], cDf[\"b\"])).show()\n",
      "        +--------------+\n",
      "        |coalesce(a, b)|\n",
      "        +--------------+\n",
      "        |          null|\n",
      "        |             1|\n",
      "        |             2|\n",
      "        +--------------+\n",
      "        \n",
      "        >>> cDf.select('*', coalesce(cDf[\"a\"], lit(0.0))).show()\n",
      "        +----+----+----------------+\n",
      "        |   a|   b|coalesce(a, 0.0)|\n",
      "        +----+----+----------------+\n",
      "        |null|null|             0.0|\n",
      "        |   1|null|             1.0|\n",
      "        |null|   2|             0.0|\n",
      "        +----+----+----------------+\n",
      "    \n",
      "    col(col: str) -> pyspark.sql.column.Column\n",
      "        Returns a :class:`~pyspark.sql.Column` based on the given column name.\n",
      "        \n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : str\n",
      "            the name for the column\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the corresponding column instance.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> col('x')\n",
      "        Column<'x'>\n",
      "        >>> column('x')\n",
      "        Column<'x'>\n",
      "    \n",
      "    collect_list(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns a list of objects with duplicates.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The function is non-deterministic because the order of collected results depends\n",
      "        on the order of the rows which may be non-deterministic after a shuffle.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            list of objects with duplicates.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df2 = spark.createDataFrame([(2,), (5,), (5,)], ('age',))\n",
      "        >>> df2.agg(collect_list('age')).collect()\n",
      "        [Row(collect_list(age)=[2, 5, 5])]\n",
      "    \n",
      "    collect_set(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns a set of objects with duplicate elements eliminated.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The function is non-deterministic because the order of collected results depends\n",
      "        on the order of the rows which may be non-deterministic after a shuffle.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            list of objects with no duplicates.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df2 = spark.createDataFrame([(2,), (5,), (5,)], ('age',))\n",
      "        >>> df2.agg(array_sort(collect_set('age')).alias('c')).collect()\n",
      "        [Row(c=[2, 5])]\n",
      "    \n",
      "    column = col(col: str) -> pyspark.sql.column.Column\n",
      "        Returns a :class:`~pyspark.sql.Column` based on the given column name.\n",
      "        \n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : str\n",
      "            the name for the column\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the corresponding column instance.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> col('x')\n",
      "        Column<'x'>\n",
      "        >>> column('x')\n",
      "        Column<'x'>\n",
      "    \n",
      "    concat(*cols: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Concatenates multiple input columns together into a single column.\n",
      "        The function works with strings, numeric, binary and compatible array columns.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        cols : :class:`~pyspark.sql.Column` or str\n",
      "            target column or columns to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            concatenated values. Type of the `Column` depends on input columns' type.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        :meth:`pyspark.sql.functions.array_join` : to concatenate string columns with delimiter\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('abcd','123')], ['s', 'd'])\n",
      "        >>> df = df.select(concat(df.s, df.d).alias('s'))\n",
      "        >>> df.collect()\n",
      "        [Row(s='abcd123')]\n",
      "        >>> df\n",
      "        DataFrame[s: string]\n",
      "        \n",
      "        >>> df = spark.createDataFrame([([1, 2], [3, 4], [5]), ([1, 2], None, [3])], ['a', 'b', 'c'])\n",
      "        >>> df = df.select(concat(df.a, df.b, df.c).alias(\"arr\"))\n",
      "        >>> df.collect()\n",
      "        [Row(arr=[1, 2, 3, 4, 5]), Row(arr=None)]\n",
      "        >>> df\n",
      "        DataFrame[arr: array<bigint>]\n",
      "    \n",
      "    concat_ws(sep: str, *cols: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Concatenates multiple input string columns together into a single string column,\n",
      "        using the given separator.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        sep : str\n",
      "            words separator.\n",
      "        cols : :class:`~pyspark.sql.Column` or str\n",
      "            list of columns to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            string of concatenated words.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('abcd','123')], ['s', 'd'])\n",
      "        >>> df.select(concat_ws('-', df.s, df.d).alias('s')).collect()\n",
      "        [Row(s='abcd-123')]\n",
      "    \n",
      "    conv(col: 'ColumnOrName', fromBase: int, toBase: int) -> pyspark.sql.column.Column\n",
      "        Convert a number in a string column from one base to another.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            a column to convert base for.\n",
      "        fromBase: int\n",
      "            from base number.\n",
      "        toBase: int\n",
      "            to base number.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            logariphm of given value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"010101\",)], ['n'])\n",
      "        >>> df.select(conv(df.n, 2, 16).alias('hex')).collect()\n",
      "        [Row(hex='15')]\n",
      "    \n",
      "    corr(col1: 'ColumnOrName', col2: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a new :class:`~pyspark.sql.Column` for the Pearson Correlation Coefficient for\n",
      "        ``col1`` and ``col2``.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col1 : :class:`~pyspark.sql.Column` or str\n",
      "            first column to calculate correlation.\n",
      "        col1 : :class:`~pyspark.sql.Column` or str\n",
      "            second column to calculate correlation.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            Pearson Correlation Coefficient of these two column values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> a = range(20)\n",
      "        >>> b = [2 * x for x in range(20)]\n",
      "        >>> df = spark.createDataFrame(zip(a, b), [\"a\", \"b\"])\n",
      "        >>> df.agg(corr(\"a\", \"b\").alias('c')).collect()\n",
      "        [Row(c=1.0)]\n",
      "    \n",
      "    cos(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes cosine of the input column.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            angle in radians\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            cosine of the angle, as if computed by `java.lang.Math.cos()`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import math\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(cos(lit(math.pi))).first()\n",
      "        Row(COS(3.14159...)=-1.0)\n",
      "    \n",
      "    cosh(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes hyperbolic cosine of the input column.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            hyperbolic angle\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            hyperbolic cosine of the angle, as if computed by `java.lang.Math.cosh()`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(cosh(lit(1))).first()\n",
      "        Row(COSH(1)=1.54308...)\n",
      "    \n",
      "    cot(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes cotangent of the input column.\n",
      "        \n",
      "        .. versionadded:: 3.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            angle in radians.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            cotangent of the angle.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import math\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(cot(lit(math.radians(45)))).first()\n",
      "        Row(COT(0.78539...)=1.00000...)\n",
      "    \n",
      "    count(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the number of items in a group.\n",
      "        \n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Count by all columns (start), and by a column that does not count ``None``.\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(None,), (\"a\",), (\"b\",), (\"c\",)], schema=[\"alphabets\"])\n",
      "        >>> df.select(count(expr(\"*\")), count(df.alphabets)).show()\n",
      "        +--------+----------------+\n",
      "        |count(1)|count(alphabets)|\n",
      "        +--------+----------------+\n",
      "        |       4|               3|\n",
      "        +--------+----------------+\n",
      "    \n",
      "    countDistinct(col: 'ColumnOrName', *cols: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a new :class:`~pyspark.sql.Column` for distinct count of ``col`` or ``cols``.\n",
      "        \n",
      "        An alias of :func:`count_distinct`, and it is encouraged to use :func:`count_distinct`\n",
      "        directly.\n",
      "        \n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "    \n",
      "    count_distinct(col: 'ColumnOrName', *cols: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a new :class:`Column` for distinct count of ``col`` or ``cols``.\n",
      "        \n",
      "        .. versionadded:: 3.2.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            first column to compute on.\n",
      "        cols : :class:`~pyspark.sql.Column` or str\n",
      "            other columns to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            distinct values of these two column values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import types\n",
      "        >>> df1 = spark.createDataFrame([1, 1, 3], types.IntegerType())\n",
      "        >>> df2 = spark.createDataFrame([1, 2], types.IntegerType())\n",
      "        >>> df1.join(df2).show()\n",
      "        +-----+-----+\n",
      "        |value|value|\n",
      "        +-----+-----+\n",
      "        |    1|    1|\n",
      "        |    1|    2|\n",
      "        |    1|    1|\n",
      "        |    1|    2|\n",
      "        |    3|    1|\n",
      "        |    3|    2|\n",
      "        +-----+-----+\n",
      "        >>> df1.join(df2).select(count_distinct(df1.value, df2.value)).show()\n",
      "        +----------------------------+\n",
      "        |count(DISTINCT value, value)|\n",
      "        +----------------------------+\n",
      "        |                           4|\n",
      "        +----------------------------+\n",
      "    \n",
      "    covar_pop(col1: 'ColumnOrName', col2: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a new :class:`~pyspark.sql.Column` for the population covariance of ``col1`` and\n",
      "        ``col2``.\n",
      "        \n",
      "        .. versionadded:: 2.0.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col1 : :class:`~pyspark.sql.Column` or str\n",
      "            first column to calculate covariance.\n",
      "        col1 : :class:`~pyspark.sql.Column` or str\n",
      "            second column to calculate covariance.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            covariance of these two column values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> a = [1] * 10\n",
      "        >>> b = [1] * 10\n",
      "        >>> df = spark.createDataFrame(zip(a, b), [\"a\", \"b\"])\n",
      "        >>> df.agg(covar_pop(\"a\", \"b\").alias('c')).collect()\n",
      "        [Row(c=0.0)]\n",
      "    \n",
      "    covar_samp(col1: 'ColumnOrName', col2: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a new :class:`~pyspark.sql.Column` for the sample covariance of ``col1`` and\n",
      "        ``col2``.\n",
      "        \n",
      "        .. versionadded:: 2.0.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col1 : :class:`~pyspark.sql.Column` or str\n",
      "            first column to calculate covariance.\n",
      "        col1 : :class:`~pyspark.sql.Column` or str\n",
      "            second column to calculate covariance.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            sample covariance of these two column values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> a = [1] * 10\n",
      "        >>> b = [1] * 10\n",
      "        >>> df = spark.createDataFrame(zip(a, b), [\"a\", \"b\"])\n",
      "        >>> df.agg(covar_samp(\"a\", \"b\").alias('c')).collect()\n",
      "        [Row(c=0.0)]\n",
      "    \n",
      "    crc32(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Calculates the cyclic redundancy check value  (CRC32) of a binary column and\n",
      "        returns the value as a bigint.\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for computed results.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.createDataFrame([('ABC',)], ['a']).select(crc32('a').alias('crc32')).collect()\n",
      "        [Row(crc32=2743272264)]\n",
      "    \n",
      "    create_map(*cols: Union[ForwardRef('ColumnOrName'), List[ForwardRef('ColumnOrName_')], Tuple[ForwardRef('ColumnOrName_'), ...]]) -> pyspark.sql.column.Column\n",
      "        Creates a new map column.\n",
      "        \n",
      "        .. versionadded:: 2.0.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        cols : :class:`~pyspark.sql.Column` or str\n",
      "            column names or :class:`~pyspark.sql.Column`\\s that are\n",
      "            grouped as key-value pairs, e.g. (key1, value1, key2, value2, ...).\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"Alice\", 2), (\"Bob\", 5)], (\"name\", \"age\"))\n",
      "        >>> df.select(create_map('name', 'age').alias(\"map\")).collect()\n",
      "        [Row(map={'Alice': 2}), Row(map={'Bob': 5})]\n",
      "        >>> df.select(create_map([df.name, df.age]).alias(\"map\")).collect()\n",
      "        [Row(map={'Alice': 2}), Row(map={'Bob': 5})]\n",
      "    \n",
      "    csc(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes cosecant of the input column.\n",
      "        \n",
      "        .. versionadded:: 3.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            angle in radians.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            cosecant of the angle.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import math\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(csc(lit(math.radians(90)))).first()\n",
      "        Row(CSC(1.57079...)=1.0)\n",
      "    \n",
      "    cume_dist() -> pyspark.sql.column.Column\n",
      "        Window function: returns the cumulative distribution of values within a window partition,\n",
      "        i.e. the fraction of rows that are below the current row.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for calculating cumulative distribution.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Window, types\n",
      "        >>> df = spark.createDataFrame([1, 2, 3, 3, 4], types.IntegerType())\n",
      "        >>> w = Window.orderBy(\"value\")\n",
      "        >>> df.withColumn(\"cd\", cume_dist().over(w)).show()\n",
      "        +-----+---+\n",
      "        |value| cd|\n",
      "        +-----+---+\n",
      "        |    1|0.2|\n",
      "        |    2|0.4|\n",
      "        |    3|0.8|\n",
      "        |    3|0.8|\n",
      "        |    4|1.0|\n",
      "        +-----+---+\n",
      "    \n",
      "    current_date() -> pyspark.sql.column.Column\n",
      "        Returns the current date at the start of query evaluation as a :class:`DateType` column.\n",
      "        All calls of current_date within the same query return the same value.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            current date.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(current_date()).show() # doctest: +SKIP\n",
      "        +--------------+\n",
      "        |current_date()|\n",
      "        +--------------+\n",
      "        |    2022-08-26|\n",
      "        +--------------+\n",
      "    \n",
      "    current_timestamp() -> pyspark.sql.column.Column\n",
      "        Returns the current timestamp at the start of query evaluation as a :class:`TimestampType`\n",
      "        column. All calls of current_timestamp within the same query return the same value.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            current date and time.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(current_timestamp()).show(truncate=False) # doctest: +SKIP\n",
      "        +-----------------------+\n",
      "        |current_timestamp()    |\n",
      "        +-----------------------+\n",
      "        |2022-08-26 21:23:22.716|\n",
      "        +-----------------------+\n",
      "    \n",
      "    date_add(start: 'ColumnOrName', days: Union[ForwardRef('ColumnOrName'), int]) -> pyspark.sql.column.Column\n",
      "        Returns the date that is `days` days after `start`. If `days` is a negative value\n",
      "        then these amount of days will be deducted from `start`.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        start : :class:`~pyspark.sql.Column` or str\n",
      "            date column to work on.\n",
      "        days : :class:`~pyspark.sql.Column` or str or int\n",
      "            how many days after the given date to calculate.\n",
      "            Accepts negative value as well to calculate backwards in time.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a date after/before given number of days.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08', 2,)], ['dt', 'add'])\n",
      "        >>> df.select(date_add(df.dt, 1).alias('next_date')).collect()\n",
      "        [Row(next_date=datetime.date(2015, 4, 9))]\n",
      "        >>> df.select(date_add(df.dt, df.add.cast('integer')).alias('next_date')).collect()\n",
      "        [Row(next_date=datetime.date(2015, 4, 10))]\n",
      "        >>> df.select(date_add('dt', -1).alias('prev_date')).collect()\n",
      "        [Row(prev_date=datetime.date(2015, 4, 7))]\n",
      "    \n",
      "    date_format(date: 'ColumnOrName', format: str) -> pyspark.sql.column.Column\n",
      "        Converts a date/timestamp/string to a value of string in the format specified by the date\n",
      "        format given by the second argument.\n",
      "        \n",
      "        A pattern could be for instance `dd.MM.yyyy` and could return a string like '18.03.1993'. All\n",
      "        pattern letters of `datetime pattern`_. can be used.\n",
      "        \n",
      "        .. _datetime pattern: https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        Whenever possible, use specialized functions like `year`.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        date : :class:`~pyspark.sql.Column` or str\n",
      "            input column of values to format.\n",
      "        format: str\n",
      "            format to use to represent datetime values.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            string value representing formatted datetime.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(date_format('dt', 'MM/dd/yyy').alias('date')).collect()\n",
      "        [Row(date='04/08/2015')]\n",
      "    \n",
      "    date_sub(start: 'ColumnOrName', days: Union[ForwardRef('ColumnOrName'), int]) -> pyspark.sql.column.Column\n",
      "        Returns the date that is `days` days before `start`. If `days` is a negative value\n",
      "        then these amount of days will be added to `start`.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        start : :class:`~pyspark.sql.Column` or str\n",
      "            date column to work on.\n",
      "        days : :class:`~pyspark.sql.Column` or str or int\n",
      "            how many days before the given date to calculate.\n",
      "            Accepts negative value as well to calculate forward in time.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a date before/after given number of days.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08', 2,)], ['dt', 'sub'])\n",
      "        >>> df.select(date_sub(df.dt, 1).alias('prev_date')).collect()\n",
      "        [Row(prev_date=datetime.date(2015, 4, 7))]\n",
      "        >>> df.select(date_sub(df.dt, df.sub.cast('integer')).alias('prev_date')).collect()\n",
      "        [Row(prev_date=datetime.date(2015, 4, 6))]\n",
      "        >>> df.select(date_sub('dt', -1).alias('next_date')).collect()\n",
      "        [Row(next_date=datetime.date(2015, 4, 9))]\n",
      "    \n",
      "    date_trunc(format: str, timestamp: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns timestamp truncated to the unit specified by the format.\n",
      "        \n",
      "        .. versionadded:: 2.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        format : str\n",
      "            'year', 'yyyy', 'yy' to truncate by year,\n",
      "            'month', 'mon', 'mm' to truncate by month,\n",
      "            'day', 'dd' to truncate by day,\n",
      "            Other options are:\n",
      "            'microsecond', 'millisecond', 'second', 'minute', 'hour', 'week', 'quarter'\n",
      "        timestamp : :class:`~pyspark.sql.Column` or str\n",
      "            input column of values to truncate.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            truncated timestamp.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('1997-02-28 05:02:11',)], ['t'])\n",
      "        >>> df.select(date_trunc('year', df.t).alias('year')).collect()\n",
      "        [Row(year=datetime.datetime(1997, 1, 1, 0, 0))]\n",
      "        >>> df.select(date_trunc('mon', df.t).alias('month')).collect()\n",
      "        [Row(month=datetime.datetime(1997, 2, 1, 0, 0))]\n",
      "    \n",
      "    datediff(end: 'ColumnOrName', start: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the number of days from `start` to `end`.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        end : :class:`~pyspark.sql.Column` or str\n",
      "            to date column to work on.\n",
      "        start : :class:`~pyspark.sql.Column` or str\n",
      "            from date column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            difference in days between two dates.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08','2015-05-10')], ['d1', 'd2'])\n",
      "        >>> df.select(datediff(df.d2, df.d1).alias('diff')).collect()\n",
      "        [Row(diff=32)]\n",
      "    \n",
      "    dayofmonth(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Extract the day of the month of a given date/timestamp as integer.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target date/timestamp column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            day of the month for given date/timestamp as integer.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(dayofmonth('dt').alias('day')).collect()\n",
      "        [Row(day=8)]\n",
      "    \n",
      "    dayofweek(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Extract the day of the week of a given date/timestamp as integer.\n",
      "        Ranges from 1 for a Sunday through to 7 for a Saturday\n",
      "        \n",
      "        .. versionadded:: 2.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target date/timestamp column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            day of the week for given date/timestamp as integer.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(dayofweek('dt').alias('day')).collect()\n",
      "        [Row(day=4)]\n",
      "    \n",
      "    dayofyear(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Extract the day of the year of a given date/timestamp as integer.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target date/timestamp column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            day of the year for given date/timestamp as integer.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(dayofyear('dt').alias('day')).collect()\n",
      "        [Row(day=98)]\n",
      "    \n",
      "    days(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Partition transform function: A transform for timestamps and dates\n",
      "        to partition data into days.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target date or timestamp column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            data partitioned by days.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.writeTo(\"catalog.db.table\").partitionedBy(  # doctest: +SKIP\n",
      "        ...     days(\"ts\")\n",
      "        ... ).createOrReplace()\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This function can be used only in combination with\n",
      "        :py:meth:`~pyspark.sql.readwriter.DataFrameWriterV2.partitionedBy`\n",
      "        method of the `DataFrameWriterV2`.\n",
      "    \n",
      "    decode(col: 'ColumnOrName', charset: str) -> pyspark.sql.column.Column\n",
      "        Computes the first argument into a string from a binary using the provided character set\n",
      "        (one of 'US-ASCII', 'ISO-8859-1', 'UTF-8', 'UTF-16BE', 'UTF-16LE', 'UTF-16').\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        charset : str\n",
      "            charset to use to decode to.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('abcd',)], ['a'])\n",
      "        >>> df.select(decode(\"a\", \"UTF-8\")).show()\n",
      "        +----------------+\n",
      "        |decode(a, UTF-8)|\n",
      "        +----------------+\n",
      "        |            abcd|\n",
      "        +----------------+\n",
      "    \n",
      "    degrees(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Converts an angle measured in radians to an approximately equivalent angle\n",
      "        measured in degrees.\n",
      "        \n",
      "        .. versionadded:: 2.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            angle in radians\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            angle in degrees, as if computed by `java.lang.Math.toDegrees()`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import math\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(degrees(lit(math.pi))).first()\n",
      "        Row(DEGREES(3.14159...)=180.0)\n",
      "    \n",
      "    dense_rank() -> pyspark.sql.column.Column\n",
      "        Window function: returns the rank of rows within a window partition, without any gaps.\n",
      "        \n",
      "        The difference between rank and dense_rank is that dense_rank leaves no gaps in ranking\n",
      "        sequence when there are ties. That is, if you were ranking a competition using dense_rank\n",
      "        and had three people tie for second place, you would say that all three were in second\n",
      "        place and that the next person came in third. Rank would give me sequential numbers, making\n",
      "        the person that came in third place (after the ties) would register as coming in fifth.\n",
      "        \n",
      "        This is equivalent to the DENSE_RANK function in SQL.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for calculating ranks.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Window, types\n",
      "        >>> df = spark.createDataFrame([1, 1, 2, 3, 3, 4], types.IntegerType())\n",
      "        >>> w = Window.orderBy(\"value\")\n",
      "        >>> df.withColumn(\"drank\", dense_rank().over(w)).show()\n",
      "        +-----+-----+\n",
      "        |value|drank|\n",
      "        +-----+-----+\n",
      "        |    1|    1|\n",
      "        |    1|    1|\n",
      "        |    2|    2|\n",
      "        |    3|    3|\n",
      "        |    3|    3|\n",
      "        |    4|    4|\n",
      "        +-----+-----+\n",
      "    \n",
      "    desc(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a sort expression based on the descending order of the given column name.\n",
      "        \n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to sort by in the descending order.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column specifying the order.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        Sort by the column 'id' in the descending order.\n",
      "        \n",
      "        >>> spark.range(5).orderBy(desc(\"id\")).show()\n",
      "        +---+\n",
      "        | id|\n",
      "        +---+\n",
      "        |  4|\n",
      "        |  3|\n",
      "        |  2|\n",
      "        |  1|\n",
      "        |  0|\n",
      "        +---+\n",
      "    \n",
      "    desc_nulls_first(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a sort expression based on the descending order of the given\n",
      "        column name, and null values appear before non-null values.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to sort by in the descending order.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column specifying the order.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df1 = spark.createDataFrame([(0, None),\n",
      "        ...                              (1, \"Bob\"),\n",
      "        ...                              (2, \"Alice\")], [\"age\", \"name\"])\n",
      "        >>> df1.sort(desc_nulls_first(df1.name)).show()\n",
      "        +---+-----+\n",
      "        |age| name|\n",
      "        +---+-----+\n",
      "        |  0| null|\n",
      "        |  1|  Bob|\n",
      "        |  2|Alice|\n",
      "        +---+-----+\n",
      "    \n",
      "    desc_nulls_last(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a sort expression based on the descending order of the given\n",
      "        column name, and null values appear after non-null values.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to sort by in the descending order.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column specifying the order.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df1 = spark.createDataFrame([(0, None),\n",
      "        ...                              (1, \"Bob\"),\n",
      "        ...                              (2, \"Alice\")], [\"age\", \"name\"])\n",
      "        >>> df1.sort(desc_nulls_last(df1.name)).show()\n",
      "        +---+-----+\n",
      "        |age| name|\n",
      "        +---+-----+\n",
      "        |  1|  Bob|\n",
      "        |  2|Alice|\n",
      "        |  0| null|\n",
      "        +---+-----+\n",
      "    \n",
      "    element_at(col: 'ColumnOrName', extraction: Any) -> pyspark.sql.column.Column\n",
      "        Collection function: Returns element of array at given index in `extraction` if col is array.\n",
      "        Returns value for the given key in `extraction` if col is map. If position is negative\n",
      "        then location of the element will start from end, if number is outside the\n",
      "        array boundaries then None will be returned.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing array or map\n",
      "        extraction :\n",
      "            index to check for in array or key to check for in map\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            value at given position.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The position is not zero based, but 1 based index.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        :meth:`get`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([\"a\", \"b\", \"c\"],)], ['data'])\n",
      "        >>> df.select(element_at(df.data, 1)).collect()\n",
      "        [Row(element_at(data, 1)='a')]\n",
      "        >>> df.select(element_at(df.data, -1)).collect()\n",
      "        [Row(element_at(data, -1)='c')]\n",
      "        \n",
      "        >>> df = spark.createDataFrame([({\"a\": 1.0, \"b\": 2.0},)], ['data'])\n",
      "        >>> df.select(element_at(df.data, lit(\"a\"))).collect()\n",
      "        [Row(element_at(data, a)=1.0)]\n",
      "    \n",
      "    encode(col: 'ColumnOrName', charset: str) -> pyspark.sql.column.Column\n",
      "        Computes the first argument into a binary from a string using the provided character set\n",
      "        (one of 'US-ASCII', 'ISO-8859-1', 'UTF-8', 'UTF-16BE', 'UTF-16LE', 'UTF-16').\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        charset : str\n",
      "            charset to use to encode.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('abcd',)], ['c'])\n",
      "        >>> df.select(encode(\"c\", \"UTF-8\")).show()\n",
      "        +----------------+\n",
      "        |encode(c, UTF-8)|\n",
      "        +----------------+\n",
      "        |   [61 62 63 64]|\n",
      "        +----------------+\n",
      "    \n",
      "    exists(col: 'ColumnOrName', f: Callable[[pyspark.sql.column.Column], pyspark.sql.column.Column]) -> pyspark.sql.column.Column\n",
      "        Returns whether a predicate holds for one or more elements in the array.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        f : function\n",
      "            ``(x: Column) -> Column: ...``  returning the Boolean expression.\n",
      "            Can use methods of :class:`~pyspark.sql.Column`, functions defined in\n",
      "            :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\n",
      "            Python ``UserDefinedFunctions`` are not supported\n",
      "            (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            True if \"any\" element of an array evaluates to True when passed as an argument to\n",
      "            given function and False otherwise.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1, [1, 2, 3, 4]), (2, [3, -1, 0])],(\"key\", \"values\"))\n",
      "        >>> df.select(exists(\"values\", lambda x: x < 0).alias(\"any_negative\")).show()\n",
      "        +------------+\n",
      "        |any_negative|\n",
      "        +------------+\n",
      "        |       false|\n",
      "        |        true|\n",
      "        +------------+\n",
      "    \n",
      "    exp(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes the exponential of the given value.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            column to calculate exponential for.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            exponential of the given value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(exp(lit(0))).show()\n",
      "        +------+\n",
      "        |EXP(0)|\n",
      "        +------+\n",
      "        |   1.0|\n",
      "        +------+\n",
      "    \n",
      "    explode(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a new row for each element in the given array or map.\n",
      "        Uses the default column name `col` for elements in the array and\n",
      "        `key` and `value` for elements in the map unless specified otherwise.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            one row per array item or map key value.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        :meth:`pyspark.functions.posexplode`\n",
      "        :meth:`pyspark.functions.explode_outer`\n",
      "        :meth:`pyspark.functions.posexplode_outer`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Row\n",
      "        >>> eDF = spark.createDataFrame([Row(a=1, intlist=[1,2,3], mapfield={\"a\": \"b\"})])\n",
      "        >>> eDF.select(explode(eDF.intlist).alias(\"anInt\")).collect()\n",
      "        [Row(anInt=1), Row(anInt=2), Row(anInt=3)]\n",
      "        \n",
      "        >>> eDF.select(explode(eDF.mapfield).alias(\"key\", \"value\")).show()\n",
      "        +---+-----+\n",
      "        |key|value|\n",
      "        +---+-----+\n",
      "        |  a|    b|\n",
      "        +---+-----+\n",
      "    \n",
      "    explode_outer(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a new row for each element in the given array or map.\n",
      "        Unlike explode, if the array/map is null or empty then null is produced.\n",
      "        Uses the default column name `col` for elements in the array and\n",
      "        `key` and `value` for elements in the map unless specified otherwise.\n",
      "        \n",
      "        .. versionadded:: 2.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            one row per array item or map key value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame(\n",
      "        ...     [(1, [\"foo\", \"bar\"], {\"x\": 1.0}), (2, [], {}), (3, None, None)],\n",
      "        ...     (\"id\", \"an_array\", \"a_map\")\n",
      "        ... )\n",
      "        >>> df.select(\"id\", \"an_array\", explode_outer(\"a_map\")).show()\n",
      "        +---+----------+----+-----+\n",
      "        | id|  an_array| key|value|\n",
      "        +---+----------+----+-----+\n",
      "        |  1|[foo, bar]|   x|  1.0|\n",
      "        |  2|        []|null| null|\n",
      "        |  3|      null|null| null|\n",
      "        +---+----------+----+-----+\n",
      "        \n",
      "        >>> df.select(\"id\", \"a_map\", explode_outer(\"an_array\")).show()\n",
      "        +---+----------+----+\n",
      "        | id|     a_map| col|\n",
      "        +---+----------+----+\n",
      "        |  1|{x -> 1.0}| foo|\n",
      "        |  1|{x -> 1.0}| bar|\n",
      "        |  2|        {}|null|\n",
      "        |  3|      null|null|\n",
      "        +---+----------+----+\n",
      "    \n",
      "    expm1(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes the exponential of the given value minus one.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            column to calculate exponential for.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            exponential less one.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(expm1(lit(1))).first()\n",
      "        Row(EXPM1(1)=1.71828...)\n",
      "    \n",
      "    expr(str: str) -> pyspark.sql.column.Column\n",
      "        Parses the expression string into the column that it represents\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        str : str\n",
      "            expression defined in string.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            column representing the expression.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([[\"Alice\"], [\"Bob\"]], [\"name\"])\n",
      "        >>> df.select(\"name\", expr(\"length(name)\")).show()\n",
      "        +-----+------------+\n",
      "        | name|length(name)|\n",
      "        +-----+------------+\n",
      "        |Alice|           5|\n",
      "        |  Bob|           3|\n",
      "        +-----+------------+\n",
      "    \n",
      "    factorial(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes the factorial of the given value.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            a column to calculate factorial for.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            factorial of given value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(5,)], ['n'])\n",
      "        >>> df.select(factorial(df.n).alias('f')).collect()\n",
      "        [Row(f=120)]\n",
      "    \n",
      "    filter(col: 'ColumnOrName', f: Union[Callable[[pyspark.sql.column.Column], pyspark.sql.column.Column], Callable[[pyspark.sql.column.Column, pyspark.sql.column.Column], pyspark.sql.column.Column]]) -> pyspark.sql.column.Column\n",
      "        Returns an array of elements for which a predicate holds in a given array.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        f : function\n",
      "            A function that returns the Boolean expression.\n",
      "            Can take one of the following forms:\n",
      "        \n",
      "            - Unary ``(x: Column) -> Column: ...``\n",
      "            - Binary ``(x: Column, i: Column) -> Column...``, where the second argument is\n",
      "                a 0-based index of the element.\n",
      "        \n",
      "            and can use methods of :class:`~pyspark.sql.Column`, functions defined in\n",
      "            :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\n",
      "            Python ``UserDefinedFunctions`` are not supported\n",
      "            (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            filtered array of elements where given function evaluated to True\n",
      "            when passed as an argument.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame(\n",
      "        ...     [(1, [\"2018-09-20\",  \"2019-02-03\", \"2019-07-01\", \"2020-06-01\"])],\n",
      "        ...     (\"key\", \"values\")\n",
      "        ... )\n",
      "        >>> def after_second_quarter(x):\n",
      "        ...     return month(to_date(x)) > 6\n",
      "        >>> df.select(\n",
      "        ...     filter(\"values\", after_second_quarter).alias(\"after_second_quarter\")\n",
      "        ... ).show(truncate=False)\n",
      "        +------------------------+\n",
      "        |after_second_quarter    |\n",
      "        +------------------------+\n",
      "        |[2018-09-20, 2019-07-01]|\n",
      "        +------------------------+\n",
      "    \n",
      "    first(col: 'ColumnOrName', ignorenulls: bool = False) -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the first value in a group.\n",
      "        \n",
      "        The function by default returns the first values it sees. It will return the first non-null\n",
      "        value it sees when ignoreNulls is set to true. If all values are null, then null is returned.\n",
      "        \n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The function is non-deterministic because its results depends on the order of the\n",
      "        rows which may be non-deterministic after a shuffle.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            column to fetch first value for.\n",
      "        ignorenulls : :class:`~pyspark.sql.Column` or str\n",
      "            if first value is null then look for first non-null value.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            first value of the group.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"Alice\", 2), (\"Bob\", 5), (\"Alice\", None)], (\"name\", \"age\"))\n",
      "        >>> df = df.orderBy(df.age)\n",
      "        >>> df.groupby(\"name\").agg(first(\"age\")).orderBy(\"name\").show()\n",
      "        +-----+----------+\n",
      "        | name|first(age)|\n",
      "        +-----+----------+\n",
      "        |Alice|      null|\n",
      "        |  Bob|         5|\n",
      "        +-----+----------+\n",
      "        \n",
      "        Now, to ignore any nulls we needs to set ``ignorenulls`` to `True`\n",
      "        \n",
      "        >>> df.groupby(\"name\").agg(first(\"age\", ignorenulls=True)).orderBy(\"name\").show()\n",
      "        +-----+----------+\n",
      "        | name|first(age)|\n",
      "        +-----+----------+\n",
      "        |Alice|         2|\n",
      "        |  Bob|         5|\n",
      "        +-----+----------+\n",
      "    \n",
      "    flatten(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Collection function: creates a single array from an array of arrays.\n",
      "        If a structure of nested arrays is deeper than two levels,\n",
      "        only one level of nesting is removed.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            flattened array.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([[1, 2, 3], [4, 5], [6]],), ([None, [4, 5]],)], ['data'])\n",
      "        >>> df.show(truncate=False)\n",
      "        +------------------------+\n",
      "        |data                    |\n",
      "        +------------------------+\n",
      "        |[[1, 2, 3], [4, 5], [6]]|\n",
      "        |[null, [4, 5]]          |\n",
      "        +------------------------+\n",
      "        >>> df.select(flatten(df.data).alias('r')).show()\n",
      "        +------------------+\n",
      "        |                 r|\n",
      "        +------------------+\n",
      "        |[1, 2, 3, 4, 5, 6]|\n",
      "        |              null|\n",
      "        +------------------+\n",
      "    \n",
      "    floor(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes the floor of the given value.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            column to find floor for.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            nearest integer that is less than or equal to given value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(floor(lit(2.5))).show()\n",
      "        +----------+\n",
      "        |FLOOR(2.5)|\n",
      "        +----------+\n",
      "        |         2|\n",
      "        +----------+\n",
      "    \n",
      "    forall(col: 'ColumnOrName', f: Callable[[pyspark.sql.column.Column], pyspark.sql.column.Column]) -> pyspark.sql.column.Column\n",
      "        Returns whether a predicate holds for every element in the array.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        f : function\n",
      "            ``(x: Column) -> Column: ...``  returning the Boolean expression.\n",
      "            Can use methods of :class:`~pyspark.sql.Column`, functions defined in\n",
      "            :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\n",
      "            Python ``UserDefinedFunctions`` are not supported\n",
      "            (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            True if \"all\" elements of an array evaluates to True when passed as an argument to\n",
      "            given function and False otherwise.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame(\n",
      "        ...     [(1, [\"bar\"]), (2, [\"foo\", \"bar\"]), (3, [\"foobar\", \"foo\"])],\n",
      "        ...     (\"key\", \"values\")\n",
      "        ... )\n",
      "        >>> df.select(forall(\"values\", lambda x: x.rlike(\"foo\")).alias(\"all_foo\")).show()\n",
      "        +-------+\n",
      "        |all_foo|\n",
      "        +-------+\n",
      "        |  false|\n",
      "        |  false|\n",
      "        |   true|\n",
      "        +-------+\n",
      "    \n",
      "    format_number(col: 'ColumnOrName', d: int) -> pyspark.sql.column.Column\n",
      "        Formats the number X to a format like '#,--#,--#.--', rounded to d decimal places\n",
      "        with HALF_EVEN round mode, and returns the result as a string.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            the column name of the numeric value to be formatted\n",
      "        d : int\n",
      "            the N decimal places\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column of formatted results.\n",
      "        \n",
      "        >>> spark.createDataFrame([(5,)], ['a']).select(format_number('a', 4).alias('v')).collect()\n",
      "        [Row(v='5.0000')]\n",
      "    \n",
      "    format_string(format: str, *cols: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Formats the arguments in printf-style and returns the result as a string column.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        format : str\n",
      "            string that can contain embedded format tags and used as result column's value\n",
      "        cols : :class:`~pyspark.sql.Column` or str\n",
      "            column names or :class:`~pyspark.sql.Column`\\s to be used in formatting\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column of formatted results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(5, \"hello\")], ['a', 'b'])\n",
      "        >>> df.select(format_string('%d %s', df.a, df.b).alias('v')).collect()\n",
      "        [Row(v='5 hello')]\n",
      "    \n",
      "    from_csv(col: 'ColumnOrName', schema: Union[pyspark.sql.column.Column, str], options: Optional[Dict[str, str]] = None) -> pyspark.sql.column.Column\n",
      "        Parses a column containing a CSV string to a row with the specified schema.\n",
      "        Returns `null`, in the case of an unparseable string.\n",
      "        \n",
      "        .. versionadded:: 3.0.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            a column or column name in CSV format\n",
      "        schema :class:`~pyspark.sql.Column` or str\n",
      "            a column, or Python string literal with schema in DDL format, to use when parsing the CSV column.\n",
      "        options : dict, optional\n",
      "            options to control parsing. accepts the same options as the CSV datasource.\n",
      "            See `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-csv.html#data-source-option>`_\n",
      "            for the version you use.\n",
      "        \n",
      "            .. # noqa\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a column of parsed CSV values\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> data = [(\"1,2,3\",)]\n",
      "        >>> df = spark.createDataFrame(data, (\"value\",))\n",
      "        >>> df.select(from_csv(df.value, \"a INT, b INT, c INT\").alias(\"csv\")).collect()\n",
      "        [Row(csv=Row(a=1, b=2, c=3))]\n",
      "        >>> value = data[0][0]\n",
      "        >>> df.select(from_csv(df.value, schema_of_csv(value)).alias(\"csv\")).collect()\n",
      "        [Row(csv=Row(_c0=1, _c1=2, _c2=3))]\n",
      "        >>> data = [(\"   abc\",)]\n",
      "        >>> df = spark.createDataFrame(data, (\"value\",))\n",
      "        >>> options = {'ignoreLeadingWhiteSpace': True}\n",
      "        >>> df.select(from_csv(df.value, \"s string\", options).alias(\"csv\")).collect()\n",
      "        [Row(csv=Row(s='abc'))]\n",
      "    \n",
      "    from_json(col: 'ColumnOrName', schema: Union[pyspark.sql.types.ArrayType, pyspark.sql.types.StructType, pyspark.sql.column.Column, str], options: Optional[Dict[str, str]] = None) -> pyspark.sql.column.Column\n",
      "        Parses a column containing a JSON string into a :class:`MapType` with :class:`StringType`\n",
      "        as keys type, :class:`StructType` or :class:`ArrayType` with\n",
      "        the specified schema. Returns `null`, in the case of an unparseable string.\n",
      "        \n",
      "        .. versionadded:: 2.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            a column or column name in JSON format\n",
      "        schema : :class:`DataType` or str\n",
      "            a StructType, ArrayType of StructType or Python string literal with a DDL-formatted string\n",
      "            to use when parsing the json column\n",
      "        options : dict, optional\n",
      "            options to control parsing. accepts the same options as the json datasource.\n",
      "            See `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-json.html#data-source-option>`_\n",
      "            for the version you use.\n",
      "        \n",
      "            .. # noqa\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a new column of complex type from given JSON object.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.types import *\n",
      "        >>> data = [(1, '''{\"a\": 1}''')]\n",
      "        >>> schema = StructType([StructField(\"a\", IntegerType())])\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
      "        >>> df.select(from_json(df.value, schema).alias(\"json\")).collect()\n",
      "        [Row(json=Row(a=1))]\n",
      "        >>> df.select(from_json(df.value, \"a INT\").alias(\"json\")).collect()\n",
      "        [Row(json=Row(a=1))]\n",
      "        >>> df.select(from_json(df.value, \"MAP<STRING,INT>\").alias(\"json\")).collect()\n",
      "        [Row(json={'a': 1})]\n",
      "        >>> data = [(1, '''[{\"a\": 1}]''')]\n",
      "        >>> schema = ArrayType(StructType([StructField(\"a\", IntegerType())]))\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
      "        >>> df.select(from_json(df.value, schema).alias(\"json\")).collect()\n",
      "        [Row(json=[Row(a=1)])]\n",
      "        >>> schema = schema_of_json(lit('''{\"a\": 0}'''))\n",
      "        >>> df.select(from_json(df.value, schema).alias(\"json\")).collect()\n",
      "        [Row(json=Row(a=None))]\n",
      "        >>> data = [(1, '''[1, 2, 3]''')]\n",
      "        >>> schema = ArrayType(IntegerType())\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
      "        >>> df.select(from_json(df.value, schema).alias(\"json\")).collect()\n",
      "        [Row(json=[1, 2, 3])]\n",
      "    \n",
      "    from_unixtime(timestamp: 'ColumnOrName', format: str = 'yyyy-MM-dd HH:mm:ss') -> pyspark.sql.column.Column\n",
      "        Converts the number of seconds from unix epoch (1970-01-01 00:00:00 UTC) to a string\n",
      "        representing the timestamp of that moment in the current system time zone in the given\n",
      "        format.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        timestamp : :class:`~pyspark.sql.Column` or str\n",
      "            column of unix time values.\n",
      "        format : str, optional\n",
      "            format to use to convert to (default: yyyy-MM-dd HH:mm:ss)\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            formatted timestamp as string.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")\n",
      "        >>> time_df = spark.createDataFrame([(1428476400,)], ['unix_time'])\n",
      "        >>> time_df.select(from_unixtime('unix_time').alias('ts')).collect()\n",
      "        [Row(ts='2015-04-08 00:00:00')]\n",
      "        >>> spark.conf.unset(\"spark.sql.session.timeZone\")\n",
      "    \n",
      "    from_utc_timestamp(timestamp: 'ColumnOrName', tz: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        This is a common function for databases supporting TIMESTAMP WITHOUT TIMEZONE. This function\n",
      "        takes a timestamp which is timezone-agnostic, and interprets it as a timestamp in UTC, and\n",
      "        renders that timestamp as a timestamp in the given time zone.\n",
      "        \n",
      "        However, timestamp in Spark represents number of microseconds from the Unix epoch, which is not\n",
      "        timezone-agnostic. So in Spark this function just shift the timestamp value from UTC timezone to\n",
      "        the given timezone.\n",
      "        \n",
      "        This function may return confusing result if the input is a string with timezone, e.g.\n",
      "        '2018-03-13T06:18:23+00:00'. The reason is that, Spark firstly cast the string to timestamp\n",
      "        according to the timezone in the string, and finally display the result by converting the\n",
      "        timestamp to string according to the session local timezone.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        timestamp : :class:`~pyspark.sql.Column` or str\n",
      "            the column that contains timestamps\n",
      "        tz : :class:`~pyspark.sql.Column` or str\n",
      "            A string detailing the time zone ID that the input should be adjusted to. It should\n",
      "            be in the format of either region-based zone IDs or zone offsets. Region IDs must\n",
      "            have the form 'area/city', such as 'America/Los_Angeles'. Zone offsets must be in\n",
      "            the format '(+|-)HH:mm', for example '-08:00' or '+01:00'. Also 'UTC' and 'Z' are\n",
      "            supported as aliases of '+00:00'. Other short names are not recommended to use\n",
      "            because they can be ambiguous.\n",
      "        \n",
      "            .. versionchanged:: 2.4\n",
      "               `tz` can take a :class:`~pyspark.sql.Column` containing timezone ID strings.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            timestamp value represented in given timezone.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('1997-02-28 10:30:00', 'JST')], ['ts', 'tz'])\n",
      "        >>> df.select(from_utc_timestamp(df.ts, \"PST\").alias('local_time')).collect()\n",
      "        [Row(local_time=datetime.datetime(1997, 2, 28, 2, 30))]\n",
      "        >>> df.select(from_utc_timestamp(df.ts, df.tz).alias('local_time')).collect()\n",
      "        [Row(local_time=datetime.datetime(1997, 2, 28, 19, 30))]\n",
      "    \n",
      "    get(col: 'ColumnOrName', index: Union[ForwardRef('ColumnOrName'), int]) -> pyspark.sql.column.Column\n",
      "        Collection function: Returns element of array at given (0-based) index.\n",
      "        If the index points outside of the array boundaries, then this function\n",
      "        returns NULL.\n",
      "        \n",
      "        .. versionadded:: 3.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing array\n",
      "        index : :class:`~pyspark.sql.Column` or str or int\n",
      "            index to check for in array\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            value at given position.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The position is not 1 based, but 0 based index.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        :meth:`element_at`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([\"a\", \"b\", \"c\"], 1)], ['data', 'index'])\n",
      "        >>> df.select(get(df.data, 1)).show()\n",
      "        +------------+\n",
      "        |get(data, 1)|\n",
      "        +------------+\n",
      "        |           b|\n",
      "        +------------+\n",
      "        \n",
      "        >>> df.select(get(df.data, -1)).show()\n",
      "        +-------------+\n",
      "        |get(data, -1)|\n",
      "        +-------------+\n",
      "        |         null|\n",
      "        +-------------+\n",
      "        \n",
      "        >>> df.select(get(df.data, 3)).show()\n",
      "        +------------+\n",
      "        |get(data, 3)|\n",
      "        +------------+\n",
      "        |        null|\n",
      "        +------------+\n",
      "        \n",
      "        >>> df.select(get(df.data, \"index\")).show()\n",
      "        +----------------+\n",
      "        |get(data, index)|\n",
      "        +----------------+\n",
      "        |               b|\n",
      "        +----------------+\n",
      "        \n",
      "        >>> df.select(get(df.data, col(\"index\") - 1)).show()\n",
      "        +----------------------+\n",
      "        |get(data, (index - 1))|\n",
      "        +----------------------+\n",
      "        |                     a|\n",
      "        +----------------------+\n",
      "    \n",
      "    get_json_object(col: 'ColumnOrName', path: str) -> pyspark.sql.column.Column\n",
      "        Extracts json object from a json string based on json `path` specified, and returns json string\n",
      "        of the extracted json object. It will return null if the input json string is invalid.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            string column in json format\n",
      "        path : str\n",
      "            path to the json object to extract\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            string representation of given JSON object value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> data = [(\"1\", '''{\"f1\": \"value1\", \"f2\": \"value2\"}'''), (\"2\", '''{\"f1\": \"value12\"}''')]\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"jstring\"))\n",
      "        >>> df.select(df.key, get_json_object(df.jstring, '$.f1').alias(\"c0\"), \\\n",
      "        ...                   get_json_object(df.jstring, '$.f2').alias(\"c1\") ).collect()\n",
      "        [Row(key='1', c0='value1', c1='value2'), Row(key='2', c0='value12', c1=None)]\n",
      "    \n",
      "    greatest(*cols: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the greatest value of the list of column names, skipping null values.\n",
      "        This function takes at least 2 parameters. It will return null if all parameters are null.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            columns to check for gratest value.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            gratest value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1, 4, 3)], ['a', 'b', 'c'])\n",
      "        >>> df.select(greatest(df.a, df.b, df.c).alias(\"greatest\")).collect()\n",
      "        [Row(greatest=4)]\n",
      "    \n",
      "    grouping(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: indicates whether a specified column in a GROUP BY list is aggregated\n",
      "        or not, returns 1 for aggregated or 0 for not aggregated in the result set.\n",
      "        \n",
      "        .. versionadded:: 2.0.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            column to check if it's aggregated.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            returns 1 for aggregated or 0 for not aggregated in the result set.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"Alice\", 2), (\"Bob\", 5)], (\"name\", \"age\"))\n",
      "        >>> df.cube(\"name\").agg(grouping(\"name\"), sum(\"age\")).orderBy(\"name\").show()\n",
      "        +-----+--------------+--------+\n",
      "        | name|grouping(name)|sum(age)|\n",
      "        +-----+--------------+--------+\n",
      "        | null|             1|       7|\n",
      "        |Alice|             0|       2|\n",
      "        |  Bob|             0|       5|\n",
      "        +-----+--------------+--------+\n",
      "    \n",
      "    grouping_id(*cols: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the level of grouping, equals to\n",
      "        \n",
      "           (grouping(c1) << (n-1)) + (grouping(c2) << (n-2)) + ... + grouping(cn)\n",
      "        \n",
      "        .. versionadded:: 2.0.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The list of columns should match with grouping columns exactly, or empty (means all\n",
      "        the grouping columns).\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        cols : :class:`~pyspark.sql.Column` or str\n",
      "            columns to check for.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            returns level of the grouping it relates to.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1, \"a\", \"a\"),\n",
      "        ...                             (3, \"a\", \"a\"),\n",
      "        ...                             (4, \"b\", \"c\")], [\"c1\", \"c2\", \"c3\"])\n",
      "        >>> df.cube(\"c2\", \"c3\").agg(grouping_id(), sum(\"c1\")).orderBy(\"c2\", \"c3\").show()\n",
      "        +----+----+-------------+-------+\n",
      "        |  c2|  c3|grouping_id()|sum(c1)|\n",
      "        +----+----+-------------+-------+\n",
      "        |null|null|            3|      8|\n",
      "        |null|   a|            2|      4|\n",
      "        |null|   c|            2|      4|\n",
      "        |   a|null|            1|      4|\n",
      "        |   a|   a|            0|      4|\n",
      "        |   b|null|            1|      4|\n",
      "        |   b|   c|            0|      4|\n",
      "        +----+----+-------------+-------+\n",
      "    \n",
      "    hash(*cols: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Calculates the hash code of given columns, and returns the result as an int column.\n",
      "        \n",
      "        .. versionadded:: 2.0.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        cols : :class:`~pyspark.sql.Column` or str\n",
      "            one or more columns to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            hash value as int column.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('ABC', 'DEF')], ['c1', 'c2'])\n",
      "        \n",
      "        Hash for one column\n",
      "        \n",
      "        >>> df.select(hash('c1').alias('hash')).show()\n",
      "        +----------+\n",
      "        |      hash|\n",
      "        +----------+\n",
      "        |-757602832|\n",
      "        +----------+\n",
      "        \n",
      "        Two or more columns\n",
      "        \n",
      "        >>> df.select(hash('c1', 'c2').alias('hash')).show()\n",
      "        +---------+\n",
      "        |     hash|\n",
      "        +---------+\n",
      "        |599895104|\n",
      "        +---------+\n",
      "    \n",
      "    hex(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes hex value of the given column, which could be :class:`pyspark.sql.types.StringType`,\n",
      "        :class:`pyspark.sql.types.BinaryType`, :class:`pyspark.sql.types.IntegerType` or\n",
      "        :class:`pyspark.sql.types.LongType`.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            hexadecimal representation of given value as string.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.createDataFrame([('ABC', 3)], ['a', 'b']).select(hex('a'), hex('b')).collect()\n",
      "        [Row(hex(a)='414243', hex(b)='3')]\n",
      "    \n",
      "    hour(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Extract the hours of a given timestamp as integer.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target date/timestamp column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            hour part of the timestamp as integer.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import datetime\n",
      "        >>> df = spark.createDataFrame([(datetime.datetime(2015, 4, 8, 13, 8, 15),)], ['ts'])\n",
      "        >>> df.select(hour('ts').alias('hour')).collect()\n",
      "        [Row(hour=13)]\n",
      "    \n",
      "    hours(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Partition transform function: A transform for timestamps\n",
      "        to partition data into hours.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target date or timestamp column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            data partitioned by hours.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.writeTo(\"catalog.db.table\").partitionedBy(   # doctest: +SKIP\n",
      "        ...     hours(\"ts\")\n",
      "        ... ).createOrReplace()\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This function can be used only in combination with\n",
      "        :py:meth:`~pyspark.sql.readwriter.DataFrameWriterV2.partitionedBy`\n",
      "        method of the `DataFrameWriterV2`.\n",
      "    \n",
      "    hypot(col1: Union[ForwardRef('ColumnOrName'), float], col2: Union[ForwardRef('ColumnOrName'), float]) -> pyspark.sql.column.Column\n",
      "        Computes ``sqrt(a^2 + b^2)`` without intermediate overflow or underflow.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col1 : str, :class:`~pyspark.sql.Column` or float\n",
      "            a leg.\n",
      "        col2 : str, :class:`~pyspark.sql.Column` or float\n",
      "            b leg.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            length of the hypotenuse.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(hypot(lit(1), lit(2))).first()\n",
      "        Row(HYPOT(1, 2)=2.23606...)\n",
      "    \n",
      "    initcap(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Translate the first letter of each word to upper case in the sentence.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            string with all first letters are uppercase in each word.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.createDataFrame([('ab cd',)], ['a']).select(initcap(\"a\").alias('v')).collect()\n",
      "        [Row(v='Ab Cd')]\n",
      "    \n",
      "    inline(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Explodes an array of structs into a table.\n",
      "        \n",
      "        .. versionadded:: 3.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            input column of values to explode.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            generator expression with the inline exploded result.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        :meth:`explode`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Row\n",
      "        >>> df = spark.createDataFrame([Row(structlist=[Row(a=1, b=2), Row(a=3, b=4)])])\n",
      "        >>> df.select(inline(df.structlist)).show()\n",
      "        +---+---+\n",
      "        |  a|  b|\n",
      "        +---+---+\n",
      "        |  1|  2|\n",
      "        |  3|  4|\n",
      "        +---+---+\n",
      "    \n",
      "    inline_outer(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Explodes an array of structs into a table.\n",
      "        Unlike inline, if the array is null or empty then null is produced for each nested column.\n",
      "        \n",
      "        .. versionadded:: 3.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            input column of values to explode.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            generator expression with the inline exploded result.\n",
      "        \n",
      "        See Also\n",
      "        --------\n",
      "        :meth:`explode_outer`\n",
      "        :meth:`inline`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Row\n",
      "        >>> df = spark.createDataFrame([\n",
      "        ...     Row(id=1, structlist=[Row(a=1, b=2), Row(a=3, b=4)]),\n",
      "        ...     Row(id=2, structlist=[])\n",
      "        ... ])\n",
      "        >>> df.select('id', inline_outer(df.structlist)).show()\n",
      "        +---+----+----+\n",
      "        | id|   a|   b|\n",
      "        +---+----+----+\n",
      "        |  1|   1|   2|\n",
      "        |  1|   3|   4|\n",
      "        |  2|null|null|\n",
      "        +---+----+----+\n",
      "    \n",
      "    input_file_name() -> pyspark.sql.column.Column\n",
      "        Creates a string column for the file name of the current Spark task.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            file names.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import os\n",
      "        >>> path = os.path.abspath(__file__)\n",
      "        >>> df = spark.read.text(path)\n",
      "        >>> df.select(input_file_name()).first()\n",
      "        Row(input_file_name()='file:///...')\n",
      "    \n",
      "    instr(str: 'ColumnOrName', substr: str) -> pyspark.sql.column.Column\n",
      "        Locate the position of the first occurrence of substr column in the given string.\n",
      "        Returns null if either of the arguments are null.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The position is not zero based, but 1 based index. Returns 0 if substr\n",
      "        could not be found in str.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        str : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        substr : str\n",
      "            substring to look for.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            location of the first occurence of the substring as integer.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('abcd',)], ['s',])\n",
      "        >>> df.select(instr(df.s, 'b').alias('s')).collect()\n",
      "        [Row(s=2)]\n",
      "    \n",
      "    isnan(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        An expression that returns true if the column is NaN.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            True if value is NaN and False otherwise.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1.0, float('nan')), (float('nan'), 2.0)], (\"a\", \"b\"))\n",
      "        >>> df.select(\"a\", \"b\", isnan(\"a\").alias(\"r1\"), isnan(df.b).alias(\"r2\")).show()\n",
      "        +---+---+-----+-----+\n",
      "        |  a|  b|   r1|   r2|\n",
      "        +---+---+-----+-----+\n",
      "        |1.0|NaN|false| true|\n",
      "        |NaN|2.0| true|false|\n",
      "        +---+---+-----+-----+\n",
      "    \n",
      "    isnull(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        An expression that returns true if the column is null.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            True if value is null and False otherwise.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1, None), (None, 2)], (\"a\", \"b\"))\n",
      "        >>> df.select(\"a\", \"b\", isnull(\"a\").alias(\"r1\"), isnull(df.b).alias(\"r2\")).show()\n",
      "        +----+----+-----+-----+\n",
      "        |   a|   b|   r1|   r2|\n",
      "        +----+----+-----+-----+\n",
      "        |   1|null|false| true|\n",
      "        |null|   2| true|false|\n",
      "        +----+----+-----+-----+\n",
      "    \n",
      "    json_tuple(col: 'ColumnOrName', *fields: str) -> pyspark.sql.column.Column\n",
      "        Creates a new row for a json column according to the given field names.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            string column in json format\n",
      "        fields : str\n",
      "            a field or fields to extract\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a new row for each given field value from json object\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> data = [(\"1\", '''{\"f1\": \"value1\", \"f2\": \"value2\"}'''), (\"2\", '''{\"f1\": \"value12\"}''')]\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"jstring\"))\n",
      "        >>> df.select(df.key, json_tuple(df.jstring, 'f1', 'f2')).collect()\n",
      "        [Row(key='1', c0='value1', c1='value2'), Row(key='2', c0='value12', c1=None)]\n",
      "    \n",
      "    kurtosis(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the kurtosis of the values in a group.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            kurtosis of given column.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([[1],[1],[2]], [\"c\"])\n",
      "        >>> df.select(kurtosis(df.c)).show()\n",
      "        +-----------+\n",
      "        |kurtosis(c)|\n",
      "        +-----------+\n",
      "        |       -1.5|\n",
      "        +-----------+\n",
      "    \n",
      "    lag(col: 'ColumnOrName', offset: int = 1, default: Optional[Any] = None) -> pyspark.sql.column.Column\n",
      "        Window function: returns the value that is `offset` rows before the current row, and\n",
      "        `default` if there is less than `offset` rows before the current row. For example,\n",
      "        an `offset` of one will return the previous row at any given point in the window partition.\n",
      "        \n",
      "        This is equivalent to the LAG function in SQL.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        offset : int, optional default 1\n",
      "            number of row to extend\n",
      "        default : optional\n",
      "            default value\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            value before current row based on `offset`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Window\n",
      "        >>> df = spark.createDataFrame([(\"a\", 1),\n",
      "        ...                             (\"a\", 2),\n",
      "        ...                             (\"a\", 3),\n",
      "        ...                             (\"b\", 8),\n",
      "        ...                             (\"b\", 2)], [\"c1\", \"c2\"])\n",
      "        >>> df.show()\n",
      "        +---+---+\n",
      "        | c1| c2|\n",
      "        +---+---+\n",
      "        |  a|  1|\n",
      "        |  a|  2|\n",
      "        |  a|  3|\n",
      "        |  b|  8|\n",
      "        |  b|  2|\n",
      "        +---+---+\n",
      "        >>> w = Window.partitionBy(\"c1\").orderBy(\"c2\")\n",
      "        >>> df.withColumn(\"previos_value\", lag(\"c2\").over(w)).show()\n",
      "        +---+---+-------------+\n",
      "        | c1| c2|previos_value|\n",
      "        +---+---+-------------+\n",
      "        |  a|  1|         null|\n",
      "        |  a|  2|            1|\n",
      "        |  a|  3|            2|\n",
      "        |  b|  2|         null|\n",
      "        |  b|  8|            2|\n",
      "        +---+---+-------------+\n",
      "        >>> df.withColumn(\"previos_value\", lag(\"c2\", 1, 0).over(w)).show()\n",
      "        +---+---+-------------+\n",
      "        | c1| c2|previos_value|\n",
      "        +---+---+-------------+\n",
      "        |  a|  1|            0|\n",
      "        |  a|  2|            1|\n",
      "        |  a|  3|            2|\n",
      "        |  b|  2|            0|\n",
      "        |  b|  8|            2|\n",
      "        +---+---+-------------+\n",
      "        >>> df.withColumn(\"previos_value\", lag(\"c2\", 2, -1).over(w)).show()\n",
      "        +---+---+-------------+\n",
      "        | c1| c2|previos_value|\n",
      "        +---+---+-------------+\n",
      "        |  a|  1|           -1|\n",
      "        |  a|  2|           -1|\n",
      "        |  a|  3|            1|\n",
      "        |  b|  2|           -1|\n",
      "        |  b|  8|           -1|\n",
      "        +---+---+-------------+\n",
      "    \n",
      "    last(col: 'ColumnOrName', ignorenulls: bool = False) -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the last value in a group.\n",
      "        \n",
      "        The function by default returns the last values it sees. It will return the last non-null\n",
      "        value it sees when ignoreNulls is set to true. If all values are null, then null is returned.\n",
      "        \n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The function is non-deterministic because its results depends on the order of the\n",
      "        rows which may be non-deterministic after a shuffle.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            column to fetch last value for.\n",
      "        ignorenulls : :class:`~pyspark.sql.Column` or str\n",
      "            if last value is null then look for non-null value.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            last value of the group.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"Alice\", 2), (\"Bob\", 5), (\"Alice\", None)], (\"name\", \"age\"))\n",
      "        >>> df = df.orderBy(df.age.desc())\n",
      "        >>> df.groupby(\"name\").agg(last(\"age\")).orderBy(\"name\").show()\n",
      "        +-----+---------+\n",
      "        | name|last(age)|\n",
      "        +-----+---------+\n",
      "        |Alice|     null|\n",
      "        |  Bob|        5|\n",
      "        +-----+---------+\n",
      "        \n",
      "        Now, to ignore any nulls we needs to set ``ignorenulls`` to `True`\n",
      "        \n",
      "        >>> df.groupby(\"name\").agg(last(\"age\", ignorenulls=True)).orderBy(\"name\").show()\n",
      "        +-----+---------+\n",
      "        | name|last(age)|\n",
      "        +-----+---------+\n",
      "        |Alice|        2|\n",
      "        |  Bob|        5|\n",
      "        +-----+---------+\n",
      "    \n",
      "    last_day(date: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the last day of the month which the given date belongs to.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        date : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            last day of the month.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('1997-02-10',)], ['d'])\n",
      "        >>> df.select(last_day(df.d).alias('date')).collect()\n",
      "        [Row(date=datetime.date(1997, 2, 28))]\n",
      "    \n",
      "    lead(col: 'ColumnOrName', offset: int = 1, default: Optional[Any] = None) -> pyspark.sql.column.Column\n",
      "        Window function: returns the value that is `offset` rows after the current row, and\n",
      "        `default` if there is less than `offset` rows after the current row. For example,\n",
      "        an `offset` of one will return the next row at any given point in the window partition.\n",
      "        \n",
      "        This is equivalent to the LEAD function in SQL.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        offset : int, optional default 1\n",
      "            number of row to extend\n",
      "        default : optional\n",
      "            default value\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            value after current row based on `offset`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Window\n",
      "        >>> df = spark.createDataFrame([(\"a\", 1),\n",
      "        ...                             (\"a\", 2),\n",
      "        ...                             (\"a\", 3),\n",
      "        ...                             (\"b\", 8),\n",
      "        ...                             (\"b\", 2)], [\"c1\", \"c2\"])\n",
      "        >>> df.show()\n",
      "        +---+---+\n",
      "        | c1| c2|\n",
      "        +---+---+\n",
      "        |  a|  1|\n",
      "        |  a|  2|\n",
      "        |  a|  3|\n",
      "        |  b|  8|\n",
      "        |  b|  2|\n",
      "        +---+---+\n",
      "        >>> w = Window.partitionBy(\"c1\").orderBy(\"c2\")\n",
      "        >>> df.withColumn(\"next_value\", lead(\"c2\").over(w)).show()\n",
      "        +---+---+----------+\n",
      "        | c1| c2|next_value|\n",
      "        +---+---+----------+\n",
      "        |  a|  1|         2|\n",
      "        |  a|  2|         3|\n",
      "        |  a|  3|      null|\n",
      "        |  b|  2|         8|\n",
      "        |  b|  8|      null|\n",
      "        +---+---+----------+\n",
      "        >>> df.withColumn(\"next_value\", lead(\"c2\", 1, 0).over(w)).show()\n",
      "        +---+---+----------+\n",
      "        | c1| c2|next_value|\n",
      "        +---+---+----------+\n",
      "        |  a|  1|         2|\n",
      "        |  a|  2|         3|\n",
      "        |  a|  3|         0|\n",
      "        |  b|  2|         8|\n",
      "        |  b|  8|         0|\n",
      "        +---+---+----------+\n",
      "        >>> df.withColumn(\"next_value\", lead(\"c2\", 2, -1).over(w)).show()\n",
      "        +---+---+----------+\n",
      "        | c1| c2|next_value|\n",
      "        +---+---+----------+\n",
      "        |  a|  1|         3|\n",
      "        |  a|  2|        -1|\n",
      "        |  a|  3|        -1|\n",
      "        |  b|  2|        -1|\n",
      "        |  b|  8|        -1|\n",
      "        +---+---+----------+\n",
      "    \n",
      "    least(*cols: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the least value of the list of column names, skipping null values.\n",
      "        This function takes at least 2 parameters. It will return null if all parameters are null.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        cols : :class:`~pyspark.sql.Column` or str\n",
      "            column names or columns to be compared\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            least value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1, 4, 3)], ['a', 'b', 'c'])\n",
      "        >>> df.select(least(df.a, df.b, df.c).alias(\"least\")).collect()\n",
      "        [Row(least=1)]\n",
      "    \n",
      "    length(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes the character length of string data or number of bytes of binary data.\n",
      "        The length of character data includes the trailing spaces. The length of binary data\n",
      "        includes binary zeros.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            length of the value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.createDataFrame([('ABC ',)], ['a']).select(length('a').alias('length')).collect()\n",
      "        [Row(length=4)]\n",
      "    \n",
      "    levenshtein(left: 'ColumnOrName', right: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes the Levenshtein distance of the two given strings.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        left : :class:`~pyspark.sql.Column` or str\n",
      "            first column value.\n",
      "        right : :class:`~pyspark.sql.Column` or str\n",
      "            second column value.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            Levenshtein distance as integer value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df0 = spark.createDataFrame([('kitten', 'sitting',)], ['l', 'r'])\n",
      "        >>> df0.select(levenshtein('l', 'r').alias('d')).collect()\n",
      "        [Row(d=3)]\n",
      "    \n",
      "    lit(col: Any) -> pyspark.sql.column.Column\n",
      "        Creates a :class:`~pyspark.sql.Column` of literal value.\n",
      "        \n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column`, str, int, float, bool or list, NumPy literals or ndarray.\n",
      "            the value to make it as a PySpark literal. If a column is passed,\n",
      "            it returns the column as is.\n",
      "        \n",
      "            .. versionchanged:: 3.4.0\n",
      "                Since 3.4.0, it supports the list type.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the literal instance.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(lit(5).alias('height'), df.id).show()\n",
      "        +------+---+\n",
      "        |height| id|\n",
      "        +------+---+\n",
      "        |     5|  0|\n",
      "        +------+---+\n",
      "        \n",
      "        Create a literal from a list.\n",
      "        \n",
      "        >>> spark.range(1).select(lit([1, 2, 3])).show()\n",
      "        +--------------+\n",
      "        |array(1, 2, 3)|\n",
      "        +--------------+\n",
      "        |     [1, 2, 3]|\n",
      "        +--------------+\n",
      "    \n",
      "    localtimestamp() -> pyspark.sql.column.Column\n",
      "        Returns the current timestamp without time zone at the start of query evaluation\n",
      "        as a timestamp without time zone column. All calls of localtimestamp within the\n",
      "        same query return the same value.\n",
      "        \n",
      "        .. versionadded:: 3.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            current local date and time.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(localtimestamp()).show(truncate=False) # doctest: +SKIP\n",
      "        +-----------------------+\n",
      "        |localtimestamp()       |\n",
      "        +-----------------------+\n",
      "        |2022-08-26 21:28:34.639|\n",
      "        +-----------------------+\n",
      "    \n",
      "    locate(substr: str, str: 'ColumnOrName', pos: int = 1) -> pyspark.sql.column.Column\n",
      "        Locate the position of the first occurrence of substr in a string column, after position pos.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        substr : str\n",
      "            a string\n",
      "        str : :class:`~pyspark.sql.Column` or str\n",
      "            a Column of :class:`pyspark.sql.types.StringType`\n",
      "        pos : int, optional\n",
      "            start position (zero based)\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            position of the substring.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The position is not zero based, but 1 based index. Returns 0 if substr\n",
      "        could not be found in str.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('abcd',)], ['s',])\n",
      "        >>> df.select(locate('b', df.s, 1).alias('s')).collect()\n",
      "        [Row(s=2)]\n",
      "    \n",
      "    log(arg1: Union[ForwardRef('ColumnOrName'), float], arg2: Optional[ForwardRef('ColumnOrName')] = None) -> pyspark.sql.column.Column\n",
      "        Returns the first argument-based logarithm of the second argument.\n",
      "        \n",
      "        If there is only one argument, then this takes the natural logarithm of the argument.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        arg1 : :class:`~pyspark.sql.Column`, str or float\n",
      "            base number or actual number (in this case base is `e`)\n",
      "        arg2 : :class:`~pyspark.sql.Column`, str or float\n",
      "            number to calculate logariphm for.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            logariphm of given value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([10, 100, 1000], \"INT\")\n",
      "        >>> df.select(log(10.0, df.value).alias('ten')).show() # doctest: +SKIP\n",
      "        +---+\n",
      "        |ten|\n",
      "        +---+\n",
      "        |1.0|\n",
      "        |2.0|\n",
      "        |3.0|\n",
      "        +---+\n",
      "        \n",
      "        And Natural logarithm\n",
      "        \n",
      "        >>> df.select(log(df.value)).show() # doctest: +SKIP\n",
      "        +-----------------+\n",
      "        |        ln(value)|\n",
      "        +-----------------+\n",
      "        |2.302585092994046|\n",
      "        |4.605170185988092|\n",
      "        |4.605170185988092|\n",
      "        +-----------------+\n",
      "    \n",
      "    log10(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes the logarithm of the given value in Base 10.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            column to calculate logarithm for.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            logarithm of the given value in Base 10.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(log10(lit(100))).show()\n",
      "        +----------+\n",
      "        |LOG10(100)|\n",
      "        +----------+\n",
      "        |       2.0|\n",
      "        +----------+\n",
      "    \n",
      "    log1p(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes the natural logarithm of the \"given value plus one\".\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            column to calculate natural logarithm for.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            natural logarithm of the \"given value plus one\".\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import math\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(log1p(lit(math.e))).first()\n",
      "        Row(LOG1P(2.71828...)=1.31326...)\n",
      "        \n",
      "        Same as:\n",
      "        \n",
      "        >>> df.select(log(lit(math.e+1))).first()\n",
      "        Row(ln(3.71828...)=1.31326...)\n",
      "    \n",
      "    log2(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the base-2 logarithm of the argument.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            a column to calculate logariphm for.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            logariphm of given value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(4,)], ['a'])\n",
      "        >>> df.select(log2('a').alias('log2')).show()\n",
      "        +----+\n",
      "        |log2|\n",
      "        +----+\n",
      "        | 2.0|\n",
      "        +----+\n",
      "    \n",
      "    lower(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Converts a string expression to lower case.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            lower case values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([\"Spark\", \"PySpark\", \"Pandas API\"], \"STRING\")\n",
      "        >>> df.select(lower(\"value\")).show()\n",
      "        +------------+\n",
      "        |lower(value)|\n",
      "        +------------+\n",
      "        |       spark|\n",
      "        |     pyspark|\n",
      "        |  pandas api|\n",
      "        +------------+\n",
      "    \n",
      "    lpad(col: 'ColumnOrName', len: int, pad: str) -> pyspark.sql.column.Column\n",
      "        Left-pad the string column to width `len` with `pad`.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        len : int\n",
      "            length of the final string.\n",
      "        pad : str\n",
      "            chars to prepend.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            left padded result.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('abcd',)], ['s',])\n",
      "        >>> df.select(lpad(df.s, 6, '#').alias('s')).collect()\n",
      "        [Row(s='##abcd')]\n",
      "    \n",
      "    ltrim(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Trim the spaces from left end for the specified string value.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            left trimmed values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([\"   Spark\", \"Spark  \", \" Spark\"], \"STRING\")\n",
      "        >>> df.select(ltrim(\"value\").alias(\"r\")).withColumn(\"length\", length(\"r\")).show()\n",
      "        +-------+------+\n",
      "        |      r|length|\n",
      "        +-------+------+\n",
      "        |  Spark|     5|\n",
      "        |Spark  |     7|\n",
      "        |  Spark|     5|\n",
      "        +-------+------+\n",
      "    \n",
      "    make_date(year: 'ColumnOrName', month: 'ColumnOrName', day: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a column with a date built from the year, month and day columns.\n",
      "        \n",
      "        .. versionadded:: 3.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        year : :class:`~pyspark.sql.Column` or str\n",
      "            The year to build the date\n",
      "        month : :class:`~pyspark.sql.Column` or str\n",
      "            The month to build the date\n",
      "        day : :class:`~pyspark.sql.Column` or str\n",
      "            The day to build the date\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a date built from given parts.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(2020, 6, 26)], ['Y', 'M', 'D'])\n",
      "        >>> df.select(make_date(df.Y, df.M, df.D).alias(\"datefield\")).collect()\n",
      "        [Row(datefield=datetime.date(2020, 6, 26))]\n",
      "    \n",
      "    map_concat(*cols: Union[ForwardRef('ColumnOrName'), List[ForwardRef('ColumnOrName_')], Tuple[ForwardRef('ColumnOrName_'), ...]]) -> pyspark.sql.column.Column\n",
      "        Returns the union of all the given maps.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        cols : :class:`~pyspark.sql.Column` or str\n",
      "            column names or :class:`~pyspark.sql.Column`\\s\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a map of merged entries from other maps.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.functions import map_concat\n",
      "        >>> df = spark.sql(\"SELECT map(1, 'a', 2, 'b') as map1, map(3, 'c') as map2\")\n",
      "        >>> df.select(map_concat(\"map1\", \"map2\").alias(\"map3\")).show(truncate=False)\n",
      "        +------------------------+\n",
      "        |map3                    |\n",
      "        +------------------------+\n",
      "        |{1 -> a, 2 -> b, 3 -> c}|\n",
      "        +------------------------+\n",
      "    \n",
      "    map_contains_key(col: 'ColumnOrName', value: Any) -> pyspark.sql.column.Column\n",
      "        Returns true if the map contains the key.\n",
      "        \n",
      "        .. versionadded:: 3.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        value :\n",
      "            a literal value\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            True if key is in the map and False otherwise.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.functions import map_contains_key\n",
      "        >>> df = spark.sql(\"SELECT map(1, 'a', 2, 'b') as data\")\n",
      "        >>> df.select(map_contains_key(\"data\", 1)).show()\n",
      "        +---------------------------------+\n",
      "        |array_contains(map_keys(data), 1)|\n",
      "        +---------------------------------+\n",
      "        |                             true|\n",
      "        +---------------------------------+\n",
      "        >>> df.select(map_contains_key(\"data\", -1)).show()\n",
      "        +----------------------------------+\n",
      "        |array_contains(map_keys(data), -1)|\n",
      "        +----------------------------------+\n",
      "        |                             false|\n",
      "        +----------------------------------+\n",
      "    \n",
      "    map_entries(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Collection function: Returns an unordered array of all entries in the given map.\n",
      "        \n",
      "        .. versionadded:: 3.0.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            an array of key value pairs as a struct type\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.functions import map_entries\n",
      "        >>> df = spark.sql(\"SELECT map(1, 'a', 2, 'b') as data\")\n",
      "        >>> df = df.select(map_entries(\"data\").alias(\"entries\"))\n",
      "        >>> df.show()\n",
      "        +----------------+\n",
      "        |         entries|\n",
      "        +----------------+\n",
      "        |[{1, a}, {2, b}]|\n",
      "        +----------------+\n",
      "        >>> df.printSchema()\n",
      "        root\n",
      "         |-- entries: array (nullable = false)\n",
      "         |    |-- element: struct (containsNull = false)\n",
      "         |    |    |-- key: integer (nullable = false)\n",
      "         |    |    |-- value: string (nullable = false)\n",
      "    \n",
      "    map_filter(col: 'ColumnOrName', f: Callable[[pyspark.sql.column.Column, pyspark.sql.column.Column], pyspark.sql.column.Column]) -> pyspark.sql.column.Column\n",
      "        Returns a map whose key-value pairs satisfy a predicate.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        f : function\n",
      "            a binary function ``(k: Column, v: Column) -> Column...``\n",
      "            Can use methods of :class:`~pyspark.sql.Column`, functions defined in\n",
      "            :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\n",
      "            Python ``UserDefinedFunctions`` are not supported\n",
      "            (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            filtered map.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1, {\"foo\": 42.0, \"bar\": 1.0, \"baz\": 32.0})], (\"id\", \"data\"))\n",
      "        >>> row = df.select(map_filter(\n",
      "        ...     \"data\", lambda _, v: v > 30.0).alias(\"data_filtered\")\n",
      "        ... ).head()\n",
      "        >>> sorted(row[\"data_filtered\"].items())\n",
      "        [('baz', 32.0), ('foo', 42.0)]\n",
      "    \n",
      "    map_from_arrays(col1: 'ColumnOrName', col2: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Creates a new map from two arrays.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col1 : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing a set of keys. All elements should not be null\n",
      "        col2 : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing a set of values\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a column of map type.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([2, 5], ['a', 'b'])], ['k', 'v'])\n",
      "        >>> df = df.select(map_from_arrays(df.k, df.v).alias(\"col\"))\n",
      "        >>> df.show()\n",
      "        +----------------+\n",
      "        |             col|\n",
      "        +----------------+\n",
      "        |{2 -> a, 5 -> b}|\n",
      "        +----------------+\n",
      "        >>> df.printSchema()\n",
      "        root\n",
      "         |-- col: map (nullable = true)\n",
      "         |    |-- key: long\n",
      "         |    |-- value: string (valueContainsNull = true)\n",
      "    \n",
      "    map_from_entries(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Collection function: Converts an array of entries (key value struct types) to a map\n",
      "        of values.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a map created from the given array of entries.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.functions import map_from_entries\n",
      "        >>> df = spark.sql(\"SELECT array(struct(1, 'a'), struct(2, 'b')) as data\")\n",
      "        >>> df.select(map_from_entries(\"data\").alias(\"map\")).show()\n",
      "        +----------------+\n",
      "        |             map|\n",
      "        +----------------+\n",
      "        |{1 -> a, 2 -> b}|\n",
      "        +----------------+\n",
      "    \n",
      "    map_keys(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Collection function: Returns an unordered array containing the keys of the map.\n",
      "        \n",
      "        .. versionadded:: 2.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            keys of the map as an array.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.functions import map_keys\n",
      "        >>> df = spark.sql(\"SELECT map(1, 'a', 2, 'b') as data\")\n",
      "        >>> df.select(map_keys(\"data\").alias(\"keys\")).show()\n",
      "        +------+\n",
      "        |  keys|\n",
      "        +------+\n",
      "        |[1, 2]|\n",
      "        +------+\n",
      "    \n",
      "    map_values(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Collection function: Returns an unordered array containing the values of the map.\n",
      "        \n",
      "        .. versionadded:: 2.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            values of the map as an array.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.functions import map_values\n",
      "        >>> df = spark.sql(\"SELECT map(1, 'a', 2, 'b') as data\")\n",
      "        >>> df.select(map_values(\"data\").alias(\"values\")).show()\n",
      "        +------+\n",
      "        |values|\n",
      "        +------+\n",
      "        |[a, b]|\n",
      "        +------+\n",
      "    \n",
      "    map_zip_with(col1: 'ColumnOrName', col2: 'ColumnOrName', f: Callable[[pyspark.sql.column.Column, pyspark.sql.column.Column, pyspark.sql.column.Column], pyspark.sql.column.Column]) -> pyspark.sql.column.Column\n",
      "        Merge two given maps, key-wise into a single map using a function.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col1 : :class:`~pyspark.sql.Column` or str\n",
      "            name of the first column or expression\n",
      "        col2 : :class:`~pyspark.sql.Column` or str\n",
      "            name of the second column or expression\n",
      "        f : function\n",
      "            a ternary function ``(k: Column, v1: Column, v2: Column) -> Column...``\n",
      "            Can use methods of :class:`~pyspark.sql.Column`, functions defined in\n",
      "            :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\n",
      "            Python ``UserDefinedFunctions`` are not supported\n",
      "            (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            zipped map where entries are calculated by applying given function to each\n",
      "            pair of arguments.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([\n",
      "        ...     (1, {\"IT\": 24.0, \"SALES\": 12.00}, {\"IT\": 2.0, \"SALES\": 1.4})],\n",
      "        ...     (\"id\", \"base\", \"ratio\")\n",
      "        ... )\n",
      "        >>> row = df.select(map_zip_with(\n",
      "        ...     \"base\", \"ratio\", lambda k, v1, v2: round(v1 * v2, 2)).alias(\"updated_data\")\n",
      "        ... ).head()\n",
      "        >>> sorted(row[\"updated_data\"].items())\n",
      "        [('IT', 48.0), ('SALES', 16.8)]\n",
      "    \n",
      "    max(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the maximum value of the expression in a group.\n",
      "        \n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(10)\n",
      "        >>> df.select(max(col(\"id\"))).show()\n",
      "        +-------+\n",
      "        |max(id)|\n",
      "        +-------+\n",
      "        |      9|\n",
      "        +-------+\n",
      "    \n",
      "    max_by(col: 'ColumnOrName', ord: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the value associated with the maximum value of ord.\n",
      "        \n",
      "        .. versionadded:: 3.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        ord : :class:`~pyspark.sql.Column` or str\n",
      "            column to be maximized\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            value associated with the maximum value of ord.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([\n",
      "        ...     (\"Java\", 2012, 20000), (\"dotNET\", 2012, 5000),\n",
      "        ...     (\"dotNET\", 2013, 48000), (\"Java\", 2013, 30000)],\n",
      "        ...     schema=(\"course\", \"year\", \"earnings\"))\n",
      "        >>> df.groupby(\"course\").agg(max_by(\"year\", \"earnings\")).show()\n",
      "        +------+----------------------+\n",
      "        |course|max_by(year, earnings)|\n",
      "        +------+----------------------+\n",
      "        |  Java|                  2013|\n",
      "        |dotNET|                  2013|\n",
      "        +------+----------------------+\n",
      "    \n",
      "    md5(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Calculates the MD5 digest and returns the value as a 32 character hex string.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.createDataFrame([('ABC',)], ['a']).select(md5('a').alias('hash')).collect()\n",
      "        [Row(hash='902fbdd2b1df0c4f70b4a5d23525e932')]\n",
      "    \n",
      "    mean(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the average of the values in a group.\n",
      "        An alias of :func:`avg`.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(10)\n",
      "        >>> df.select(mean(df.id)).show()\n",
      "        +-------+\n",
      "        |avg(id)|\n",
      "        +-------+\n",
      "        |    4.5|\n",
      "        +-------+\n",
      "    \n",
      "    median(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the median of the values in a group.\n",
      "        \n",
      "        .. versionadded:: 3.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the median of the values in a group.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([\n",
      "        ...     (\"Java\", 2012, 20000), (\"dotNET\", 2012, 5000),\n",
      "        ...     (\"Java\", 2012, 22000), (\"dotNET\", 2012, 10000),\n",
      "        ...     (\"dotNET\", 2013, 48000), (\"Java\", 2013, 30000)],\n",
      "        ...     schema=(\"course\", \"year\", \"earnings\"))\n",
      "        >>> df.groupby(\"course\").agg(median(\"earnings\")).show()\n",
      "        +------+----------------+\n",
      "        |course|median(earnings)|\n",
      "        +------+----------------+\n",
      "        |  Java|         22000.0|\n",
      "        |dotNET|         10000.0|\n",
      "        +------+----------------+\n",
      "    \n",
      "    min(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the minimum value of the expression in a group.\n",
      "        \n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(10)\n",
      "        >>> df.select(min(df.id)).show()\n",
      "        +-------+\n",
      "        |min(id)|\n",
      "        +-------+\n",
      "        |      0|\n",
      "        +-------+\n",
      "    \n",
      "    min_by(col: 'ColumnOrName', ord: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the value associated with the minimum value of ord.\n",
      "        \n",
      "        .. versionadded:: 3.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        ord : :class:`~pyspark.sql.Column` or str\n",
      "            column to be minimized\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            value associated with the minimum value of ord.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([\n",
      "        ...     (\"Java\", 2012, 20000), (\"dotNET\", 2012, 5000),\n",
      "        ...     (\"dotNET\", 2013, 48000), (\"Java\", 2013, 30000)],\n",
      "        ...     schema=(\"course\", \"year\", \"earnings\"))\n",
      "        >>> df.groupby(\"course\").agg(min_by(\"year\", \"earnings\")).show()\n",
      "        +------+----------------------+\n",
      "        |course|min_by(year, earnings)|\n",
      "        +------+----------------------+\n",
      "        |  Java|                  2012|\n",
      "        |dotNET|                  2012|\n",
      "        +------+----------------------+\n",
      "    \n",
      "    minute(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Extract the minutes of a given timestamp as integer.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target date/timestamp column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            minutes part of the timestamp as integer.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import datetime\n",
      "        >>> df = spark.createDataFrame([(datetime.datetime(2015, 4, 8, 13, 8, 15),)], ['ts'])\n",
      "        >>> df.select(minute('ts').alias('minute')).collect()\n",
      "        [Row(minute=8)]\n",
      "    \n",
      "    mode(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the most frequent value in a group.\n",
      "        \n",
      "        .. versionadded:: 3.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the most frequent value in a group.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([\n",
      "        ...     (\"Java\", 2012, 20000), (\"dotNET\", 2012, 5000),\n",
      "        ...     (\"Java\", 2012, 20000), (\"dotNET\", 2012, 5000),\n",
      "        ...     (\"dotNET\", 2013, 48000), (\"Java\", 2013, 30000)],\n",
      "        ...     schema=(\"course\", \"year\", \"earnings\"))\n",
      "        >>> df.groupby(\"course\").agg(mode(\"year\")).show()\n",
      "        +------+----------+\n",
      "        |course|mode(year)|\n",
      "        +------+----------+\n",
      "        |  Java|      2012|\n",
      "        |dotNET|      2012|\n",
      "        +------+----------+\n",
      "    \n",
      "    monotonically_increasing_id() -> pyspark.sql.column.Column\n",
      "        A column that generates monotonically increasing 64-bit integers.\n",
      "        \n",
      "        The generated ID is guaranteed to be monotonically increasing and unique, but not consecutive.\n",
      "        The current implementation puts the partition ID in the upper 31 bits, and the record number\n",
      "        within each partition in the lower 33 bits. The assumption is that the data frame has\n",
      "        less than 1 billion partitions, and each partition has less than 8 billion records.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The function is non-deterministic because its result depends on partition IDs.\n",
      "        \n",
      "        As an example, consider a :class:`DataFrame` with two partitions, each with 3 records.\n",
      "        This expression would return the following IDs:\n",
      "        0, 1, 2, 8589934592 (1L << 33), 8589934593, 8589934594.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            last value of the group.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df0 = sc.parallelize(range(2), 2).mapPartitions(lambda x: [(1,), (2,), (3,)]).toDF(['col1'])\n",
      "        >>> df0.select(monotonically_increasing_id().alias('id')).collect()\n",
      "        [Row(id=0), Row(id=1), Row(id=2), Row(id=8589934592), Row(id=8589934593), Row(id=8589934594)]\n",
      "    \n",
      "    month(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Extract the month of a given date/timestamp as integer.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target date/timestamp column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            month part of the date/timestamp as integer.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(month('dt').alias('month')).collect()\n",
      "        [Row(month=4)]\n",
      "    \n",
      "    months(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Partition transform function: A transform for timestamps and dates\n",
      "        to partition data into months.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target date or timestamp column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            data partitioned by months.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.writeTo(\"catalog.db.table\").partitionedBy(\n",
      "        ...     months(\"ts\")\n",
      "        ... ).createOrReplace()  # doctest: +SKIP\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This function can be used only in combination with\n",
      "        :py:meth:`~pyspark.sql.readwriter.DataFrameWriterV2.partitionedBy`\n",
      "        method of the `DataFrameWriterV2`.\n",
      "    \n",
      "    months_between(date1: 'ColumnOrName', date2: 'ColumnOrName', roundOff: bool = True) -> pyspark.sql.column.Column\n",
      "        Returns number of months between dates date1 and date2.\n",
      "        If date1 is later than date2, then the result is positive.\n",
      "        A whole number is returned if both inputs have the same day of month or both are the last day\n",
      "        of their respective months. Otherwise, the difference is calculated assuming 31 days per month.\n",
      "        The result is rounded off to 8 digits unless `roundOff` is set to `False`.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        date1 : :class:`~pyspark.sql.Column` or str\n",
      "            first date column.\n",
      "        date2 : :class:`~pyspark.sql.Column` or str\n",
      "            second date column.\n",
      "        roundOff : bool, optional\n",
      "            whether to round (to 8 digits) the final value or not (default: True).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            number of months between two dates.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('1997-02-28 10:30:00', '1996-10-30')], ['date1', 'date2'])\n",
      "        >>> df.select(months_between(df.date1, df.date2).alias('months')).collect()\n",
      "        [Row(months=3.94959677)]\n",
      "        >>> df.select(months_between(df.date1, df.date2, False).alias('months')).collect()\n",
      "        [Row(months=3.9495967741935485)]\n",
      "    \n",
      "    nanvl(col1: 'ColumnOrName', col2: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns col1 if it is not NaN, or col2 if col1 is NaN.\n",
      "        \n",
      "        Both inputs should be floating point columns (:class:`DoubleType` or :class:`FloatType`).\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col1 : :class:`~pyspark.sql.Column` or str\n",
      "            first column to check.\n",
      "        col2 : :class:`~pyspark.sql.Column` or str\n",
      "            second column to return if first is NaN.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            value from first column or second if first is NaN .\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1.0, float('nan')), (float('nan'), 2.0)], (\"a\", \"b\"))\n",
      "        >>> df.select(nanvl(\"a\", \"b\").alias(\"r1\"), nanvl(df.a, df.b).alias(\"r2\")).collect()\n",
      "        [Row(r1=1.0, r2=1.0), Row(r1=2.0, r2=2.0)]\n",
      "    \n",
      "    next_day(date: 'ColumnOrName', dayOfWeek: str) -> pyspark.sql.column.Column\n",
      "        Returns the first date which is later than the value of the date column\n",
      "        based on second `week day` argument.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        date : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        dayOfWeek : str\n",
      "            day of the week, case-insensitive, accepts:\n",
      "                \"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column of computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-07-27',)], ['d'])\n",
      "        >>> df.select(next_day(df.d, 'Sun').alias('date')).collect()\n",
      "        [Row(date=datetime.date(2015, 8, 2))]\n",
      "    \n",
      "    nth_value(col: 'ColumnOrName', offset: int, ignoreNulls: Optional[bool] = False) -> pyspark.sql.column.Column\n",
      "        Window function: returns the value that is the `offset`\\th row of the window frame\n",
      "        (counting from 1), and `null` if the size of window frame is less than `offset` rows.\n",
      "        \n",
      "        It will return the `offset`\\th non-null value it sees when `ignoreNulls` is set to\n",
      "        true. If all values are null, then null is returned.\n",
      "        \n",
      "        This is equivalent to the nth_value function in SQL.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        offset : int\n",
      "            number of row to use as the value\n",
      "        ignoreNulls : bool, optional\n",
      "            indicates the Nth value should skip null in the\n",
      "            determination of which row to use\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            value of nth row.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Window\n",
      "        >>> df = spark.createDataFrame([(\"a\", 1),\n",
      "        ...                             (\"a\", 2),\n",
      "        ...                             (\"a\", 3),\n",
      "        ...                             (\"b\", 8),\n",
      "        ...                             (\"b\", 2)], [\"c1\", \"c2\"])\n",
      "        >>> df.show()\n",
      "        +---+---+\n",
      "        | c1| c2|\n",
      "        +---+---+\n",
      "        |  a|  1|\n",
      "        |  a|  2|\n",
      "        |  a|  3|\n",
      "        |  b|  8|\n",
      "        |  b|  2|\n",
      "        +---+---+\n",
      "        >>> w = Window.partitionBy(\"c1\").orderBy(\"c2\")\n",
      "        >>> df.withColumn(\"nth_value\", nth_value(\"c2\", 1).over(w)).show()\n",
      "        +---+---+---------+\n",
      "        | c1| c2|nth_value|\n",
      "        +---+---+---------+\n",
      "        |  a|  1|        1|\n",
      "        |  a|  2|        1|\n",
      "        |  a|  3|        1|\n",
      "        |  b|  2|        2|\n",
      "        |  b|  8|        2|\n",
      "        +---+---+---------+\n",
      "        >>> df.withColumn(\"nth_value\", nth_value(\"c2\", 2).over(w)).show()\n",
      "        +---+---+---------+\n",
      "        | c1| c2|nth_value|\n",
      "        +---+---+---------+\n",
      "        |  a|  1|     null|\n",
      "        |  a|  2|        2|\n",
      "        |  a|  3|        2|\n",
      "        |  b|  2|     null|\n",
      "        |  b|  8|        8|\n",
      "        +---+---+---------+\n",
      "    \n",
      "    ntile(n: int) -> pyspark.sql.column.Column\n",
      "        Window function: returns the ntile group id (from 1 to `n` inclusive)\n",
      "        in an ordered window partition. For example, if `n` is 4, the first\n",
      "        quarter of the rows will get value 1, the second quarter will get 2,\n",
      "        the third quarter will get 3, and the last quarter will get 4.\n",
      "        \n",
      "        This is equivalent to the NTILE function in SQL.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        n : int\n",
      "            an integer\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            portioned group id.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Window\n",
      "        >>> df = spark.createDataFrame([(\"a\", 1),\n",
      "        ...                             (\"a\", 2),\n",
      "        ...                             (\"a\", 3),\n",
      "        ...                             (\"b\", 8),\n",
      "        ...                             (\"b\", 2)], [\"c1\", \"c2\"])\n",
      "        >>> df.show()\n",
      "        +---+---+\n",
      "        | c1| c2|\n",
      "        +---+---+\n",
      "        |  a|  1|\n",
      "        |  a|  2|\n",
      "        |  a|  3|\n",
      "        |  b|  8|\n",
      "        |  b|  2|\n",
      "        +---+---+\n",
      "        >>> w = Window.partitionBy(\"c1\").orderBy(\"c2\")\n",
      "        >>> df.withColumn(\"ntile\", ntile(2).over(w)).show()\n",
      "        +---+---+-----+\n",
      "        | c1| c2|ntile|\n",
      "        +---+---+-----+\n",
      "        |  a|  1|    1|\n",
      "        |  a|  2|    1|\n",
      "        |  a|  3|    2|\n",
      "        |  b|  2|    1|\n",
      "        |  b|  8|    2|\n",
      "        +---+---+-----+\n",
      "    \n",
      "    octet_length(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Calculates the byte length for the specified string column.\n",
      "        \n",
      "        .. versionadded:: 3.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            Source column or strings\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            Byte length of the col\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.functions import octet_length\n",
      "        >>> spark.createDataFrame([('cat',), ( '🐈',)], ['cat']) \\\n",
      "        ...      .select(octet_length('cat')).collect()\n",
      "            [Row(octet_length(cat)=3), Row(octet_length(cat)=4)]\n",
      "    \n",
      "    overlay(src: 'ColumnOrName', replace: 'ColumnOrName', pos: Union[ForwardRef('ColumnOrName'), int], len: Union[ForwardRef('ColumnOrName'), int] = -1) -> pyspark.sql.column.Column\n",
      "        Overlay the specified portion of `src` with `replace`,\n",
      "        starting from byte position `pos` of `src` and proceeding for `len` bytes.\n",
      "        \n",
      "        .. versionadded:: 3.0.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        src : :class:`~pyspark.sql.Column` or str\n",
      "            column name or column containing the string that will be replaced\n",
      "        replace : :class:`~pyspark.sql.Column` or str\n",
      "            column name or column containing the substitution string\n",
      "        pos : :class:`~pyspark.sql.Column` or str or int\n",
      "            column name, column, or int containing the starting position in src\n",
      "        len : :class:`~pyspark.sql.Column` or str or int, optional\n",
      "            column name, column, or int containing the number of bytes to replace in src\n",
      "            string by 'replace' defaults to -1, which represents the length of the 'replace' string\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            string with replaced values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"SPARK_SQL\", \"CORE\")], (\"x\", \"y\"))\n",
      "        >>> df.select(overlay(\"x\", \"y\", 7).alias(\"overlayed\")).collect()\n",
      "        [Row(overlayed='SPARK_CORE')]\n",
      "        >>> df.select(overlay(\"x\", \"y\", 7, 0).alias(\"overlayed\")).collect()\n",
      "        [Row(overlayed='SPARK_CORESQL')]\n",
      "        >>> df.select(overlay(\"x\", \"y\", 7, 2).alias(\"overlayed\")).collect()\n",
      "        [Row(overlayed='SPARK_COREL')]\n",
      "    \n",
      "    percent_rank() -> pyspark.sql.column.Column\n",
      "        Window function: returns the relative rank (i.e. percentile) of rows within a window partition.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for calculating relative rank.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Window, types\n",
      "        >>> df = spark.createDataFrame([1, 1, 2, 3, 3, 4], types.IntegerType())\n",
      "        >>> w = Window.orderBy(\"value\")\n",
      "        >>> df.withColumn(\"pr\", percent_rank().over(w)).show()\n",
      "        +-----+---+\n",
      "        |value| pr|\n",
      "        +-----+---+\n",
      "        |    1|0.0|\n",
      "        |    1|0.0|\n",
      "        |    2|0.4|\n",
      "        |    3|0.6|\n",
      "        |    3|0.6|\n",
      "        |    4|1.0|\n",
      "        +-----+---+\n",
      "    \n",
      "    percentile_approx(col: 'ColumnOrName', percentage: Union[pyspark.sql.column.Column, float, List[float], Tuple[float]], accuracy: Union[pyspark.sql.column.Column, float] = 10000) -> pyspark.sql.column.Column\n",
      "        Returns the approximate `percentile` of the numeric column `col` which is the smallest value\n",
      "        in the ordered `col` values (sorted from least to greatest) such that no more than `percentage`\n",
      "        of `col` values is less than the value or equal to that value.\n",
      "        \n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            input column.\n",
      "        percentage : :class:`~pyspark.sql.Column`, float, list of floats or tuple of floats\n",
      "            percentage in decimal (must be between 0.0 and 1.0).\n",
      "            When percentage is an array, each value of the percentage array must be between 0.0 and 1.0.\n",
      "            In this case, returns the approximate percentile array of column col\n",
      "            at the given percentage array.\n",
      "        accuracy : :class:`~pyspark.sql.Column` or float\n",
      "            is a positive numeric literal which controls approximation accuracy\n",
      "            at the cost of memory. Higher value of accuracy yields better accuracy,\n",
      "            1.0/accuracy is the relative error of the approximation. (default: 10000).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            approximate `percentile` of the numeric column.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> key = (col(\"id\") % 3).alias(\"key\")\n",
      "        >>> value = (randn(42) + key * 10).alias(\"value\")\n",
      "        >>> df = spark.range(0, 1000, 1, 1).select(key, value)\n",
      "        >>> df.select(\n",
      "        ...     percentile_approx(\"value\", [0.25, 0.5, 0.75], 1000000).alias(\"quantiles\")\n",
      "        ... ).printSchema()\n",
      "        root\n",
      "         |-- quantiles: array (nullable = true)\n",
      "         |    |-- element: double (containsNull = false)\n",
      "        \n",
      "        >>> df.groupBy(\"key\").agg(\n",
      "        ...     percentile_approx(\"value\", 0.5, lit(1000000)).alias(\"median\")\n",
      "        ... ).printSchema()\n",
      "        root\n",
      "         |-- key: long (nullable = true)\n",
      "         |-- median: double (nullable = true)\n",
      "    \n",
      "    pmod(dividend: Union[ForwardRef('ColumnOrName'), float], divisor: Union[ForwardRef('ColumnOrName'), float]) -> pyspark.sql.column.Column\n",
      "        Returns the positive value of dividend mod divisor.\n",
      "        \n",
      "        .. versionadded:: 3.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        dividend : str, :class:`~pyspark.sql.Column` or float\n",
      "            the column that contains dividend, or the specified dividend value\n",
      "        divisor : str, :class:`~pyspark.sql.Column` or float\n",
      "            the column that contains divisor, or the specified divisor value\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            positive value of dividend mod divisor.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.functions import pmod\n",
      "        >>> df = spark.createDataFrame([\n",
      "        ...     (1.0, float('nan')), (float('nan'), 2.0), (10.0, 3.0),\n",
      "        ...     (float('nan'), float('nan')), (-3.0, 4.0), (-10.0, 3.0),\n",
      "        ...     (-5.0, -6.0), (7.0, -8.0), (1.0, 2.0)],\n",
      "        ...     (\"a\", \"b\"))\n",
      "        >>> df.select(pmod(\"a\", \"b\")).show()\n",
      "        +----------+\n",
      "        |pmod(a, b)|\n",
      "        +----------+\n",
      "        |       NaN|\n",
      "        |       NaN|\n",
      "        |       1.0|\n",
      "        |       NaN|\n",
      "        |       1.0|\n",
      "        |       2.0|\n",
      "        |      -5.0|\n",
      "        |       7.0|\n",
      "        |       1.0|\n",
      "        +----------+\n",
      "    \n",
      "    posexplode(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a new row for each element with position in the given array or map.\n",
      "        Uses the default column name `pos` for position, and `col` for elements in the\n",
      "        array and `key` and `value` for elements in the map unless specified otherwise.\n",
      "        \n",
      "        .. versionadded:: 2.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            one row per array item or map key value including positions as a separate column.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Row\n",
      "        >>> eDF = spark.createDataFrame([Row(a=1, intlist=[1,2,3], mapfield={\"a\": \"b\"})])\n",
      "        >>> eDF.select(posexplode(eDF.intlist)).collect()\n",
      "        [Row(pos=0, col=1), Row(pos=1, col=2), Row(pos=2, col=3)]\n",
      "        \n",
      "        >>> eDF.select(posexplode(eDF.mapfield)).show()\n",
      "        +---+---+-----+\n",
      "        |pos|key|value|\n",
      "        +---+---+-----+\n",
      "        |  0|  a|    b|\n",
      "        +---+---+-----+\n",
      "    \n",
      "    posexplode_outer(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns a new row for each element with position in the given array or map.\n",
      "        Unlike posexplode, if the array/map is null or empty then the row (null, null) is produced.\n",
      "        Uses the default column name `pos` for position, and `col` for elements in the\n",
      "        array and `key` and `value` for elements in the map unless specified otherwise.\n",
      "        \n",
      "        .. versionadded:: 2.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            one row per array item or map key value including positions as a separate column.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame(\n",
      "        ...     [(1, [\"foo\", \"bar\"], {\"x\": 1.0}), (2, [], {}), (3, None, None)],\n",
      "        ...     (\"id\", \"an_array\", \"a_map\")\n",
      "        ... )\n",
      "        >>> df.select(\"id\", \"an_array\", posexplode_outer(\"a_map\")).show()\n",
      "        +---+----------+----+----+-----+\n",
      "        | id|  an_array| pos| key|value|\n",
      "        +---+----------+----+----+-----+\n",
      "        |  1|[foo, bar]|   0|   x|  1.0|\n",
      "        |  2|        []|null|null| null|\n",
      "        |  3|      null|null|null| null|\n",
      "        +---+----------+----+----+-----+\n",
      "        >>> df.select(\"id\", \"a_map\", posexplode_outer(\"an_array\")).show()\n",
      "        +---+----------+----+----+\n",
      "        | id|     a_map| pos| col|\n",
      "        +---+----------+----+----+\n",
      "        |  1|{x -> 1.0}|   0| foo|\n",
      "        |  1|{x -> 1.0}|   1| bar|\n",
      "        |  2|        {}|null|null|\n",
      "        |  3|      null|null|null|\n",
      "        +---+----------+----+----+\n",
      "    \n",
      "    pow(col1: Union[ForwardRef('ColumnOrName'), float], col2: Union[ForwardRef('ColumnOrName'), float]) -> pyspark.sql.column.Column\n",
      "        Returns the value of the first argument raised to the power of the second argument.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col1 : str, :class:`~pyspark.sql.Column` or float\n",
      "            the base number.\n",
      "        col2 : str, :class:`~pyspark.sql.Column` or float\n",
      "            the exponent number.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the base rased to the power the argument.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(pow(lit(3), lit(2))).first()\n",
      "        Row(POWER(3, 2)=9.0)\n",
      "    \n",
      "    product(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the product of the values in a group.\n",
      "        \n",
      "        .. versionadded:: 3.2.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : str, :class:`Column`\n",
      "            column containing values to be multiplied together\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1, 10).toDF('x').withColumn('mod3', col('x') % 3)\n",
      "        >>> prods = df.groupBy('mod3').agg(product('x').alias('product'))\n",
      "        >>> prods.orderBy('mod3').show()\n",
      "        +----+-------+\n",
      "        |mod3|product|\n",
      "        +----+-------+\n",
      "        |   0|  162.0|\n",
      "        |   1|   28.0|\n",
      "        |   2|   80.0|\n",
      "        +----+-------+\n",
      "    \n",
      "    quarter(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Extract the quarter of a given date/timestamp as integer.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target date/timestamp column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            quarter of the date/timestamp as integer.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(quarter('dt').alias('quarter')).collect()\n",
      "        [Row(quarter=2)]\n",
      "    \n",
      "    radians(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Converts an angle measured in degrees to an approximately equivalent angle\n",
      "        measured in radians.\n",
      "        \n",
      "        .. versionadded:: 2.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            angle in degrees\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            angle in radians, as if computed by `java.lang.Math.toRadians()`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(radians(lit(180))).first()\n",
      "        Row(RADIANS(180)=3.14159...)\n",
      "    \n",
      "    raise_error(errMsg: Union[pyspark.sql.column.Column, str]) -> pyspark.sql.column.Column\n",
      "        Throws an exception with the provided error message.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        errMsg : :class:`~pyspark.sql.Column` or str\n",
      "            A Python string literal or column containing the error message\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            throws an error with specified message.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(raise_error(\"My error message\")).show() # doctest: +SKIP\n",
      "        ...\n",
      "        java.lang.RuntimeException: My error message\n",
      "        ...\n",
      "    \n",
      "    rand(seed: Optional[int] = None) -> pyspark.sql.column.Column\n",
      "        Generates a random column with independent and identically distributed (i.i.d.) samples\n",
      "        uniformly distributed in [0.0, 1.0).\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The function is non-deterministic in general case.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        seed : int (default: None)\n",
      "            seed value for random generator.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            random values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(2)\n",
      "        >>> df.withColumn('rand', rand(seed=42) * 3).show() # doctest: +SKIP\n",
      "        +---+------------------+\n",
      "        | id|              rand|\n",
      "        +---+------------------+\n",
      "        |  0|1.4385751892400076|\n",
      "        |  1|1.7082186019706387|\n",
      "        +---+------------------+\n",
      "    \n",
      "    randn(seed: Optional[int] = None) -> pyspark.sql.column.Column\n",
      "        Generates a column with independent and identically distributed (i.i.d.) samples from\n",
      "        the standard normal distribution.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The function is non-deterministic in general case.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        seed : int (default: None)\n",
      "            seed value for random generator.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            random values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(2)\n",
      "        >>> df.withColumn('randn', randn(seed=42)).show() # doctest: +SKIP\n",
      "        +---+--------------------+\n",
      "        | id|               randn|\n",
      "        +---+--------------------+\n",
      "        |  0|-0.04167221574820542|\n",
      "        |  1| 0.15241403986452778|\n",
      "        +---+--------------------+\n",
      "    \n",
      "    rank() -> pyspark.sql.column.Column\n",
      "        Window function: returns the rank of rows within a window partition.\n",
      "        \n",
      "        The difference between rank and dense_rank is that dense_rank leaves no gaps in ranking\n",
      "        sequence when there are ties. That is, if you were ranking a competition using dense_rank\n",
      "        and had three people tie for second place, you would say that all three were in second\n",
      "        place and that the next person came in third. Rank would give me sequential numbers, making\n",
      "        the person that came in third place (after the ties) would register as coming in fifth.\n",
      "        \n",
      "        This is equivalent to the RANK function in SQL.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for calculating ranks.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Window, types\n",
      "        >>> df = spark.createDataFrame([1, 1, 2, 3, 3, 4], types.IntegerType())\n",
      "        >>> w = Window.orderBy(\"value\")\n",
      "        >>> df.withColumn(\"drank\", rank().over(w)).show()\n",
      "        +-----+-----+\n",
      "        |value|drank|\n",
      "        +-----+-----+\n",
      "        |    1|    1|\n",
      "        |    1|    1|\n",
      "        |    2|    3|\n",
      "        |    3|    4|\n",
      "        |    3|    4|\n",
      "        |    4|    6|\n",
      "        +-----+-----+\n",
      "    \n",
      "    regexp_extract(str: 'ColumnOrName', pattern: str, idx: int) -> pyspark.sql.column.Column\n",
      "        Extract a specific group matched by a Java regex, from the specified string column.\n",
      "        If the regex did not match, or the specified group did not match, an empty string is returned.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        str : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        pattern : str\n",
      "            regex pattern to apply.\n",
      "        idx : int\n",
      "            matched group id.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            matched value specified by `idx` group id.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('100-200',)], ['str'])\n",
      "        >>> df.select(regexp_extract('str', r'(\\d+)-(\\d+)', 1).alias('d')).collect()\n",
      "        [Row(d='100')]\n",
      "        >>> df = spark.createDataFrame([('foo',)], ['str'])\n",
      "        >>> df.select(regexp_extract('str', r'(\\d+)', 1).alias('d')).collect()\n",
      "        [Row(d='')]\n",
      "        >>> df = spark.createDataFrame([('aaaac',)], ['str'])\n",
      "        >>> df.select(regexp_extract('str', '(a+)(b)?(c)', 2).alias('d')).collect()\n",
      "        [Row(d='')]\n",
      "    \n",
      "    regexp_replace(string: 'ColumnOrName', pattern: Union[str, pyspark.sql.column.Column], replacement: Union[str, pyspark.sql.column.Column]) -> pyspark.sql.column.Column\n",
      "        Replace all substrings of the specified string value that match regexp with replacement.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        string : :class:`~pyspark.sql.Column` or str\n",
      "            column name or column containing the string value\n",
      "        pattern : :class:`~pyspark.sql.Column` or str\n",
      "            column object or str containing the regexp pattern\n",
      "        replacement : :class:`~pyspark.sql.Column` or str\n",
      "            column object or str containing the replacement\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            string with all substrings replaced.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"100-200\", r\"(\\d+)\", \"--\")], [\"str\", \"pattern\", \"replacement\"])\n",
      "        >>> df.select(regexp_replace('str', r'(\\d+)', '--').alias('d')).collect()\n",
      "        [Row(d='-----')]\n",
      "        >>> df.select(regexp_replace(\"str\", col(\"pattern\"), col(\"replacement\")).alias('d')).collect()\n",
      "        [Row(d='-----')]\n",
      "    \n",
      "    repeat(col: 'ColumnOrName', n: int) -> pyspark.sql.column.Column\n",
      "        Repeats a string column n times, and returns it as a new string column.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        n : int\n",
      "            number of times to repeat value.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            string with repeated values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('ab',)], ['s',])\n",
      "        >>> df.select(repeat(df.s, 3).alias('s')).collect()\n",
      "        [Row(s='ababab')]\n",
      "    \n",
      "    reverse(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Collection function: returns a reversed string or an array with reverse order of elements.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            array of elements in reverse order.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('Spark SQL',)], ['data'])\n",
      "        >>> df.select(reverse(df.data).alias('s')).collect()\n",
      "        [Row(s='LQS krapS')]\n",
      "        >>> df = spark.createDataFrame([([2, 1, 3],) ,([1],) ,([],)], ['data'])\n",
      "        >>> df.select(reverse(df.data).alias('r')).collect()\n",
      "        [Row(r=[3, 1, 2]), Row(r=[1]), Row(r=[])]\n",
      "    \n",
      "    rint(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the double value that is closest in value to the argument and\n",
      "        is equal to a mathematical integer.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(rint(lit(10.6))).show()\n",
      "        +----------+\n",
      "        |rint(10.6)|\n",
      "        +----------+\n",
      "        |      11.0|\n",
      "        +----------+\n",
      "        \n",
      "        >>> df.select(rint(lit(10.3))).show()\n",
      "        +----------+\n",
      "        |rint(10.3)|\n",
      "        +----------+\n",
      "        |      10.0|\n",
      "        +----------+\n",
      "    \n",
      "    round(col: 'ColumnOrName', scale: int = 0) -> pyspark.sql.column.Column\n",
      "        Round the given value to `scale` decimal places using HALF_UP rounding mode if `scale` >= 0\n",
      "        or at integral part when `scale` < 0.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            input column to round.\n",
      "        scale : int optional default 0\n",
      "            scale value.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            rounded values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.createDataFrame([(2.5,)], ['a']).select(round('a', 0).alias('r')).collect()\n",
      "        [Row(r=3.0)]\n",
      "    \n",
      "    row_number() -> pyspark.sql.column.Column\n",
      "        Window function: returns a sequential number starting at 1 within a window partition.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for calculating row numbers.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Window\n",
      "        >>> df = spark.range(3)\n",
      "        >>> w = Window.orderBy(df.id.desc())\n",
      "        >>> df.withColumn(\"desc_order\", row_number().over(w)).show()\n",
      "        +---+----------+\n",
      "        | id|desc_order|\n",
      "        +---+----------+\n",
      "        |  2|         1|\n",
      "        |  1|         2|\n",
      "        |  0|         3|\n",
      "        +---+----------+\n",
      "    \n",
      "    rpad(col: 'ColumnOrName', len: int, pad: str) -> pyspark.sql.column.Column\n",
      "        Right-pad the string column to width `len` with `pad`.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        len : int\n",
      "            length of the final string.\n",
      "        pad : str\n",
      "            chars to append.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            right padded result.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('abcd',)], ['s',])\n",
      "        >>> df.select(rpad(df.s, 6, '#').alias('s')).collect()\n",
      "        [Row(s='abcd##')]\n",
      "    \n",
      "    rtrim(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Trim the spaces from right end for the specified string value.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            right trimmed values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([\"   Spark\", \"Spark  \", \" Spark\"], \"STRING\")\n",
      "        >>> df.select(rtrim(\"value\").alias(\"r\")).withColumn(\"length\", length(\"r\")).show()\n",
      "        +--------+------+\n",
      "        |       r|length|\n",
      "        +--------+------+\n",
      "        |   Spark|     8|\n",
      "        |   Spark|     5|\n",
      "        |   Spark|     6|\n",
      "        +--------+------+\n",
      "    \n",
      "    schema_of_csv(csv: 'ColumnOrName', options: Optional[Dict[str, str]] = None) -> pyspark.sql.column.Column\n",
      "        Parses a CSV string and infers its schema in DDL format.\n",
      "        \n",
      "        .. versionadded:: 3.0.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        csv : :class:`~pyspark.sql.Column` or str\n",
      "            a CSV string or a foldable string column containing a CSV string.\n",
      "        options : dict, optional\n",
      "            options to control parsing. accepts the same options as the CSV datasource.\n",
      "            See `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-csv.html#data-source-option>`_\n",
      "            for the version you use.\n",
      "        \n",
      "            .. # noqa\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a string representation of a :class:`StructType` parsed from given CSV.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(schema_of_csv(lit('1|a'), {'sep':'|'}).alias(\"csv\")).collect()\n",
      "        [Row(csv='STRUCT<_c0: INT, _c1: STRING>')]\n",
      "        >>> df.select(schema_of_csv('1|a', {'sep':'|'}).alias(\"csv\")).collect()\n",
      "        [Row(csv='STRUCT<_c0: INT, _c1: STRING>')]\n",
      "    \n",
      "    schema_of_json(json: 'ColumnOrName', options: Optional[Dict[str, str]] = None) -> pyspark.sql.column.Column\n",
      "        Parses a JSON string and infers its schema in DDL format.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        json : :class:`~pyspark.sql.Column` or str\n",
      "            a JSON string or a foldable string column containing a JSON string.\n",
      "        options : dict, optional\n",
      "            options to control parsing. accepts the same options as the JSON datasource.\n",
      "            See `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-json.html#data-source-option>`_\n",
      "            for the version you use.\n",
      "        \n",
      "            .. # noqa\n",
      "        \n",
      "            .. versionchanged:: 3.0.0\n",
      "               It accepts `options` parameter to control schema inferring.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a string representation of a :class:`StructType` parsed from given JSON.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(schema_of_json(lit('{\"a\": 0}')).alias(\"json\")).collect()\n",
      "        [Row(json='STRUCT<a: BIGINT>')]\n",
      "        >>> schema = schema_of_json('{a: 1}', {'allowUnquotedFieldNames':'true'})\n",
      "        >>> df.select(schema.alias(\"json\")).collect()\n",
      "        [Row(json='STRUCT<a: BIGINT>')]\n",
      "    \n",
      "    sec(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes secant of the input column.\n",
      "        \n",
      "        .. versionadded:: 3.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            Angle in radians\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            Secant of the angle.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(sec(lit(1.5))).first()\n",
      "        Row(SEC(1.5)=14.13683...)\n",
      "    \n",
      "    second(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Extract the seconds of a given date as integer.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target date/timestamp column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            `seconds` part of the timestamp as integer.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import datetime\n",
      "        >>> df = spark.createDataFrame([(datetime.datetime(2015, 4, 8, 13, 8, 15),)], ['ts'])\n",
      "        >>> df.select(second('ts').alias('second')).collect()\n",
      "        [Row(second=15)]\n",
      "    \n",
      "    sentences(string: 'ColumnOrName', language: Optional[ForwardRef('ColumnOrName')] = None, country: Optional[ForwardRef('ColumnOrName')] = None) -> pyspark.sql.column.Column\n",
      "        Splits a string into arrays of sentences, where each sentence is an array of words.\n",
      "        The 'language' and 'country' arguments are optional, and if omitted, the default locale is used.\n",
      "        \n",
      "        .. versionadded:: 3.2.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        string : :class:`~pyspark.sql.Column` or str\n",
      "            a string to be split\n",
      "        language : :class:`~pyspark.sql.Column` or str, optional\n",
      "            a language of the locale\n",
      "        country : :class:`~pyspark.sql.Column` or str, optional\n",
      "            a country of the locale\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            arrays of split sentences.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([[\"This is an example sentence.\"]], [\"string\"])\n",
      "        >>> df.select(sentences(df.string, lit(\"en\"), lit(\"US\"))).show(truncate=False)\n",
      "        +-----------------------------------+\n",
      "        |sentences(string, en, US)          |\n",
      "        +-----------------------------------+\n",
      "        |[[This, is, an, example, sentence]]|\n",
      "        +-----------------------------------+\n",
      "        >>> df = spark.createDataFrame([[\"Hello world. How are you?\"]], [\"s\"])\n",
      "        >>> df.select(sentences(\"s\")).show(truncate=False)\n",
      "        +---------------------------------+\n",
      "        |sentences(s, , )                 |\n",
      "        +---------------------------------+\n",
      "        |[[Hello, world], [How, are, you]]|\n",
      "        +---------------------------------+\n",
      "    \n",
      "    sequence(start: 'ColumnOrName', stop: 'ColumnOrName', step: Optional[ForwardRef('ColumnOrName')] = None) -> pyspark.sql.column.Column\n",
      "        Generate a sequence of integers from `start` to `stop`, incrementing by `step`.\n",
      "        If `step` is not set, incrementing by 1 if `start` is less than or equal to `stop`,\n",
      "        otherwise -1.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        start : :class:`~pyspark.sql.Column` or str\n",
      "            starting value (inclusive)\n",
      "        stop : :class:`~pyspark.sql.Column` or str\n",
      "            last values (inclusive)\n",
      "        step : :class:`~pyspark.sql.Column` or str, optional\n",
      "            value to add to current to get next element (default is 1)\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            an array of sequence values\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df1 = spark.createDataFrame([(-2, 2)], ('C1', 'C2'))\n",
      "        >>> df1.select(sequence('C1', 'C2').alias('r')).collect()\n",
      "        [Row(r=[-2, -1, 0, 1, 2])]\n",
      "        >>> df2 = spark.createDataFrame([(4, -4, -2)], ('C1', 'C2', 'C3'))\n",
      "        >>> df2.select(sequence('C1', 'C2', 'C3').alias('r')).collect()\n",
      "        [Row(r=[4, 2, 0, -2, -4])]\n",
      "    \n",
      "    session_window(timeColumn: 'ColumnOrName', gapDuration: Union[pyspark.sql.column.Column, str]) -> pyspark.sql.column.Column\n",
      "        Generates session window given a timestamp specifying column.\n",
      "        Session window is one of dynamic windows, which means the length of window is varying\n",
      "        according to the given inputs. The length of session window is defined as \"the timestamp\n",
      "        of latest input of the session + gap duration\", so when the new inputs are bound to the\n",
      "        current session window, the end time of session window can be expanded according to the new\n",
      "        inputs.\n",
      "        Windows can support microsecond precision. Windows in the order of months are not supported.\n",
      "        For a streaming query, you may use the function `current_timestamp` to generate windows on\n",
      "        processing time.\n",
      "        gapDuration is provided as strings, e.g. '1 second', '1 day 12 hours', '2 minutes'. Valid\n",
      "        interval strings are 'week', 'day', 'hour', 'minute', 'second', 'millisecond', 'microsecond'.\n",
      "        It could also be a Column which can be evaluated to gap duration dynamically based on the\n",
      "        input row.\n",
      "        The output column will be a struct called 'session_window' by default with the nested columns\n",
      "        'start' and 'end', where 'start' and 'end' will be of :class:`pyspark.sql.types.TimestampType`.\n",
      "        \n",
      "        .. versionadded:: 3.2.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        timeColumn : :class:`~pyspark.sql.Column` or str\n",
      "            The column name or column to use as the timestamp for windowing by time.\n",
      "            The time column must be of TimestampType or TimestampNTZType.\n",
      "        gapDuration : :class:`~pyspark.sql.Column` or str\n",
      "            A Python string literal or column specifying the timeout of the session. It could be\n",
      "            static value, e.g. `10 minutes`, `1 second`, or an expression/UDF that specifies gap\n",
      "            duration dynamically based on the input row.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"2016-03-11 09:00:07\", 1)]).toDF(\"date\", \"val\")\n",
      "        >>> w = df.groupBy(session_window(\"date\", \"5 seconds\")).agg(sum(\"val\").alias(\"sum\"))\n",
      "        >>> w.select(w.session_window.start.cast(\"string\").alias(\"start\"),\n",
      "        ...          w.session_window.end.cast(\"string\").alias(\"end\"), \"sum\").collect()\n",
      "        [Row(start='2016-03-11 09:00:07', end='2016-03-11 09:00:12', sum=1)]\n",
      "        >>> w = df.groupBy(session_window(\"date\", lit(\"5 seconds\"))).agg(sum(\"val\").alias(\"sum\"))\n",
      "        >>> w.select(w.session_window.start.cast(\"string\").alias(\"start\"),\n",
      "        ...          w.session_window.end.cast(\"string\").alias(\"end\"), \"sum\").collect()\n",
      "        [Row(start='2016-03-11 09:00:07', end='2016-03-11 09:00:12', sum=1)]\n",
      "    \n",
      "    sha1(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the hex string result of SHA-1.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.createDataFrame([('ABC',)], ['a']).select(sha1('a').alias('hash')).collect()\n",
      "        [Row(hash='3c01bdbb26f358bab27f267924aa2c9a03fcfdb8')]\n",
      "    \n",
      "    sha2(col: 'ColumnOrName', numBits: int) -> pyspark.sql.column.Column\n",
      "        Returns the hex string result of SHA-2 family of hash functions (SHA-224, SHA-256, SHA-384,\n",
      "        and SHA-512). The numBits indicates the desired bit length of the result, which must have a\n",
      "        value of 224, 256, 384, 512, or 0 (which is equivalent to 256).\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        numBits : int\n",
      "            the desired bit length of the result, which must have a\n",
      "            value of 224, 256, 384, 512, or 0 (which is equivalent to 256).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([[\"Alice\"], [\"Bob\"]], [\"name\"])\n",
      "        >>> df.withColumn(\"sha2\", sha2(df.name, 256)).show(truncate=False)\n",
      "        +-----+----------------------------------------------------------------+\n",
      "        |name |sha2                                                            |\n",
      "        +-----+----------------------------------------------------------------+\n",
      "        |Alice|3bc51062973c458d5a6f2d8d64a023246354ad7e064b1e4e009ec8a0699a3043|\n",
      "        |Bob  |cd9fb1e148ccd8442e5aa74904cc73bf6fb54d1d54d333bd596aa9bb4bb4e961|\n",
      "        +-----+----------------------------------------------------------------+\n",
      "    \n",
      "    shiftLeft(col: 'ColumnOrName', numBits: int) -> pyspark.sql.column.Column\n",
      "        Shift the given value numBits left.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        .. deprecated:: 3.2.0\n",
      "            Use :func:`shiftleft` instead.\n",
      "    \n",
      "    shiftRight(col: 'ColumnOrName', numBits: int) -> pyspark.sql.column.Column\n",
      "        (Signed) shift the given value numBits right.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        .. deprecated:: 3.2.0\n",
      "            Use :func:`shiftright` instead.\n",
      "    \n",
      "    shiftRightUnsigned(col: 'ColumnOrName', numBits: int) -> pyspark.sql.column.Column\n",
      "        Unsigned shift the given value numBits right.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        .. deprecated:: 3.2.0\n",
      "            Use :func:`shiftrightunsigned` instead.\n",
      "    \n",
      "    shiftleft(col: 'ColumnOrName', numBits: int) -> pyspark.sql.column.Column\n",
      "        Shift the given value numBits left.\n",
      "        \n",
      "        .. versionadded:: 3.2.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            input column of values to shift.\n",
      "        numBits : int\n",
      "            number of bits to shift.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            shifted value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.createDataFrame([(21,)], ['a']).select(shiftleft('a', 1).alias('r')).collect()\n",
      "        [Row(r=42)]\n",
      "    \n",
      "    shiftright(col: 'ColumnOrName', numBits: int) -> pyspark.sql.column.Column\n",
      "        (Signed) shift the given value numBits right.\n",
      "        \n",
      "        .. versionadded:: 3.2.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            input column of values to shift.\n",
      "        numBits : int\n",
      "            number of bits to shift.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            shifted values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.createDataFrame([(42,)], ['a']).select(shiftright('a', 1).alias('r')).collect()\n",
      "        [Row(r=21)]\n",
      "    \n",
      "    shiftrightunsigned(col: 'ColumnOrName', numBits: int) -> pyspark.sql.column.Column\n",
      "        Unsigned shift the given value numBits right.\n",
      "        \n",
      "        .. versionadded:: 3.2.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            input column of values to shift.\n",
      "        numBits : int\n",
      "            number of bits to shift.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            shifted value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(-42,)], ['a'])\n",
      "        >>> df.select(shiftrightunsigned('a', 1).alias('r')).collect()\n",
      "        [Row(r=9223372036854775787)]\n",
      "    \n",
      "    shuffle(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Collection function: Generates a random permutation of the given array.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The function is non-deterministic.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            an array of elements in random order.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([1, 20, 3, 5],), ([1, 20, None, 3],)], ['data'])\n",
      "        >>> df.select(shuffle(df.data).alias('s')).collect()  # doctest: +SKIP\n",
      "        [Row(s=[3, 1, 5, 20]), Row(s=[20, None, 3, 1])]\n",
      "    \n",
      "    signum(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes the signum of the given value.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(signum(lit(-5))).show()\n",
      "        +----------+\n",
      "        |SIGNUM(-5)|\n",
      "        +----------+\n",
      "        |      -1.0|\n",
      "        +----------+\n",
      "        \n",
      "        >>> df.select(signum(lit(6))).show()\n",
      "        +---------+\n",
      "        |SIGNUM(6)|\n",
      "        +---------+\n",
      "        |      1.0|\n",
      "        +---------+\n",
      "    \n",
      "    sin(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes sine of the input column.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            sine of the angle, as if computed by `java.lang.Math.sin()`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import math\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(sin(lit(math.radians(90)))).first()\n",
      "        Row(SIN(1.57079...)=1.0)\n",
      "    \n",
      "    sinh(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes hyperbolic sine of the input column.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            hyperbolic angle.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            hyperbolic sine of the given value,\n",
      "            as if computed by `java.lang.Math.sinh()`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(sinh(lit(1.1))).first()\n",
      "        Row(SINH(1.1)=1.33564...)\n",
      "    \n",
      "    size(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Collection function: returns the length of the array or map stored in the column.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            length of the array/map.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([1, 2, 3],),([1],),([],)], ['data'])\n",
      "        >>> df.select(size(df.data)).collect()\n",
      "        [Row(size(data)=3), Row(size(data)=1), Row(size(data)=0)]\n",
      "    \n",
      "    skewness(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the skewness of the values in a group.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            skewness of given column.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([[1],[1],[2]], [\"c\"])\n",
      "        >>> df.select(skewness(df.c)).first()\n",
      "        Row(skewness(c)=0.70710...)\n",
      "    \n",
      "    slice(x: 'ColumnOrName', start: Union[ForwardRef('ColumnOrName'), int], length: Union[ForwardRef('ColumnOrName'), int]) -> pyspark.sql.column.Column\n",
      "        Collection function: returns an array containing all the elements in `x` from index `start`\n",
      "        (array indices start at 1, or from the end if `start` is negative) with the specified `length`.\n",
      "        \n",
      "        .. versionadded:: 2.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        x : :class:`~pyspark.sql.Column` or str\n",
      "            column name or column containing the array to be sliced\n",
      "        start : :class:`~pyspark.sql.Column` or str or int\n",
      "            column name, column, or int containing the starting index\n",
      "        length : :class:`~pyspark.sql.Column` or str or int\n",
      "            column name, column, or int containing the length of the slice\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a column of array type. Subset of array.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([1, 2, 3],), ([4, 5],)], ['x'])\n",
      "        >>> df.select(slice(df.x, 2, 2).alias(\"sliced\")).collect()\n",
      "        [Row(sliced=[2, 3]), Row(sliced=[5])]\n",
      "    \n",
      "    sort_array(col: 'ColumnOrName', asc: bool = True) -> pyspark.sql.column.Column\n",
      "        Collection function: sorts the input array in ascending or descending order according\n",
      "        to the natural ordering of the array elements. Null elements will be placed at the beginning\n",
      "        of the returned array in ascending order or at the end of the returned array in descending\n",
      "        order.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        asc : bool, optional\n",
      "            whether to sort in ascending or descending order. If `asc` is True (default)\n",
      "            then ascending and if False then descending.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            sorted array.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([([2, 1, None, 3],),([1],),([],)], ['data'])\n",
      "        >>> df.select(sort_array(df.data).alias('r')).collect()\n",
      "        [Row(r=[None, 1, 2, 3]), Row(r=[1]), Row(r=[])]\n",
      "        >>> df.select(sort_array(df.data, asc=False).alias('r')).collect()\n",
      "        [Row(r=[3, 2, 1, None]), Row(r=[1]), Row(r=[])]\n",
      "    \n",
      "    soundex(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Returns the SoundEx encoding for a string\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            SoundEx encoded string.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"Peters\",),(\"Uhrbach\",)], ['name'])\n",
      "        >>> df.select(soundex(df.name).alias(\"soundex\")).collect()\n",
      "        [Row(soundex='P362'), Row(soundex='U612')]\n",
      "    \n",
      "    spark_partition_id() -> pyspark.sql.column.Column\n",
      "        A column for partition ID.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This is non deterministic because it depends on data partitioning and task scheduling.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            partition id the record belongs to.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(2)\n",
      "        >>> df.repartition(1).select(spark_partition_id().alias(\"pid\")).collect()\n",
      "        [Row(pid=0), Row(pid=0)]\n",
      "    \n",
      "    split(str: 'ColumnOrName', pattern: str, limit: int = -1) -> pyspark.sql.column.Column\n",
      "        Splits str around matches of the given pattern.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        str : :class:`~pyspark.sql.Column` or str\n",
      "            a string expression to split\n",
      "        pattern : str\n",
      "            a string representing a regular expression. The regex string should be\n",
      "            a Java regular expression.\n",
      "        limit : int, optional\n",
      "            an integer which controls the number of times `pattern` is applied.\n",
      "        \n",
      "            * ``limit > 0``: The resulting array's length will not be more than `limit`, and the\n",
      "                             resulting array's last entry will contain all input beyond the last\n",
      "                             matched pattern.\n",
      "            * ``limit <= 0``: `pattern` will be applied as many times as possible, and the resulting\n",
      "                              array can be of any size.\n",
      "        \n",
      "            .. versionchanged:: 3.0\n",
      "               `split` now takes an optional `limit` field. If not provided, default limit value is -1.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            array of separated strings.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('oneAtwoBthreeC',)], ['s',])\n",
      "        >>> df.select(split(df.s, '[ABC]', 2).alias('s')).collect()\n",
      "        [Row(s=['one', 'twoBthreeC'])]\n",
      "        >>> df.select(split(df.s, '[ABC]', -1).alias('s')).collect()\n",
      "        [Row(s=['one', 'two', 'three', ''])]\n",
      "    \n",
      "    sqrt(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes the square root of the specified float value.\n",
      "        \n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(sqrt(lit(4))).show()\n",
      "        +-------+\n",
      "        |SQRT(4)|\n",
      "        +-------+\n",
      "        |    2.0|\n",
      "        +-------+\n",
      "    \n",
      "    stddev(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: alias for stddev_samp.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            standard deviation of given column.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(6)\n",
      "        >>> df.select(stddev(df.id)).first()\n",
      "        Row(stddev_samp(id)=1.87082...)\n",
      "    \n",
      "    stddev_pop(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns population standard deviation of\n",
      "        the expression in a group.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            standard deviation of given column.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(6)\n",
      "        >>> df.select(stddev_pop(df.id)).first()\n",
      "        Row(stddev_pop(id)=1.70782...)\n",
      "    \n",
      "    stddev_samp(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the unbiased sample standard deviation of\n",
      "        the expression in a group.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            standard deviation of given column.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(6)\n",
      "        >>> df.select(stddev_samp(df.id)).first()\n",
      "        Row(stddev_samp(id)=1.87082...)\n",
      "    \n",
      "    struct(*cols: Union[ForwardRef('ColumnOrName'), List[ForwardRef('ColumnOrName_')], Tuple[ForwardRef('ColumnOrName_'), ...]]) -> pyspark.sql.column.Column\n",
      "        Creates a new struct column.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        cols : list, set, str or :class:`~pyspark.sql.Column`\n",
      "            column names or :class:`~pyspark.sql.Column`\\s to contain in the output struct.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a struct type column of given columns.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(\"Alice\", 2), (\"Bob\", 5)], (\"name\", \"age\"))\n",
      "        >>> df.select(struct('age', 'name').alias(\"struct\")).collect()\n",
      "        [Row(struct=Row(age=2, name='Alice')), Row(struct=Row(age=5, name='Bob'))]\n",
      "        >>> df.select(struct([df.age, df.name]).alias(\"struct\")).collect()\n",
      "        [Row(struct=Row(age=2, name='Alice')), Row(struct=Row(age=5, name='Bob'))]\n",
      "    \n",
      "    substring(str: 'ColumnOrName', pos: int, len: int) -> pyspark.sql.column.Column\n",
      "        Substring starts at `pos` and is of length `len` when str is String type or\n",
      "        returns the slice of byte array that starts at `pos` in byte and is of length `len`\n",
      "        when str is Binary type.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The position is not zero based, but 1 based index.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        str : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        pos : int\n",
      "            starting position in str.\n",
      "        len : int\n",
      "            length of chars.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            substring of given value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('abcd',)], ['s',])\n",
      "        >>> df.select(substring(df.s, 1, 2).alias('s')).collect()\n",
      "        [Row(s='ab')]\n",
      "    \n",
      "    substring_index(str: 'ColumnOrName', delim: str, count: int) -> pyspark.sql.column.Column\n",
      "        Returns the substring from string str before count occurrences of the delimiter delim.\n",
      "        If count is positive, everything the left of the final delimiter (counting from left) is\n",
      "        returned. If count is negative, every to the right of the final delimiter (counting from the\n",
      "        right) is returned. substring_index performs a case-sensitive match when searching for delim.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        str : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        delim : str\n",
      "            delimiter of values.\n",
      "        count : int\n",
      "            number of occurences.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            substring of given value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('a.b.c.d',)], ['s'])\n",
      "        >>> df.select(substring_index(df.s, '.', 2).alias('s')).collect()\n",
      "        [Row(s='a.b')]\n",
      "        >>> df.select(substring_index(df.s, '.', -3).alias('s')).collect()\n",
      "        [Row(s='b.c.d')]\n",
      "    \n",
      "    sum(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the sum of all values in the expression.\n",
      "        \n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(10)\n",
      "        >>> df.select(sum(df[\"id\"])).show()\n",
      "        +-------+\n",
      "        |sum(id)|\n",
      "        +-------+\n",
      "        |     45|\n",
      "        +-------+\n",
      "    \n",
      "    sumDistinct(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the sum of distinct values in the expression.\n",
      "        \n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        .. deprecated:: 3.2.0\n",
      "            Use :func:`sum_distinct` instead.\n",
      "    \n",
      "    sum_distinct(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the sum of distinct values in the expression.\n",
      "        \n",
      "        .. versionadded:: 3.2.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(None,), (1,), (1,), (2,)], schema=[\"numbers\"])\n",
      "        >>> df.select(sum_distinct(col(\"numbers\"))).show()\n",
      "        +---------------------+\n",
      "        |sum(DISTINCT numbers)|\n",
      "        +---------------------+\n",
      "        |                    3|\n",
      "        +---------------------+\n",
      "    \n",
      "    tan(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes tangent of the input column.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            angle in radians\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            tangent of the given value, as if computed by `java.lang.Math.tan()`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import math\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(tan(lit(math.radians(45)))).first()\n",
      "        Row(TAN(0.78539...)=0.99999...)\n",
      "    \n",
      "    tanh(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes hyperbolic tangent of the input column.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            hyperbolic angle\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            hyperbolic tangent of the given value\n",
      "            as if computed by `java.lang.Math.tanh()`\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import math\n",
      "        >>> df = spark.range(1)\n",
      "        >>> df.select(tanh(lit(math.radians(90)))).first()\n",
      "        Row(TANH(1.57079...)=0.91715...)\n",
      "    \n",
      "    timestamp_seconds(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Converts the number of seconds from the Unix epoch (1970-01-01T00:00:00Z)\n",
      "        to a timestamp.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            unix time values.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            converted timestamp value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.functions import timestamp_seconds\n",
      "        >>> spark.conf.set(\"spark.sql.session.timeZone\", \"UTC\")\n",
      "        >>> time_df = spark.createDataFrame([(1230219000,)], ['unix_time'])\n",
      "        >>> time_df.select(timestamp_seconds(time_df.unix_time).alias('ts')).show()\n",
      "        +-------------------+\n",
      "        |                 ts|\n",
      "        +-------------------+\n",
      "        |2008-12-25 15:30:00|\n",
      "        +-------------------+\n",
      "        >>> time_df.select(timestamp_seconds('unix_time').alias('ts')).printSchema()\n",
      "        root\n",
      "         |-- ts: timestamp (nullable = true)\n",
      "        >>> spark.conf.unset(\"spark.sql.session.timeZone\")\n",
      "    \n",
      "    toDegrees(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        .. deprecated:: 2.1.0\n",
      "            Use :func:`degrees` instead.\n",
      "    \n",
      "    toRadians(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        .. deprecated:: 2.1.0\n",
      "            Use :func:`radians` instead.\n",
      "    \n",
      "    to_csv(col: 'ColumnOrName', options: Optional[Dict[str, str]] = None) -> pyspark.sql.column.Column\n",
      "        Converts a column containing a :class:`StructType` into a CSV string.\n",
      "        Throws an exception, in the case of an unsupported type.\n",
      "        \n",
      "        .. versionadded:: 3.0.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing a struct.\n",
      "        options: dict, optional\n",
      "            options to control converting. accepts the same options as the CSV datasource.\n",
      "            See `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-csv.html#data-source-option>`_\n",
      "            for the version you use.\n",
      "        \n",
      "            .. # noqa\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a CSV string converted from given :class:`StructType`.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Row\n",
      "        >>> data = [(1, Row(age=2, name='Alice'))]\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
      "        >>> df.select(to_csv(df.value).alias(\"csv\")).collect()\n",
      "        [Row(csv='2,Alice')]\n",
      "    \n",
      "    to_date(col: 'ColumnOrName', format: Optional[str] = None) -> pyspark.sql.column.Column\n",
      "        Converts a :class:`~pyspark.sql.Column` into :class:`pyspark.sql.types.DateType`\n",
      "        using the optionally specified format. Specify formats according to `datetime pattern`_.\n",
      "        By default, it follows casting rules to :class:`pyspark.sql.types.DateType` if the format\n",
      "        is omitted. Equivalent to ``col.cast(\"date\")``.\n",
      "        \n",
      "        .. _datetime pattern: https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n",
      "        \n",
      "        .. versionadded:: 2.2.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            input column of values to convert.\n",
      "        format: str, optional\n",
      "            format to use to convert date values.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            date value as :class:`pyspark.sql.types.DateType` type.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
      "        >>> df.select(to_date(df.t).alias('date')).collect()\n",
      "        [Row(date=datetime.date(1997, 2, 28))]\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
      "        >>> df.select(to_date(df.t, 'yyyy-MM-dd HH:mm:ss').alias('date')).collect()\n",
      "        [Row(date=datetime.date(1997, 2, 28))]\n",
      "    \n",
      "    to_json(col: 'ColumnOrName', options: Optional[Dict[str, str]] = None) -> pyspark.sql.column.Column\n",
      "        Converts a column containing a :class:`StructType`, :class:`ArrayType` or a :class:`MapType`\n",
      "        into a JSON string. Throws an exception, in the case of an unsupported type.\n",
      "        \n",
      "        .. versionadded:: 2.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column containing a struct, an array or a map.\n",
      "        options : dict, optional\n",
      "            options to control converting. accepts the same options as the JSON datasource.\n",
      "            See `Data Source Option <https://spark.apache.org/docs/latest/sql-data-sources-json.html#data-source-option>`_\n",
      "            for the version you use.\n",
      "            Additionally the function supports the `pretty` option which enables\n",
      "            pretty JSON generation.\n",
      "        \n",
      "            .. # noqa\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            JSON object as string column.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql import Row\n",
      "        >>> from pyspark.sql.types import *\n",
      "        >>> data = [(1, Row(age=2, name='Alice'))]\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
      "        >>> df.select(to_json(df.value).alias(\"json\")).collect()\n",
      "        [Row(json='{\"age\":2,\"name\":\"Alice\"}')]\n",
      "        >>> data = [(1, [Row(age=2, name='Alice'), Row(age=3, name='Bob')])]\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
      "        >>> df.select(to_json(df.value).alias(\"json\")).collect()\n",
      "        [Row(json='[{\"age\":2,\"name\":\"Alice\"},{\"age\":3,\"name\":\"Bob\"}]')]\n",
      "        >>> data = [(1, {\"name\": \"Alice\"})]\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
      "        >>> df.select(to_json(df.value).alias(\"json\")).collect()\n",
      "        [Row(json='{\"name\":\"Alice\"}')]\n",
      "        >>> data = [(1, [{\"name\": \"Alice\"}, {\"name\": \"Bob\"}])]\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
      "        >>> df.select(to_json(df.value).alias(\"json\")).collect()\n",
      "        [Row(json='[{\"name\":\"Alice\"},{\"name\":\"Bob\"}]')]\n",
      "        >>> data = [(1, [\"Alice\", \"Bob\"])]\n",
      "        >>> df = spark.createDataFrame(data, (\"key\", \"value\"))\n",
      "        >>> df.select(to_json(df.value).alias(\"json\")).collect()\n",
      "        [Row(json='[\"Alice\",\"Bob\"]')]\n",
      "    \n",
      "    to_timestamp(col: 'ColumnOrName', format: Optional[str] = None) -> pyspark.sql.column.Column\n",
      "        Converts a :class:`~pyspark.sql.Column` into :class:`pyspark.sql.types.TimestampType`\n",
      "        using the optionally specified format. Specify formats according to `datetime pattern`_.\n",
      "        By default, it follows casting rules to :class:`pyspark.sql.types.TimestampType` if the format\n",
      "        is omitted. Equivalent to ``col.cast(\"timestamp\")``.\n",
      "        \n",
      "        .. _datetime pattern: https://spark.apache.org/docs/latest/sql-ref-datetime-pattern.html\n",
      "        \n",
      "        .. versionadded:: 2.2.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            column values to convert.\n",
      "        format: str, optional\n",
      "            format to use to convert timestamp values.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            timestamp value as :class:`pyspark.sql.types.TimestampType` type.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
      "        >>> df.select(to_timestamp(df.t).alias('dt')).collect()\n",
      "        [Row(dt=datetime.datetime(1997, 2, 28, 10, 30))]\n",
      "        \n",
      "        >>> df = spark.createDataFrame([('1997-02-28 10:30:00',)], ['t'])\n",
      "        >>> df.select(to_timestamp(df.t, 'yyyy-MM-dd HH:mm:ss').alias('dt')).collect()\n",
      "        [Row(dt=datetime.datetime(1997, 2, 28, 10, 30))]\n",
      "    \n",
      "    to_utc_timestamp(timestamp: 'ColumnOrName', tz: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        This is a common function for databases supporting TIMESTAMP WITHOUT TIMEZONE. This function\n",
      "        takes a timestamp which is timezone-agnostic, and interprets it as a timestamp in the given\n",
      "        timezone, and renders that timestamp as a timestamp in UTC.\n",
      "        \n",
      "        However, timestamp in Spark represents number of microseconds from the Unix epoch, which is not\n",
      "        timezone-agnostic. So in Spark this function just shift the timestamp value from the given\n",
      "        timezone to UTC timezone.\n",
      "        \n",
      "        This function may return confusing result if the input is a string with timezone, e.g.\n",
      "        '2018-03-13T06:18:23+00:00'. The reason is that, Spark firstly cast the string to timestamp\n",
      "        according to the timezone in the string, and finally display the result by converting the\n",
      "        timestamp to string according to the session local timezone.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        timestamp : :class:`~pyspark.sql.Column` or str\n",
      "            the column that contains timestamps\n",
      "        tz : :class:`~pyspark.sql.Column` or str\n",
      "            A string detailing the time zone ID that the input should be adjusted to. It should\n",
      "            be in the format of either region-based zone IDs or zone offsets. Region IDs must\n",
      "            have the form 'area/city', such as 'America/Los_Angeles'. Zone offsets must be in\n",
      "            the format '(+|-)HH:mm', for example '-08:00' or '+01:00'. Also 'UTC' and 'Z' are\n",
      "            supported as aliases of '+00:00'. Other short names are not recommended to use\n",
      "            because they can be ambiguous.\n",
      "        \n",
      "            .. versionchanged:: 2.4.0\n",
      "               `tz` can take a :class:`~pyspark.sql.Column` containing timezone ID strings.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            timestamp value represented in UTC timezone.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('1997-02-28 10:30:00', 'JST')], ['ts', 'tz'])\n",
      "        >>> df.select(to_utc_timestamp(df.ts, \"PST\").alias('utc_time')).collect()\n",
      "        [Row(utc_time=datetime.datetime(1997, 2, 28, 18, 30))]\n",
      "        >>> df.select(to_utc_timestamp(df.ts, df.tz).alias('utc_time')).collect()\n",
      "        [Row(utc_time=datetime.datetime(1997, 2, 28, 1, 30))]\n",
      "    \n",
      "    transform(col: 'ColumnOrName', f: Union[Callable[[pyspark.sql.column.Column], pyspark.sql.column.Column], Callable[[pyspark.sql.column.Column, pyspark.sql.column.Column], pyspark.sql.column.Column]]) -> pyspark.sql.column.Column\n",
      "        Returns an array of elements after applying a transformation to each element in the input array.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        f : function\n",
      "            a function that is applied to each element of the input array.\n",
      "            Can take one of the following forms:\n",
      "        \n",
      "            - Unary ``(x: Column) -> Column: ...``\n",
      "            - Binary ``(x: Column, i: Column) -> Column...``, where the second argument is\n",
      "                a 0-based index of the element.\n",
      "        \n",
      "            and can use methods of :class:`~pyspark.sql.Column`, functions defined in\n",
      "            :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\n",
      "            Python ``UserDefinedFunctions`` are not supported\n",
      "            (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a new array of transformed elements.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1, [1, 2, 3, 4])], (\"key\", \"values\"))\n",
      "        >>> df.select(transform(\"values\", lambda x: x * 2).alias(\"doubled\")).show()\n",
      "        +------------+\n",
      "        |     doubled|\n",
      "        +------------+\n",
      "        |[2, 4, 6, 8]|\n",
      "        +------------+\n",
      "        \n",
      "        >>> def alternate(x, i):\n",
      "        ...     return when(i % 2 == 0, x).otherwise(-x)\n",
      "        >>> df.select(transform(\"values\", alternate).alias(\"alternated\")).show()\n",
      "        +--------------+\n",
      "        |    alternated|\n",
      "        +--------------+\n",
      "        |[1, -2, 3, -4]|\n",
      "        +--------------+\n",
      "    \n",
      "    transform_keys(col: 'ColumnOrName', f: Callable[[pyspark.sql.column.Column, pyspark.sql.column.Column], pyspark.sql.column.Column]) -> pyspark.sql.column.Column\n",
      "        Applies a function to every key-value pair in a map and returns\n",
      "        a map with the results of those applications as the new keys for the pairs.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        f : function\n",
      "            a binary function ``(k: Column, v: Column) -> Column...``\n",
      "            Can use methods of :class:`~pyspark.sql.Column`, functions defined in\n",
      "            :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\n",
      "            Python ``UserDefinedFunctions`` are not supported\n",
      "            (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a new map of enties where new keys were calculated by applying given function to\n",
      "            each key value argument.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1, {\"foo\": -2.0, \"bar\": 2.0})], (\"id\", \"data\"))\n",
      "        >>> row = df.select(transform_keys(\n",
      "        ...     \"data\", lambda k, _: upper(k)).alias(\"data_upper\")\n",
      "        ... ).head()\n",
      "        >>> sorted(row[\"data_upper\"].items())\n",
      "        [('BAR', 2.0), ('FOO', -2.0)]\n",
      "    \n",
      "    transform_values(col: 'ColumnOrName', f: Callable[[pyspark.sql.column.Column, pyspark.sql.column.Column], pyspark.sql.column.Column]) -> pyspark.sql.column.Column\n",
      "        Applies a function to every key-value pair in a map and returns\n",
      "        a map with the results of those applications as the new values for the pairs.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            name of column or expression\n",
      "        f : function\n",
      "            a binary function ``(k: Column, v: Column) -> Column...``\n",
      "            Can use methods of :class:`~pyspark.sql.Column`, functions defined in\n",
      "            :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\n",
      "            Python ``UserDefinedFunctions`` are not supported\n",
      "            (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            a new map of enties where new values were calculated by applying given function to\n",
      "            each key value argument.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1, {\"IT\": 10.0, \"SALES\": 2.0, \"OPS\": 24.0})], (\"id\", \"data\"))\n",
      "        >>> row = df.select(transform_values(\n",
      "        ...     \"data\", lambda k, v: when(k.isin(\"IT\", \"OPS\"), v + 10.0).otherwise(v)\n",
      "        ... ).alias(\"new_data\")).head()\n",
      "        >>> sorted(row[\"new_data\"].items())\n",
      "        [('IT', 20.0), ('OPS', 34.0), ('SALES', 2.0)]\n",
      "    \n",
      "    translate(srcCol: 'ColumnOrName', matching: str, replace: str) -> pyspark.sql.column.Column\n",
      "        A function translate any character in the `srcCol` by a character in `matching`.\n",
      "        The characters in `replace` is corresponding to the characters in `matching`.\n",
      "        Translation will happen whenever any character in the string is matching with the character\n",
      "        in the `matching`.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        srcCol : :class:`~pyspark.sql.Column` or str\n",
      "            Source column or strings\n",
      "        matching : str\n",
      "            matching characters.\n",
      "        replace : str\n",
      "            characters for replacement. If this is shorter than `matching` string then\n",
      "            those chars that don't have replacement will be dropped.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            replaced value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.createDataFrame([('translate',)], ['a']).select(translate('a', \"rnlt\", \"123\") \\\n",
      "        ...     .alias('r')).collect()\n",
      "        [Row(r='1a2s3ae')]\n",
      "    \n",
      "    trim(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Trim the spaces from both ends for the specified string column.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            trimmed values from both sides.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([\"   Spark\", \"Spark  \", \" Spark\"], \"STRING\")\n",
      "        >>> df.select(trim(\"value\").alias(\"r\")).withColumn(\"length\", length(\"r\")).show()\n",
      "        +-----+------+\n",
      "        |    r|length|\n",
      "        +-----+------+\n",
      "        |Spark|     5|\n",
      "        |Spark|     5|\n",
      "        |Spark|     5|\n",
      "        +-----+------+\n",
      "    \n",
      "    trunc(date: 'ColumnOrName', format: str) -> pyspark.sql.column.Column\n",
      "        Returns date truncated to the unit specified by the format.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        date : :class:`~pyspark.sql.Column` or str\n",
      "            input column of values to truncate.\n",
      "        format : str\n",
      "            'year', 'yyyy', 'yy' to truncate by year,\n",
      "            or 'month', 'mon', 'mm' to truncate by month\n",
      "            Other options are: 'week', 'quarter'\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            truncated date.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('1997-02-28',)], ['d'])\n",
      "        >>> df.select(trunc(df.d, 'year').alias('year')).collect()\n",
      "        [Row(year=datetime.date(1997, 1, 1))]\n",
      "        >>> df.select(trunc(df.d, 'mon').alias('month')).collect()\n",
      "        [Row(month=datetime.date(1997, 2, 1))]\n",
      "    \n",
      "    udf(f: Union[Callable[..., Any], ForwardRef('DataTypeOrString'), NoneType] = None, returnType: 'DataTypeOrString' = StringType()) -> Union[ForwardRef('UserDefinedFunctionLike'), Callable[[Callable[..., Any]], ForwardRef('UserDefinedFunctionLike')]]\n",
      "        Creates a user defined function (UDF).\n",
      "        \n",
      "        .. versionadded:: 1.3.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        f : function\n",
      "            python function if used as a standalone function\n",
      "        returnType : :class:`pyspark.sql.types.DataType` or str\n",
      "            the return type of the user-defined function. The value can be either a\n",
      "            :class:`pyspark.sql.types.DataType` object or a DDL-formatted type string.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> from pyspark.sql.types import IntegerType\n",
      "        >>> slen = udf(lambda s: len(s), IntegerType())\n",
      "        >>> @udf\n",
      "        ... def to_upper(s):\n",
      "        ...     if s is not None:\n",
      "        ...         return s.upper()\n",
      "        ...\n",
      "        >>> @udf(returnType=IntegerType())\n",
      "        ... def add_one(x):\n",
      "        ...     if x is not None:\n",
      "        ...         return x + 1\n",
      "        ...\n",
      "        >>> df = spark.createDataFrame([(1, \"John Doe\", 21)], (\"id\", \"name\", \"age\"))\n",
      "        >>> df.select(slen(\"name\").alias(\"slen(name)\"), to_upper(\"name\"), add_one(\"age\")).show()\n",
      "        +----------+--------------+------------+\n",
      "        |slen(name)|to_upper(name)|add_one(age)|\n",
      "        +----------+--------------+------------+\n",
      "        |         8|      JOHN DOE|          22|\n",
      "        +----------+--------------+------------+\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        The user-defined functions are considered deterministic by default. Due to\n",
      "        optimization, duplicate invocations may be eliminated or the function may even be invoked\n",
      "        more times than it is present in the query. If your function is not deterministic, call\n",
      "        `asNondeterministic` on the user defined function. E.g.:\n",
      "        \n",
      "        >>> from pyspark.sql.types import IntegerType\n",
      "        >>> import random\n",
      "        >>> random_udf = udf(lambda: int(random.random() * 100), IntegerType()).asNondeterministic()\n",
      "        \n",
      "        The user-defined functions do not support conditional expressions or short circuiting\n",
      "        in boolean expressions and it ends up with being executed all internally. If the functions\n",
      "        can fail on special rows, the workaround is to incorporate the condition into the functions.\n",
      "        \n",
      "        The user-defined functions do not take keyword arguments on the calling side.\n",
      "    \n",
      "    unbase64(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Decodes a BASE64 encoded string column and returns it as a binary column.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            encoded string value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([\"U3Bhcms=\",\n",
      "        ...                             \"UHlTcGFyaw==\",\n",
      "        ...                             \"UGFuZGFzIEFQSQ==\"], \"STRING\")\n",
      "        >>> df.select(unbase64(\"value\")).show()\n",
      "        +--------------------+\n",
      "        |     unbase64(value)|\n",
      "        +--------------------+\n",
      "        |    [53 70 61 72 6B]|\n",
      "        |[50 79 53 70 61 7...|\n",
      "        |[50 61 6E 64 61 7...|\n",
      "        +--------------------+\n",
      "    \n",
      "    unhex(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Inverse of hex. Interprets each pair of characters as a hexadecimal number\n",
      "        and converts to the byte representation of number.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            string representation of given hexadecimal value.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.createDataFrame([('414243',)], ['a']).select(unhex('a')).collect()\n",
      "        [Row(unhex(a)=bytearray(b'ABC'))]\n",
      "    \n",
      "    unix_timestamp(timestamp: Optional[ForwardRef('ColumnOrName')] = None, format: str = 'yyyy-MM-dd HH:mm:ss') -> pyspark.sql.column.Column\n",
      "        Convert time string with given pattern ('yyyy-MM-dd HH:mm:ss', by default)\n",
      "        to Unix time stamp (in seconds), using the default timezone and the default\n",
      "        locale, returns null if failed.\n",
      "        \n",
      "        if `timestamp` is None, then it returns current timestamp.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        timestamp : :class:`~pyspark.sql.Column` or str, optional\n",
      "            timestamps of string values.\n",
      "        format : str, optional\n",
      "            alternative format to use for converting (default: yyyy-MM-dd HH:mm:ss).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            unix time as long integer.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> spark.conf.set(\"spark.sql.session.timeZone\", \"America/Los_Angeles\")\n",
      "        >>> time_df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> time_df.select(unix_timestamp('dt', 'yyyy-MM-dd').alias('unix_time')).collect()\n",
      "        [Row(unix_time=1428476400)]\n",
      "        >>> spark.conf.unset(\"spark.sql.session.timeZone\")\n",
      "    \n",
      "    unwrap_udt(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Unwrap UDT data type column into its underlying type.\n",
      "        \n",
      "        .. versionadded:: 3.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "    \n",
      "    upper(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Converts a string expression to upper case.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            upper case values.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([\"Spark\", \"PySpark\", \"Pandas API\"], \"STRING\")\n",
      "        >>> df.select(upper(\"value\")).show()\n",
      "        +------------+\n",
      "        |upper(value)|\n",
      "        +------------+\n",
      "        |       SPARK|\n",
      "        |     PYSPARK|\n",
      "        |  PANDAS API|\n",
      "        +------------+\n",
      "    \n",
      "    var_pop(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the population variance of the values in a group.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            variance of given column.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(6)\n",
      "        >>> df.select(var_pop(df.id)).first()\n",
      "        Row(var_pop(id)=2.91666...)\n",
      "    \n",
      "    var_samp(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: returns the unbiased sample variance of\n",
      "        the values in a group.\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            variance of given column.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(6)\n",
      "        >>> df.select(var_samp(df.id)).show()\n",
      "        +------------+\n",
      "        |var_samp(id)|\n",
      "        +------------+\n",
      "        |         3.5|\n",
      "        +------------+\n",
      "    \n",
      "    variance(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Aggregate function: alias for var_samp\n",
      "        \n",
      "        .. versionadded:: 1.6.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target column to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            variance of given column.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(6)\n",
      "        >>> df.select(variance(df.id)).show()\n",
      "        +------------+\n",
      "        |var_samp(id)|\n",
      "        +------------+\n",
      "        |         3.5|\n",
      "        +------------+\n",
      "    \n",
      "    weekofyear(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Extract the week number of a given date as integer.\n",
      "        A week is considered to start on a Monday and week 1 is the first week with more than 3 days,\n",
      "        as defined by ISO 8601\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target timestamp column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            `week` of the year for given date as integer.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(weekofyear(df.dt).alias('week')).collect()\n",
      "        [Row(week=15)]\n",
      "    \n",
      "    when(condition: pyspark.sql.column.Column, value: Any) -> pyspark.sql.column.Column\n",
      "        Evaluates a list of conditions and returns one of multiple possible result expressions.\n",
      "        If :func:`pyspark.sql.Column.otherwise` is not invoked, None is returned for unmatched\n",
      "        conditions.\n",
      "        \n",
      "        .. versionadded:: 1.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        condition : :class:`~pyspark.sql.Column`\n",
      "            a boolean :class:`~pyspark.sql.Column` expression.\n",
      "        value :\n",
      "            a literal value, or a :class:`~pyspark.sql.Column` expression.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            column representing when expression.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.range(3)\n",
      "        >>> df.select(when(df['id'] == 2, 3).otherwise(4).alias(\"age\")).show()\n",
      "        +---+\n",
      "        |age|\n",
      "        +---+\n",
      "        |  4|\n",
      "        |  4|\n",
      "        |  3|\n",
      "        +---+\n",
      "        \n",
      "        >>> df.select(when(df.id == 2, df.id + 1).alias(\"age\")).show()\n",
      "        +----+\n",
      "        | age|\n",
      "        +----+\n",
      "        |null|\n",
      "        |null|\n",
      "        |   3|\n",
      "        +----+\n",
      "    \n",
      "    window(timeColumn: 'ColumnOrName', windowDuration: str, slideDuration: Optional[str] = None, startTime: Optional[str] = None) -> pyspark.sql.column.Column\n",
      "        Bucketize rows into one or more time windows given a timestamp specifying column. Window\n",
      "        starts are inclusive but the window ends are exclusive, e.g. 12:05 will be in the window\n",
      "        [12:05,12:10) but not in [12:00,12:05). Windows can support microsecond precision. Windows in\n",
      "        the order of months are not supported.\n",
      "        \n",
      "        The time column must be of :class:`pyspark.sql.types.TimestampType`.\n",
      "        \n",
      "        Durations are provided as strings, e.g. '1 second', '1 day 12 hours', '2 minutes'. Valid\n",
      "        interval strings are 'week', 'day', 'hour', 'minute', 'second', 'millisecond', 'microsecond'.\n",
      "        If the ``slideDuration`` is not provided, the windows will be tumbling windows.\n",
      "        \n",
      "        The startTime is the offset with respect to 1970-01-01 00:00:00 UTC with which to start\n",
      "        window intervals. For example, in order to have hourly tumbling windows that start 15 minutes\n",
      "        past the hour, e.g. 12:15-13:15, 13:15-14:15... provide `startTime` as `15 minutes`.\n",
      "        \n",
      "        The output column will be a struct called 'window' by default with the nested columns 'start'\n",
      "        and 'end', where 'start' and 'end' will be of :class:`pyspark.sql.types.TimestampType`.\n",
      "        \n",
      "        .. versionadded:: 2.0.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        timeColumn : :class:`~pyspark.sql.Column`\n",
      "            The column or the expression to use as the timestamp for windowing by time.\n",
      "            The time column must be of TimestampType or TimestampNTZType.\n",
      "        windowDuration : str\n",
      "            A string specifying the width of the window, e.g. `10 minutes`,\n",
      "            `1 second`. Check `org.apache.spark.unsafe.types.CalendarInterval` for\n",
      "            valid duration identifiers. Note that the duration is a fixed length of\n",
      "            time, and does not vary over time according to a calendar. For example,\n",
      "            `1 day` always means 86,400,000 milliseconds, not a calendar day.\n",
      "        slideDuration : str, optional\n",
      "            A new window will be generated every `slideDuration`. Must be less than\n",
      "            or equal to the `windowDuration`. Check\n",
      "            `org.apache.spark.unsafe.types.CalendarInterval` for valid duration\n",
      "            identifiers. This duration is likewise absolute, and does not vary\n",
      "            according to a calendar.\n",
      "        startTime : str, optional\n",
      "            The offset with respect to 1970-01-01 00:00:00 UTC with which to start\n",
      "            window intervals. For example, in order to have hourly tumbling windows that\n",
      "            start 15 minutes past the hour, e.g. 12:15-13:15, 13:15-14:15... provide\n",
      "            `startTime` as `15 minutes`.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import datetime\n",
      "        >>> df = spark.createDataFrame(\n",
      "        ...     [(datetime.datetime(2016, 3, 11, 9, 0, 7), 1)],\n",
      "        ... ).toDF(\"date\", \"val\")\n",
      "        >>> w = df.groupBy(window(\"date\", \"5 seconds\")).agg(sum(\"val\").alias(\"sum\"))\n",
      "        >>> w.select(w.window.start.cast(\"string\").alias(\"start\"),\n",
      "        ...          w.window.end.cast(\"string\").alias(\"end\"), \"sum\").collect()\n",
      "        [Row(start='2016-03-11 09:00:05', end='2016-03-11 09:00:10', sum=1)]\n",
      "    \n",
      "    window_time(windowColumn: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Computes the event time from a window column. The column window values are produced\n",
      "        by window aggregating operators and are of type `STRUCT<start: TIMESTAMP, end: TIMESTAMP>`\n",
      "        where start is inclusive and end is exclusive. The event time of records produced by window\n",
      "        aggregating operators can be computed as ``window_time(window)`` and are\n",
      "        ``window.end - lit(1).alias(\"microsecond\")`` (as microsecond is the minimal supported event\n",
      "        time precision). The window column must be one produced by a window aggregating operator.\n",
      "        \n",
      "        .. versionadded:: 3.4.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        windowColumn : :class:`~pyspark.sql.Column`\n",
      "            The window column of a window aggregate records.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            the column for computed results.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> import datetime\n",
      "        >>> df = spark.createDataFrame(\n",
      "        ...     [(datetime.datetime(2016, 3, 11, 9, 0, 7), 1)],\n",
      "        ... ).toDF(\"date\", \"val\")\n",
      "        \n",
      "        Group the data into 5 second time windows and aggregate as sum.\n",
      "        \n",
      "        >>> w = df.groupBy(window(\"date\", \"5 seconds\")).agg(sum(\"val\").alias(\"sum\"))\n",
      "        \n",
      "        Extract the window event time using the window_time function.\n",
      "        \n",
      "        >>> w.select(\n",
      "        ...     w.window.end.cast(\"string\").alias(\"end\"),\n",
      "        ...     window_time(w.window).cast(\"string\").alias(\"window_time\"),\n",
      "        ...     \"sum\"\n",
      "        ... ).collect()\n",
      "        [Row(end='2016-03-11 09:00:10', window_time='2016-03-11 09:00:09.999999', sum=1)]\n",
      "    \n",
      "    xxhash64(*cols: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Calculates the hash code of given columns using the 64-bit variant of the xxHash algorithm,\n",
      "        and returns the result as a long column. The hash computation uses an initial seed of 42.\n",
      "        \n",
      "        .. versionadded:: 3.0.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        cols : :class:`~pyspark.sql.Column` or str\n",
      "            one or more columns to compute on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            hash value as long column.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('ABC', 'DEF')], ['c1', 'c2'])\n",
      "        \n",
      "        Hash for one column\n",
      "        \n",
      "        >>> df.select(xxhash64('c1').alias('hash')).show()\n",
      "        +-------------------+\n",
      "        |               hash|\n",
      "        +-------------------+\n",
      "        |4105715581806190027|\n",
      "        +-------------------+\n",
      "        \n",
      "        Two or more columns\n",
      "        \n",
      "        >>> df.select(xxhash64('c1', 'c2').alias('hash')).show()\n",
      "        +-------------------+\n",
      "        |               hash|\n",
      "        +-------------------+\n",
      "        |3233247871021311208|\n",
      "        +-------------------+\n",
      "    \n",
      "    year(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Extract the year of a given date/timestamp as integer.\n",
      "        \n",
      "        .. versionadded:: 1.5.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target date/timestamp column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            year part of the date/timestamp as integer.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([('2015-04-08',)], ['dt'])\n",
      "        >>> df.select(year('dt').alias('year')).collect()\n",
      "        [Row(year=2015)]\n",
      "    \n",
      "    years(col: 'ColumnOrName') -> pyspark.sql.column.Column\n",
      "        Partition transform function: A transform for timestamps and dates\n",
      "        to partition data into years.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        col : :class:`~pyspark.sql.Column` or str\n",
      "            target date or timestamp column to work on.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            data partitioned by years.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df.writeTo(\"catalog.db.table\").partitionedBy(  # doctest: +SKIP\n",
      "        ...     years(\"ts\")\n",
      "        ... ).createOrReplace()\n",
      "        \n",
      "        Notes\n",
      "        -----\n",
      "        This function can be used only in combination with\n",
      "        :py:meth:`~pyspark.sql.readwriter.DataFrameWriterV2.partitionedBy`\n",
      "        method of the `DataFrameWriterV2`.\n",
      "    \n",
      "    zip_with(left: 'ColumnOrName', right: 'ColumnOrName', f: Callable[[pyspark.sql.column.Column, pyspark.sql.column.Column], pyspark.sql.column.Column]) -> pyspark.sql.column.Column\n",
      "        Merge two given arrays, element-wise, into a single array using a function.\n",
      "        If one array is shorter, nulls are appended at the end to match the length of the longer\n",
      "        array, before applying the function.\n",
      "        \n",
      "        .. versionadded:: 3.1.0\n",
      "        \n",
      "        .. versionchanged:: 3.4.0\n",
      "            Supports Spark Connect.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        left : :class:`~pyspark.sql.Column` or str\n",
      "            name of the first column or expression\n",
      "        right : :class:`~pyspark.sql.Column` or str\n",
      "            name of the second column or expression\n",
      "        f : function\n",
      "            a binary function ``(x1: Column, x2: Column) -> Column...``\n",
      "            Can use methods of :class:`~pyspark.sql.Column`, functions defined in\n",
      "            :py:mod:`pyspark.sql.functions` and Scala ``UserDefinedFunctions``.\n",
      "            Python ``UserDefinedFunctions`` are not supported\n",
      "            (`SPARK-27052 <https://issues.apache.org/jira/browse/SPARK-27052>`__).\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        :class:`~pyspark.sql.Column`\n",
      "            array of calculated values derived by applying given function to each pair of arguments.\n",
      "        \n",
      "        Examples\n",
      "        --------\n",
      "        >>> df = spark.createDataFrame([(1, [1, 3, 5, 8], [0, 2, 4, 6])], (\"id\", \"xs\", \"ys\"))\n",
      "        >>> df.select(zip_with(\"xs\", \"ys\", lambda x, y: x ** y).alias(\"powers\")).show(truncate=False)\n",
      "        +---------------------------+\n",
      "        |powers                     |\n",
      "        +---------------------------+\n",
      "        |[1.0, 9.0, 625.0, 262144.0]|\n",
      "        +---------------------------+\n",
      "        \n",
      "        >>> df = spark.createDataFrame([(1, [\"foo\", \"bar\"], [1, 2, 3])], (\"id\", \"xs\", \"ys\"))\n",
      "        >>> df.select(zip_with(\"xs\", \"ys\", lambda x, y: concat_ws(\"_\", x, y)).alias(\"xs_ys\")).show()\n",
      "        +-----------------+\n",
      "        |            xs_ys|\n",
      "        +-----------------+\n",
      "        |[foo_1, bar_2, 3]|\n",
      "        +-----------------+\n",
      "\n",
      "DATA\n",
      "    Any = typing.Any\n",
      "        Special type indicating an unconstrained type.\n",
      "        \n",
      "        - Any is compatible with every type.\n",
      "        - Any assumed to have all methods.\n",
      "        - All values assumed to be instances of Any.\n",
      "        \n",
      "        Note that all the above statements are true from the point of view of\n",
      "        static type checkers. At runtime, Any should not be used with instance\n",
      "        or class checks.\n",
      "    \n",
      "    Callable = typing.Callable\n",
      "        Callable type; Callable[[int], str] is a function of (int) -> str.\n",
      "        \n",
      "        The subscription syntax must always be used with exactly two\n",
      "        values: the argument list and the return type.  The argument list\n",
      "        must be a list of types or ellipsis; the return type must be a single type.\n",
      "        \n",
      "        There is no syntax to indicate optional or keyword arguments,\n",
      "        such function types are rarely used as callback types.\n",
      "    \n",
      "    Dict = typing.Dict\n",
      "        A generic version of dict.\n",
      "    \n",
      "    Iterable = typing.Iterable\n",
      "        A generic version of collections.abc.Iterable.\n",
      "    \n",
      "    List = typing.List\n",
      "        A generic version of list.\n",
      "    \n",
      "    Optional = typing.Optional\n",
      "        Optional type.\n",
      "        \n",
      "        Optional[X] is equivalent to Union[X, None].\n",
      "    \n",
      "    TYPE_CHECKING = False\n",
      "    Tuple = typing.Tuple\n",
      "        Tuple type; Tuple[X, Y] is the cross-product type of X and Y.\n",
      "        \n",
      "        Example: Tuple[T1, T2] is a tuple of two elements corresponding\n",
      "        to type variables T1 and T2.  Tuple[int, float, str] is a tuple\n",
      "        of an int, a float and a string.\n",
      "        \n",
      "        To specify a variable-length tuple of homogeneous type, use Tuple[T, ...].\n",
      "    \n",
      "    Union = typing.Union\n",
      "        Union type; Union[X, Y] means either X or Y.\n",
      "        \n",
      "        To define a union, use e.g. Union[int, str].  Details:\n",
      "        - The arguments must be types and there must be at least one.\n",
      "        - None as an argument is a special case and is replaced by\n",
      "          type(None).\n",
      "        - Unions of unions are flattened, e.g.::\n",
      "        \n",
      "            Union[Union[int, str], float] == Union[int, str, float]\n",
      "        \n",
      "        - Unions of a single argument vanish, e.g.::\n",
      "        \n",
      "            Union[int] == int  # The constructor actually returns int\n",
      "        \n",
      "        - Redundant arguments are skipped, e.g.::\n",
      "        \n",
      "            Union[int, str, int] == Union[int, str]\n",
      "        \n",
      "        - When comparing unions, the argument order is ignored, e.g.::\n",
      "        \n",
      "            Union[int, str] == Union[str, int]\n",
      "        \n",
      "        - You cannot subclass or instantiate a union.\n",
      "        - You can use Optional[X] as a shorthand for Union[X, None].\n",
      "    \n",
      "    ValuesView = typing.ValuesView\n",
      "        A generic version of collections.abc.ValuesView.\n",
      "    \n",
      "    has_numpy = True\n",
      "\n",
      "FILE\n",
      "    /Users/tanmaysingla/opt/anaconda3/lib/python3.9/site-packages/pyspark/sql/functions.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(F)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d840f25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
